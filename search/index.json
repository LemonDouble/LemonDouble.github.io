[{"content":"AWS에는 활용 자격증이 있습니다. (Link)\n언젠간 따야지.. 따야지.. 하고 벼르고만 있다, 이러다간 평생 못 딸 것 같아서, 오늘 약속도 없겠다 당일에 시험 신청하고 온라인 시험으로 한번 봤습니다.\n이게 어떤 자격증이고, 따면 뭐가 좋고.. 공부 방법은 어떻고.. 하는 글은 엄청 많으니, 다른 블로그에 맡기도록 하고 2024 기준으로 신청/유의사항/후기 정도만 정리해보려 합니다.\n1. 신청 AWS 자격증 링크에 들어가서 시험 일정 예약을 누르고 예약을 할 수 있습니다.\n저는 Online with OnVUE 를 선택해 온라인으로 시험을 봤습니다. 시험 준비시에 웹캠 이 필요하니 만약 필요하다면 미리 준비해 주세요!\n시험은 한국어로 신청하면 되고, 한국어로 신청해도 시험 도중에 번역이 이상한 것 같으면 영어 지문을 볼 수 있습니다.\n추가로 팁을 드리면, AWS 할인 바우처는 찾아보면 하나쯤 있을 가능성이 높으니, 한번 찾아보시길 추천드립니다.\n저는 Reddit의 할인 바우처 링크를 사용했는데, 시간이 괜찮으시다면 AWS한국사용자모임 Slack 등에서 한번 할인 정보를 물어보시는것도 좋을 것 같습니다.\n2. 준비 프로그램 다운받고 지시에 따르면 시험 과정 세팅하는데는 큰 문제가 없습니다.\n단, 백그라운드 프로세스중 일부가 문제를 일으키는 경우가 있었습니다. 저의 경우에는 하라는 대로 포그라운드 프로세스를 다 껐는데도, 백그라운드 프로세스 중 크롬 원격 데스크탑 서비스가 문제가 되어 시험 진행을 못 한 뻔한 경험이 있습니다.\n만약 chrome ¿ø°Ý µ¥½ºũÅé ¼­ºñ가 뜬다면, 크롬 원격 데스크탑 관련 프로그램이니 작업 관리자 \u0026gt; 서비스 \u0026gt; chromoting 검색 \u0026gt; 중지 를 하면 시험 준비를 올바르게 마칠 수 있습니다.\n3. 시험 시험은\n시험 시간 30분 전에 Check in 하러 먼저 들어가서 얼굴 사진 찍고, 신분증 사진 찍고, 시험 칠 공간 (주변 사진) 찍고 감독관이랑 웹캠으로 연결 후 워치 차고 있는지 손 보여주기, 전신 보여주기, 웹캠으로 데스크에 아무것도 없는지, 주변에 아무것도 없는지 확인하기 이후 시험 진행 순으로 진행됩니다.\n문제는 일반적으로 평이했는데, 잘 모르는게 있다면\n비슷한 서비스가 여러 개 있다면 최근에 나온거 찍기 (EG \u0026gt; CLB랑 ALB가 있다면 ALB가 정답일 가능성이 높겠죠?) Serverless 등 AWS가 밀어주는 키워드 찍기 가 찍기 팁 정도가 되겠습니다(\u0026hellip;)\n4. 결과 발송 예전에는 시험 끝나자마자 결과가 나왔다고 하는데, 저는 그렇진 않았습니다.\n저는 11시쯤 당일 13시 15분 시험을 신청했고, 12시 45분부터 체크인을 시작해 시험을 2시쯤 끝내고 나왔습니다.\n시험 결과는 19시 15분쯤 왔으니, 대충 처리하는데 5~6시간정도 걸리는 것 같습니다.\n합격하면 다음과 같은 메일이 날아옵니다.\nAWS를 쓴지 오래 돼서, 그냥 기억을 더듬더듬 더듬으며 풀었던 문제가 많았습니다.\n가격이 비싸서 엄두도 못 내고 있었는데, 이러면 평생 못 볼 것 같으니 한번 떨어진다고 생각하고 쳐 보자 하니 좋은 결과가 있었씁니다~\n다음엔 Developer 자격증을 따 볼까 생각중인데, 언제 딸지는 고민해볼 생각입니다. 그럼 안녕~\n","date":"2024-08-04T20:11:48+09:00","image":"https://blog.lemondouble.com/p/d3c7a9de0a536286/cover_hu_750199fbdd8090be.png","permalink":"https://blog.lemondouble.com/p/d3c7a9de0a536286/","title":"AWS SAA 당일 시험 합격 후기글"},{"content":"만든 스마트홈 미리보기 (일본어 주의) Link 1. HomeAssistant? 이전에 한번 글을 쓴 적이 있는 HomeAssistant라는 플랫폼에서 출발합니다.\n2023년은 HomeAssistant에겐 음성의 해 였습니다.\nLLM을 필두로 한 인공지능이 점점 고도화되며, 이제는 정말 자비스를 만들 수 있겠다! 는 생각을 하던 해였죠.\n2. 그래서 말하고 싶은게 뭔데? 그냥 자랑 한번 해 보고 싶었습니다(\u0026hellip;)\n3. 사용/참고한 정보 HotWordPlugin - 안드로이드에서 Wake word 감지\nTasker - 안드로이드에서 HotwordPlugin 이벤트를 감지해서 특정 앱 실행\nWhisper - Speech To Text 엔진\nextended_openai_conversation - 집에 있는 디바이스 정보를 이용, function calling 기능을 이용해 실제 디바이스 상태 조절 함수를 호출해 줌.\nBert-VITS2 (TTS) - 캐릭터 목소리 귀엽죠?\nZigbee2Mqtt Zigbee 신호를 MQTT Queue와 연결해 주는 오픈소스 프로젝트에요.\nSONOFF ZBDongle-P - 라즈베리 파이에서 Zigbee 신호를 쏘게 해 주는 동글이에요\nTuya IR 리모컨 - Zigbee를 받아서 IR 신호를 쏴 줘요\n","date":"2024-07-26T22:07:33+09:00","image":"https://blog.lemondouble.com/p/30f1bf25ad8b594a/cover_hu_e3411257c5ef0155.png","permalink":"https://blog.lemondouble.com/p/30f1bf25ad8b594a/","title":"음성으로 컨트롤되는 인공지능 스마트홈 자랑하는 글"},{"content":"우리가 만들 오디오 한번 들어보기 (일본어) Link 1. TTS? Text-To-Speech는 문자 그대로, 텍스트를 음성 데이터로 변경해 주는 소프트웨어를 말합니다.\n한번 만들어 놓으면, 호출량 걱정 없이 많은 Automation에서 사용할 수 있기에, 직접 한번 만들어 보는 것이 목표입니다.\n이왕 만든다면, 내가 아는 캐릭터가 이야기 하면 더 재밌을 것 같지 않나요? 그러므로, 한번 커스텀 데이터를 이용해 직접 모델을 튜닝하고, TTS를 만들어 보는걸 해 볼 예정입니다.\n그 중, 저희는 Bert-Vits2 라는 모델을 이용하여 학습을 해 볼 예정입니다. 참고로 중국어, 영어, 일본어를 지원합니다.\n만약, 나는 캐릭터 목소리같은건 관심 없고 정갈한 한글 목소리가 필요하다! 라는 분은 Melotts와 Pretrian된 모델을 찾아보시는걸 권해 드립니다.\n마지막으로 덧붙여, 모델을 만들고 난 후에는 모델 배포 등을 하지 마시고, 개인 연구용/취미용으로만 사용하는 것을 강력히 권해 드립니다.\n준비물은 GPU 달린 우분투 서버와 근성입니다(..)\n2. 작업 프로세스 학습을 위해서는 적어도 20분~1시간 가량의 깨끗한 음성 데이터가 필요합니다.\n직접 녹음할 수 있다면 가장 좋겠지만, 중국어/일본어/영어 데이터를 20분~1시간 발화하는 것은 쉬운 일이 아닙니다.\n따라서, 저희는 애니메이션을 이용해서 학습 자료를 만들어 볼 예정입니다.\n아이디어는 다음과 같습니다.\n자막은 자막 시작 시간과 끝 시간, 텍스트 데이터가 있다. 애니메이션에서 자막 시간만큼을 끊어 음성 데이터를 추출하고, 그 음성 데이터의 Label을 자막 값으로 한다. 그리고, 전체 프로세스는 다음과 같습니다.\n좋아하는 애니메이션과, 원어 자막을 구합니다. 자막 데이터와 애니메이션 데이터를 불러와, 대사 부분을 추출합니다. 대사 부분을 추출한 후, 배경 음악 등을 머신러닝으로 제거합니다. 직접 대사를 하나하나 들으면서, 내가 원하는 캐릭터가 아닌 데이터를 삭제합니다. 그러면 \u0026ldquo;내가 원하는 캐릭터\u0026rdquo; 의 \u0026ldquo;음성 데이터와 라벨 데이터\u0026quot;가 모이게 됩니다. 이 데이터로 Bert-VITS2를 학습시킵니다. 완성된 TTS를 가지고 놀아 봅니다! 이 프로세스대로 간결하게 설명해보려고 합니다.\n3-1. 자막 파일로 대사 부분을 추출하기 애니메이션, 원어 자막은 알아서 구할 거라고 생각하고 패스합니다.\n제가 사용했던 자료를 모아놓은 Github 에 들어갑니다.\ngit clone 이후, 의존성을 설치하고 extract_wav_1.py 에서 동영상명, 자막 파일 명을 집어넣고 실행시킵니다.\n그러면 data 폴더에 1~xxx.wav 파일이 자동으로 생성됩니다.\n3-2. UVR (ULTIMATE VOCAL REMOVER)을 이용하여 배경 음악 제거하기 UVR 홈페이지 에 가서 다운 후 실행합니다.\n이후 다음과 같이 설정을 맞춰 주세요.\ninput은 방금 생성한 wav 파일을 전체 넣어줍니다. output은 cleaned 란 폴더를 만들어 넣어 주세요. Choose MDX-NET Models는 Download New Model -\u0026gt; MDX-Net에서 MDX23C-InstVoc HQ 모델을 다운받은 후 선택해 줍니다. GPU Conversion, Vocals Only 옵션을 켜 주고 실행시킵니다. 3-3. 수작업으로 저품질 오디오, 다른 캐릭터 대사 제거하기 cleaned 폴더에 들어가서 저품질 오디오 (발소리나 효과음이 섞인 경우, 말이 섞인 경우)와, 다른 캐릭터의 대사를 제거합니다. (파일 삭제)\n3-4. 라벨 데이터 만들기 change_name_2.py를 실행해 clenaed 폴더의 파일 이름을 바꿔주고, make_list_3.py를 실행해 esd.list라는 파일을 만들어 줍니다.\n3-5. 수집한 데이터 길이 확인하기 calculate_length.py 를 실행하여, 몇분 몇초만큼의 데이터를 모았는지 확인합니다.\n3-6. 반복하기 만약 충분한 데이터가 안 보였다면, 데이터가 모일때까지 앞의 작업을 반복해 주세요.\n별도의 디렉토리에 캐릭터 이름으로 폴더를 하나 만들고, 다음과 같이 배치합니다.\n1 2 3 4 5 6 캐릭터명/ ㄴesd.list ㄴraw/ ㄴf6df34d9-01c4-4db9-97e9-a35795a5f64b.wav ㄴ18bcc2c6-3e23-427c-a04a-9414b6f0b85e.wav ... esd.list는 텍스트 편집기로 열어 계속 붙여넣어주면 되고, raw 폴더 내에는 cleaned 내의 wav 파일들을 붙여넣어주면 됩니다.\n계속 반복하여 파일 총 길이가 20분~1시간이 되도록 합시다.\n3-7. 데이터 학습하기 우분투 서버로 옮깁니다.\n우분투에 CUDA가 설치되어 있지 않다면 우분투 CUDA 설치로 검색하여 CUDA를 설치합니다.\nBert-Vits2 리포지토리를 Clone하고, 가상환경을 만든 후 pip install -r requirements.txt 입력하여 의존성 설치해 줍니다.\n이후 python webui_preprocess.py 실행합니다.\n이후, 해당 Gradio 환경에 접속한 후 파일을 각각 받아 위치에 집어넣습니다.\n中文 RoBERTa -\u0026gt; BERT_VITS2/bert/chinese-roberta-wwm-ext-large 폴더에 flax_model.msgpack, pytorch_model.bin,tf_model.h5 다운로드 후 넣기\n나머지도 똑같이 용량 큰 파일 다운받아서 넣어 줍니다.\n日文 DeBERTa -\u0026gt; Bert-VITS2/bert/deberta-v2-large-japanese-char-wwm 폴더\n英文 DeBERTa -\u0026gt; Bert-VITS2/bert/deberta-v3-large 폴더,\nWaveLM -\u0026gt; Bert-VITS2/slm/wavlm-base-plus 폴더\n이후, Bert-VITS2/data/{캐릭터 이름}폴더를 만들고 학습 데이터를 옮겨줍니다.\n1 2 3 4 5 6 7 8 9 10 Bert-VITS2/ ㄴdata/ ㄴElaina/ ㄴesd.list ㄴtrain.list ㄴval.list ㄴraw/ ㄴf6df34d9-01c4-4db9-97e9-a35795a5f64b.wav ㄴ18bcc2c6-3e23-427c-a04a-9414b6f0b85e.wav ... 이 때, esd.list는 만든 파일 그대로, val.list는 esd.list의 마지막 5%정도를 떼서 복사 붙여넣기, train.list는 val.list에 넣은거 빼고 복사 붙여넣기 해서 파일을 만들어주면 됩니다.\n예시 :\nesd.list\n1 2 3 4 5 f6df34d9-01c4-4db9-97e9-a35795a5f64b.wav|Elaina|JP|ありがとう b234c567-d890-1234-e567-890f123g4567.wav|Elaina|JP|おはよう c345d678-e901-2345-f678-901g234h5678.wav|Elaina|JP|すみません d456e789-f012-3456-g789-012h345i6789.wav|Elaina|JP|さようなら e567f890-0123-4567-h890-123i456j7890.wav|Elaina|JP|元気ですか？ train.list\n1 2 3 4 f6df34d9-01c4-4db9-97e9-a35795a5f64b.wav|Elaina|JP|ありがとう b234c567-d890-1234-e567-890f123g4567.wav|Elaina|JP|おはよう c345d678-e901-2345-f678-901g234h5678.wav|Elaina|JP|すみません d456e789-f012-3456-g789-012h345i6789.wav|Elaina|JP|さようなら val.list\n1 e567f890-0123-4567-h890-123i456j7890.wav|Elaina|JP|元気ですか？ 여기까지 했다면, 아까 Gradio에 가서 다음과 같이 설정합니다.\n캐릭터 이름(제 경우 Elaina) 를 입력합니다. 배치 사이즈를 선택합니다. 배치 사이즈가 클수록 학습은 빠르나, 배치 사이즈가 너무 크면 GPU 메모리에 다 안 올라갑니다. 저의 경우는 3090이라 12로 설정했는데, 일단 큰거 써 보고 이후에 줄이면 됩니다. 다 설정했다면 버튼을 누릅니다. 버튼을 누르면, Bert-VITS2/config.yml 파일이 생깁니다. 해당 파일을 수정합니다.\n7번 라인의 \u0026ldquo;Data/\u0026rdquo; 부분을 \u0026ldquo;data/{캐릭터명}\u0026ldquo;으로 수정합니다. (제 경우 data/Elaina) 20번 라인의 in_dir을 \u0026ldquo;audios/raw\u0026rdquo; -\u0026gt; \u0026ldquo;raw\u0026rdquo; 로 변경합니다. 22번 라인의 out_dir을 \u0026ldquo;audios/wavs\u0026rdquo; -\u0026gt; \u0026ldquo;wavs\u0026quot;로 변경합니다. 29번 라인의 transcription_path를 \u0026ldquo;filelists/esd.list\u0026rdquo; 로 변경합니다. 이후 Gradio로 다시 돌아가, 第二步：预处理音频文件(2단계: 오디오 파일 전처리), 第三步：预处理标签文件(3단계: 라벨 파일 전처리), 第四步：生成 BERT 特征文件(4단계: BERT 기능 파일 생성) 을 순서대로 눌러줍니다.\n이후 Pretrain된 모델을 다운로드 받습니다.\n다음 링크 로 이동하여 DUR_0.pth, D_0.pth, G_0.pth, WD_0.pth을 다운받아 data/{캐릭터명}/models 폴더에 붙여넣기 합니다.\n이후 config.yml의 90번 라인의 config_path를 “configs/config.json” 로 변경합니다. (train_ms 부분)\n이후 Bert-VITS2 폴더에서 torchrun --nproc_per_node=1 train_ms.py 실행시 학습이 진행됩니다!\n3-8. 음성 출력하기 학습이 진행되면, 음성이 정상적으로 나오는지 확인해야 합니다.\ndata/{캐릭터명}/models 폴더에 보면, 학습이 진행될수록 여러 파일들이 생깁니다. G_1000.pth 같은 식으로요.\nG_xxxx.pth 파일이 목소리를 생성하기 위한 파일입니다. 해당 폴더를 확인 후, config.yml의 105번쨰 줄 (webui 부분)의 model을 \u0026ldquo;model/G_xxxx.pth\u0026rdquo; 로 변경합니다. (이떄 xxxx는 현재 디렉토리에 있는 모델)\n이후, Bert-VITS2 폴더에서 python webui.py 를 입력하면 Gradio가 생기게 되고,해당 Gradio에서 TTS를 테스트 해 볼 수 있습니다.\n참고로 이 데이터를 이용해 별도로 서버를 만들고 싶다면, hiyoriUI.py를 참고해 보세요! fastAPI 기반의 서버를 제공합니다.\n마무리하며 이것으로 간?단히 TTS를 만드는 법에 대한 이야기를 해 봤습니다.\n중간중간 구멍이 많이 뚫려 있는데, 비슷한 글이 있어 해당 글에서 헷갈렸던 부분만 집중적으로 다뤄서 그렇습니다.\nChapter 4. Bert-VITS2 사전 준비 및 훈련 시작\n본 글에서 구멍이 나 있는 부분은, 이 글을 같이 보시면 이해가 빠를 거에요.\n저의 경우, 별도로 돌리고 있는 머신러닝 서버에 TTS를 이식해 잘 쓰고 있답니다.\n혹시 잘 안 되는 부분 있으시면 편하게 댓글 주시고, 여러분도 즐거운 TTS 만드시길 바랍니다!\n","date":"2024-07-26T20:24:29+09:00","image":"https://blog.lemondouble.com/p/9c06f7dc324b5986/cover_hu_ea379d524a73e8d.png","permalink":"https://blog.lemondouble.com/p/9c06f7dc324b5986/","title":"Bert-VITS2로 나만의 TTS 만들기"},{"content":"1. Forward Auth? 인증 로직이 필요할 떄, 우리는 인증 로직을 어디서 구현할 수 있을까요?\n바로 드는 생각은, 어플리케이션에서 직접 구현하는 방법일 것입니다.\n하지만 어플리케이션이 많아지면 어떨까요? 혹은, 일부 어플리케이션은 내가 만든 것이 아니라 다른 사람이 만든 어플리케이션이라, 내가 맘대로 인증 로직을 구현할 수 없다면 어떨까요?\n우리는 쿠버네티스를 사용하고 있고, 모든 요청은 Traefik을 한번 거쳐갑니다.\n따라서, Traefik이 \u0026quot;요청을 전달하기 전\u0026quot; 에 인증 로직을 처리하면 어떨까요?\n간단합니다!\n모든 요청은 Traefik을 지나갑니다. 만약 Forward Auth가 설정되어 있다면, 외부 인증 서비스에 요청을 보냅니다. 외부 인증 서비스는 다음과 같은 작업을 합니다. 만약 유효한 요청이라면, 2XX대의 OK 코드를 보냅니다. 이 경우 정상적으로 만약 유효하지 않은 요청이라면 인증 서버의 응답을 그대로 반환합니다. (에러 반환) 그럼 이걸 어떻게 쓸 수 있을까요?\n2. OAuth2와 Cookie, Forward Auth 2-1. 가장 간단한 Forward Auth 서버 처음부터 천천히 생각해 봅시다.\n일단, 가장 간단한 Forward Auth 서버를 생각해 봅시다.\n어떤 요청에 대해서는 OK, 어떤 요청에 대해서는 NO를 주려면 어떻게 해야 할까요?\n맞습니다! 우리가 Basic Auth때 한번 본 적이 있듯, 각 요청에 대해 인증 정보를 같이 보내고, 그 인증 정보가 유효한 경우 통과시켜주면 될 것입니다.\n그림으로 그려보면, 가장 간단한 예시는 다음과 같습니다.\n2-2. 인증 토큰은 어디에 저장하지? 좋습니다!\n일단 가장 간단한 Forward Auth Server를 생각해 보았습니다. 그러면 다음은, \u0026ldquo;그러면 토큰은 언제, 어디서 받아야 하는데?\u0026rdquo; 라는 질문이 남습니다.\n간단한 방법으론, 로그인 페이지를 하나 만들어서 로그인에 성공하면 Token을 발급해주는 방법이 있을 것 같습니다.\n그림으로 그려보겠습니다.\n어? 그런데 auth.lemondouble.com 에서 로그인했다면, 토큰을 어떻게 공유할 수 있을까요? 어디에 토큰을 저장해야 할까요?\n그런데 나는 a.lemondouble.com, b.lemondouble.com같은 다른 사이트에서도 해당 토큰을 이용해서 로그인을 유지해야 합니다.\n(사실 꽤 자주 있는 일이긴 하지만) a.lemondouble.com에서 이미 SSO 로그인을 했는데, b.lemondouble.com에서 로그인을 다시 시킨다면, (유저 정보가 공유되더라도) 너무 귀찮은 일일 것입니다!\n우리는 이를 해결하기 위해서, Cookie에 토큰을 담을 예정입니다.\n예를 들어, .lemondouble.com 에다가 Cookie를 설정하면, 해당 Cookie는 모든 서브도메인, 즉 a.lemondouble.com , auth.lemondouble.com, 'b.lemondouble.com' 에 접속할 시 자동으로 같이 전송된다는 점을 이용하는 것입니다.\n1 2 3 res.cookie(\u0026#39;forward-auth-token\u0026#39;, token, { domain: \u0026#39;.lemondouble.com\u0026#39;, //subdomain까지 공유하려면 앞에 \u0026#34;.\u0026#34;을 붙여줘야함! }); 그러면, 많은 일이 간단해집니다! 정리해보죠.\nForward Auth는 토큰이 있다면 허용, 토큰이 없다면 거부한다. 로그인 페이지를 만들어, 로그인 페이지는 로그인 성공시 .lemondouble.com 에 쿠키를 설정하도록 합니다. 여기까지 정리해보면 다음과 같은 그림이 나옵니다.\n2-3. JWT 토큰 사용 및 만료 설정, 유효성 검증 자! 그런데 토큰 안에는 어떤 값이 들어가면 좋을까요?\n이 경우, JWT가 적절할 것으로 보입니다. Stateless해야 하고, 위변조가 아주 어려운 토큰이야 말로 이 사용사례에 아주 적합해 보입니다.\n안의 내용은.. 유저 ID나 이메일 정도가 좋겠네요.\n그런데 토큰을 한번 발급했으면 천년만년 써도 될까요\u0026hellip;? 그러면 토큰을 한번 탈취당하면 천년 만년 서버가 털리겠죠..? 따라서, 토큰 유효 기간 설정이 필요할 것 같습니다. 보안과 편의성 사이에 고려하면 될 것 같은데, 저는 편의상 3600초 (1시간) 을 사용한다고 가정해 보겠습니다.\n그러면\nJWT를 Exp (Expired Time)을 현재시간 + 3600초로 로그인 서버에서 발급해 줘야 할 것 같네요. 편의상, Cookie 유지 시간도 3600초로 통일하면 좋겠네요. Forward Auth에서 기존은 토큰이 있다면 무조건 통과였지만, 토큰 JWT 검증 및, 만료 토큰일 시 값이 유효하더라도 거절하는 로직을 추가해야 할 것 같습니다. 그림으로 다시 그려보겠습니다.\n2-4. 리다이렉트 처리 그런데 만약 토큰이 유효하지 않거나, 토큰이 없다면 그냥 에러 페이지만 보여줘도 괜찮을까요?\n물론\u0026hellip; 내가 혼자 쓰면 안 될거 뭐 있겠습니까마는\u0026hellip; 굉장히 매번 귀찮을 것입니다. 그냥 인증 실패하면 자동으로 로그인 페이지로 리다이렉트 시켜주면 편할 것 같네요.\n바로 추가해 봅시다.\n그런데 보통 SSO 성공하면 기존 사이트로 돌아가잖아요? 그것도 한번 추가해 봅시다.\n쿼리 파라미터나 헤더 등으로 \u0026ldquo;원래 어디서 왔는지\u0026quot;를 기억하고, 로그인에 성공하면 원래 어디서 왔는지 기억해서 로그인 서버가 리다이렉트 처리를 해 주면 될 것 같습니다.\n아.. OAuth는 이야기도 안 꺼냈는데 벌써 복잡하네요. 일단 한번 더 해 봅시다..\n만약 통합인증이 ID/Password 로그인이라면 여기서 완성입니다. 여기서 조금 헷갈릴 수 있어서 정리하고 가면, 로그인 서버와 Traeifk의 Forward Auth 서버는 동일해도 무방합니다.\n굳이 그림까지 그리면서 이걸 다시 쓰는 이유는, Forward Auth 인증 서버와 로그인 서버를 분리할 필요가 없어서입니다!\n2-5. OAuth 시작! 일단 스펙을 정하고 가 봅시다.\n저희가 만들 스펙은 다음과 같습니다.\n만약 어떤 서비스에 접근했는데, OAuth SSO가 필요하다면 바로 해당 Social Provider의 로그인 페이지로 이동합니다. (즉, 우리는 로그인 페이지를 만들지 않습니다!) 로그인에 성공하면 원래 접근하려는 서비스로 리다이렉트됩니다! 그리고 Client ID 등록 등의 기본내용들은 생략합니다. 이걸 하나하나 설명하면 책 한 권이 나와서요.. 만약 OAuth 2.0 프로토콜에 대해 잘 모르신다면, 생활코딩의 Web2 - OAuth 2.0 강의를 추천드릴게요. (공짭니다!)\n2-6. OAuth 전체 플로우 제가.. 진짜 이걸 쉽게 설명하려고 노력해봤는데\u0026hellip; 솔직히 답이 안 서서요..\n전체 플로우를 보여드리곘습니다.\n핵심 아이디어는 State Code를 이용하는 것입니다!\nOAuth 인증 서버에 요청을 보낼 때, 임의의 문자열을 생성해 같이 (?state=임의의문자열) 보내면, 이후 유저가 로그인 성공 후 돌아올 때 해당 문자열을 같이 보내주는 스펙이 OAuth 2.0 표준에 있습니다. (필수 파라미터는 아니라, OAuth 연동을 하면서 못 보셨을 수도 있습니다!) 이를 이용해서 유저가 \u0026ldquo;어디서 왔는지\u0026rdquo; 를 데이터베이스에 기록하고, 로그인 성공 후 해당 URL로 리다이렉트 시켜주는 아이디어를 차용합니다. 전체 플로우는 위 그림을 참고해 주시고, 그러면 우리 인증 서버는 무엇을 구현해야 할까요?\n엔드포인트 단 하나를 구현하는데, 다음을 구현하시면 됩니다. (위에서 순서대로 처리해야 합니다. 처리 순서가 중요합니다!)\n쿼리 파라미터로 code와 state가 있는 경우, State Code가 DB에 저장된 값과 일치하는지 확인합니다. 만약 DB에 해당 State Code가 존재한다면, 타사 OAuth Server과 통신하여 Authorization Code를 유저 정보로 변환합니다. 변환한 유저 정보가 미리 등록된 권한 리스트에 있는지 확인하고, 만약 없다면 오류를 반환합니다. 만약 권한 리스트에 있다면, 로그인용 신규 JWT를 생성 후 Cookie에 .yourdomain.com 으로 저장합니다. 이 때, 보안을 위해 secure, httpOnly 설정을 켜 주면 좋습니다. 또한 sameSite 옵션은 'Lax'로 설정합니다. 이후 State Code와 한 쌍인 URI로 유저를 Redirect 시킵니다.\nCookie에 JWT 토큰이 있는 경우, JWT 검증 후 정상 JWT라면 200을 반환합니다.\n만약 Cookie에 JWT 토큰이 아예 없거나, 만료된 Cookie라면 만료된 Cookie를 지우고, State Code에 X-Forwarded-Host 헤더와 X-Forwarded-Uri를 파싱하여 현재 URI를 추출합니다. 이후 신규 State Code와 해당 URI 페어를 DB에 저장하고, 현재 API를 Redirect URL로 설정해 State Code와 함께 전송합니다.\nTIP : 유저가 요청한 URI : https://\u0026quot; + GetHeader(\u0026quot;X-Forwarded-Host\u0026quot;) + GetHeader(\u0026quot;X-Forwarded-Uri\u0026quot;) 2-7. 추가 설정 마지막으로.. Traefik 설정에 다음 두 줄을 추가합니다.\n1 2 - \u0026#34;--entryPoints.web.forwardedHeaders.insecure\u0026#34; - \u0026#34;--entryPoints.websecure.forwardedHeaders.insecure\u0026#34; 해당 줄이 없으면, X-forwared-for 헤더들이 넘어오지 않습니다!\n제 블로그의 글 (4. Traefik https 인증 및 ArgoCD 설정) 을 참고하셨다면, 다음과 같이 설정하시면 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # https://traefik.io/blog/https-on-kubernetes-using-traefik-proxy/ apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: traefik namespace: kube-system spec: valuesContent: |- ports: web: redirectTo: port: websecure priority: 10 additionalArguments: - \u0026#34;--log.level=INFO\u0026#34; - \u0026#34;--certificatesresolvers.le.acme.email=yourMail@gmail.com\u0026#34; - \u0026#34;--certificatesresolvers.le.acme.storage=/data/acme.json\u0026#34; - \u0026#34;--certificatesresolvers.le.acme.tlschallenge=true\u0026#34; - \u0026#34;--certificatesresolvers.le.acme.caServer=https://acme-v02.api.letsencrypt.org/directory\u0026#34; - \u0026#34;--entryPoints.web.forwardedHeaders.insecure\u0026#34; - \u0026#34;--entryPoints.websecure.forwardedHeaders.insecure\u0026#34; 이후 kubectl apply를 실행하여, Forwared Auth 설정을 마무리 해 줍시다.!\n마치며 사실 내용이 너무 방대해서.. 가능한 쉽게 쓴다고 노력했는데, 제대로 전달은 안 되었을 것 같습니다. 나중에 글을 한번 수정해야 되지 않나 싶네요.\n그래도 혹시 모르니, 참고하여 구축하실 분들은 이해 안 되는 부분이 있다면 아래 댓글이나, contact@lemondouble.com 으로 편하게 연락 주세요! 트러블슈팅까지 같이 도와드리겠습니다.\n아 그리고 혹시 Golang을 할줄 아신다면, traefik-forward-auth 를 참고하셔도 좋습니다.\n","date":"2024-01-07T16:05:04+09:00","image":"https://blog.lemondouble.com/p/a63eb8199e2089b6/cover_hu_a6811b3c87e50ef3.png","permalink":"https://blog.lemondouble.com/p/a63eb8199e2089b6/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 10-2. Forward Auth와 OAuth2를 이용한 SSO 구현"},{"content":"아래 내용은 Sealed Secrets 섹션 3과 겹치는 부분이 많습니다. 해당 섹션을 진행하셨고, Basic Auth에 대해 잘 아신다면 스킵해도 무방합니다.\n1. 서론 쿠버네티스를 하다 보면, 이런저런 서비스를 많이 올리게 됩니다.\n물론 내부망에서만 서비스하는것이 보안상 가장 좋겠지만, 클러스터 관리를 항상 내부망에서만 할 수 없을 수도 있습니다. 예를 들어 놀러갔는데 갑자기 문제가 생겼다거나 하는 경우가 있을 수도 있겠죠.\n이런 경우 해결방법이 두 가지가 있을 수 있습니다.\nVPN을 사용하여, 다른 컴퓨터에서 내부망에 접속해서 관련 서비스 사용 인터넷에 서비스를 열어놓되, 인증 매커니즘을 추가하여 사용 저의 경우는 2를 선택했고, 개인적으로 SSO 서버를 구축해 사용하고 있습니다.\n하지만 SSO 시스템은 구축 과정이 방대한 관계로 가장 간단한 인증 매커니즘 먼저 진행해 보려고 합니다.\n2. Basic Auth의 흐름 Basic Auth는 엄청 간단합니다!\n서버에게 페이지를 요청합니다. 서버는 요청을 받고, 인증 정보가 없으면 401 응답과 함께 인증 방법 (Basic Auth) 를 요구합니다. 클라이언트는 응답을 받고, Basic Auth를 진행합니다. 이 때, HTTP Authorization Header에 Basic ${인증 토큰} 의 값을 담아 보냅니다. 이 때, 인증 토큰은 Base64Encode(\u0026ldquo;userId:password\u0026rdquo;) 입니다. 예를 들어 id가 lemondouble, 패스워드가 1q2w3e4r!라면 \u0026quot;lemondouble:1q2w3e4r!\u0026quot; 를 base64로 인코딩한 값인 bGVtb25kb3VibGU6MXEydzNlNHIh를 사용합니다. 서버는 이 토큰을 확인한 후, 정상 응답을 돌려줍니다. 3. Basic Auth의 장단점? 장점 :\n빌트인이다! : 대부분의 브라우저에서 별다른 설정 없이도 지원합니다. 구현이 간편하다. : 대부분의 웹 프레임워크에서 기본 지원합니다! 단점 :\n보안이 취약하다 : 최대 단점입니다. 패스워드가 base64로 인코딩 (=평문)으로 매 요청마다 전송됩니다. Https 프로토콜을 사용한다면 괜찮겠지만, HTTP 프로토콜을 사용하면 매 요청마다 평문으로 패스워드가 전송됩니다! 무차별 대입 공격 등을 방어하지 않는다. Id/Password의 문제이겠지만, Traefik의 기본 설정대로라면 일정 수 이상 틀린 요청을 시킨 클라이언트를 드롭한다던가 하는 기능이 없습니다. Id/Password가 취약하다면, 어떤 악의적 공격자가 무차별 대입 공격을 통해 서비스에 접근할 수 있습니다. 따라서, 가능하다면 해당 기능은\n접근해도 별 문제 없는 서비스를 대상으로, 혹은 몇몇 유저에게만 공유하고 싶은 서비스를 대상으로 https 프로토콜을 사용해서 충분히 긴 패스워드를 사용하는 것 을 권장합니다!\n4. Basic Auth 설정 방법 저희는 Seaeld Secrets를 이용해, 접속 정보 (Id/Password)를 암호화해 Git에 저장하는 방식을 사용할 예정입니다. 이렇게 하는 이유는 다음과 같습니다. 불시의 사고가 일어났을 때, 접속 정보를 포함하여 신속하게 클러스터를 복구하기 위해서 (Secrets 정보도 Git에 있으므로, 배포만 하면 됨) 아무리 Private Repo를 사용한다고 하더라도, 순간의 실수로 Id/Password가 노출되는 것을 막기 위해 Disclaimer : 아래 내용은 Sealed Secrets 섹션 3과 동일합니다!\nLonghorn Dashboard를 외부에서 접근할 수 있게 설정하며, Basic Auth 설정 방법을 배워봅시다.\napt install apache2-utils 실행하여 htpasswd 커맨드를 사용할 수 있도록 합니다. htpasswd -nb \u0026lt;id\u0026gt; \u0026lt;password\u0026gt; | openssl base64 를 이용하여, id,password가 담긴 Secret String을 얻습니다. 아무 텍스트 편집기로 secret.yaml을 생성하고, 다음을 참고하여 Secret을 추가합니다. 1 2 3 4 5 6 7 apiVersion: v1 kind: Secret metadata: name: longhorn-system-basic-auth namespace: longhorn-system data: users: \u0026lt;2에서 얻은 String\u0026gt; 4.cat secret.yaml | kubeseal --controller-namespace=sealed-secrets-system --controller-name=sealed-secrets -oyaml \u0026gt; sealed-secrets.yaml 를 입력하여 sealed-secrets.yaml 을 얻습니다. 5. 다음을 참고하여 ingress를 등록합니다.\nmodules/longhorn-system/ingress.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: longhorn-dashboard namespace: longhorn-system spec: tls: certResolver: le routes: - kind: Rule match: Host(`\u0026lt;원하는 subdomain, 예시:longhorn.lemon.com\u0026gt;`) middlewares: - name: basic-auth namespace: longhorn-system services: - name: longhorn-frontend port: 80 --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: basic-auth namespace: longhorn-system spec: basicAuth: secret: longhorn-system-basic-auth modules/longhorn-system/sealed-basic-auth-secret.yaml 에는, 방금 생성한 sealed-secrets.yaml을 복사 붙여넣기 해 줍시다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: creationTimestamp: null name: longhorn-system-basic-auth namespace: longhorn-system spec: encryptedData: users: adfasdflavasdlfj... template: metadata: creationTimestamp: null name: longhorn-system-basic-auth namespace: longhorn-system 이후 배포를 진행해 줍니다. 이후 접속하면 정상적으로 id/password 확인 창이 뜨며, 로그인을 하지 않으면 해당 페이지가 보이지 않습니다! 5. 마치며 Basic Auth는 제 초기 클러스터 관리 방법이었습니다.\n외부로 노출되는 서비스가 많지 않다면, Basic Auth를 설정하고 Bitwarden 등의 패스워드 매니저를 사용해 관리하는 것 만으로도 충분하다고 느꼈습니다.\n하지만 나중에 프로젝트가 더 많아진다면?\n그 경우를 해결하기 위한, SSO 인증 방법을 다음 포스트에 올리도록 하겠습니다.\n단, 인증 서버를 DIY로 만들어 전체 소스 코드를 제공해드리기 힘들 것 같아, 일단 사용 가능한 Basic Auth 먼저 설명드림을 양해 부탁드립니다 🙇\n","date":"2024-01-07T12:41:24+09:00","image":"https://blog.lemondouble.com/p/771435ba12d3f38c/cover_hu_283ab82a011b5d60.png","permalink":"https://blog.lemondouble.com/p/771435ba12d3f38c/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 10-1. Basic Auth로 내부용 서비스 보호하기"},{"content":"서론 데이터베이스는 관리하기 까다롭습니다.\n다른 Pod과 다르게 데이터 보관, 백업, 관리에도 신경써야 하고, Failover 및 성능에도 관심을 가져야 합니다.\n그래서 다른 워크로드는 쿠버네티스 클러스터에서 돌리더라도, DB만큼은 AWS RDS등의 관리형 시스템이나 별도 인스턴스를 사용하는 경우도 흔한 것으로 알고 있습니다.\n하지만 그게 중요할까요? 집에서 데이터센터를 차리는데 관리형 DBMS도 하나쯤 있어야 하지 않을까요?\n그러니까 한번 만들어 봅시다. 쿠버네티스의 Operation Pattern을 사용한 CloudNativePG 를 이용해 Primary 하나, Replica 2개짜리 클러스터를 배포해 보고 내부망에서 접속 가능하도록 설정까지 진행해 봅니다.\nPostgres Operator는 아니지만 Mysql Operator로 Kubernetes 환경에서 Mysql DB 운영하기 글을 읽어보시면 따라오는데 좀 더 도움이 될?수도 있습니다.\n2. 설치 설치는 두 단계로 나눠서 진행하겠습니다.\nCNPG Operator 설치 CNPG Cluster 설치 Operator는 Cluster가 정상 상태를 유지하는지 감시하는 역할을 합니다. 실제로 사용하는 데이터베이스 클러스터는 2.를 통해 설치합니다.\n차근차근 시작해 봅시다! 이번에도 ArgoCD를 이용해 빠르게 배포합니다.\n1. CNPG Operator 설치\napps/enabled/cnpg-system.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: cnpg-system namespace: argocd spec: destination: namespace: cnpg-system server: \u0026#39;https://kubernetes.default.svc\u0026#39; source: path: modules/cnpg-system repoURL: \u0026#39;git@github.com:\u0026lt;YourOrganizationName\u0026gt;/\u0026lt;YourRepositoryName\u0026gt;.git\u0026#39; targetRevision: HEAD project: default modules/cnpg-system/cnpg.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # https://github.com/cloudnative-pg/cloudnative-pg apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: cnpg namespace: argocd spec: destination: namespace: cnpg-system server: \u0026#39;https://kubernetes.default.svc\u0026#39; source: repoURL: \u0026#39;https://cloudnative-pg.github.io/charts\u0026#39; targetRevision: 0.19.1 chart: cloudnative-pg project: default 간단하죠? 설치 이후 배포를 진행해 CNPG Operator를 설치해 줍시다.\n2. CNPG Cluster 설치\n저희가 만들 클러스터의 모습은 다음과 같습니다.\n매일 UTC 0시 (KST 기준 아침 9시)에 S3에 Daily Backup을 진행합니다. 총 3개의 Pod으로 구성됩니다. 각 Pod은 여러 노드에 분산되어 혹시 모를 불행을(?) 예방합니다. 192.168.0.x IP로 내부망에서 DB 접근이 가능합니다. 하나하나 시작해 봅시다!\napps/enabled/cnpg-cluster.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: cnpg-cluster namespace: argocd spec: destination: namespace: cnpg-cluster server: \u0026#39;https://kubernetes.default.svc\u0026#39; source: path: modules/cnpg-cluster-16 repoURL: \u0026#39;git@github.com:\u0026lt;YourOrganizationName\u0026gt;/\u0026lt;YourRepositoryName\u0026gt;.git\u0026#39; targetRevision: HEAD project: default modules/cnpg-cluster/cluster.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # https://cloudnative-pg.io/documentation/1.21/quickstart/ apiVersion: postgresql.cnpg.io/v1 kind: Cluster metadata: namespace: cnpg-cluster name: cnpg-cluster spec: instances: 3 superuserSecret: name: superuser-secrets enableSuperuserAccess: true primaryUpdateStrategy: unsupervised # Persistent storage configuration storage: size: 10Gi pvcTemplate: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: longhorn-ssd volumeMode: Filesystem # Backup properties backup: retentionPolicy: \u0026#34;90d\u0026#34; barmanObjectStore: destinationPath: s3://lemon-backup/cnpg-backup s3Credentials: accessKeyId: name: aws-backup-secret key: ACCESS_KEY_ID secretAccessKey: name: aws-backup-secret key: ACCESS_SECRET_KEY wal: compression: gzip 관리 편의를 위해 Superuser 접근을 허용해 주고, DB 용량은 10GB로 주었습니다. (이후 확장 가능합니다.)\n이외에 만약 불의의 사고가 닥쳤을 때(\u0026hellip;) 쉽게 백업할 수 있도록 백업 스토리지를 AWS S3에 백업 파일을 저장하게 설정하고, 최대 90일까지 보관하도록 하였습니다.\nmodules/cnpg-cluster/daily-backup.yaml\n1 2 3 4 5 6 7 8 9 10 apiVersion: postgresql.cnpg.io/v1 kind: ScheduledBackup metadata: namespace: cnpg-cluster name: daily-backup spec: schedule: \u0026#34;0 0 0 * * *\u0026#34; # Daily backupOwnerReference: self cluster: name: cnpg-cluster 간단한 Daily Backup 리소스입니다.\nmodules/cnpg-cluster/lb.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Service metadata: name: cnpg-lb-rw namespace: cnpg-cluster spec: ports: - name: postgres port: 5432 protocol: TCP targetPort: 5432 selector: cnpg.io/cluster: cnpg-cluster role: primary type: LoadBalancer loadBalancerIP: 192.168.0.206 저의 경우 192.168.0.206 주소로 접근을 허용해 주었습니다. 이후 데이터베이스 조회나 관리 시에 내부 망에서 192.168.0.206 로 접근하여 관리가 가능해집니다.\nmodules/cnpg-cluster/sealed-aws-secrets.yaml\n1 2 3 4 5 6 7 8 9 10 apiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: name: aws-secrets namespace: cnpg-cluster annotations: {} spec: encryptedData: ACCESS_KEY_ID: adffd... ACCESS_SECRET_KEY: Aadfads... AWS에서 S3FullAccess, 또는 특정 버킷의 액세스 권한을 준 후 ACCESS KEY, Secret을 발급받은 후, 이전에 설정했던 Sealed Secret을 이용해 등록해 줍니다.\nmodules/cnpg-cluster/sealed-superuser-secrets.yaml\n생성 과정이 조금 복잡합니다!\n1 2 3 4 5 6 7 8 9 apiVersion: v1 kind: Secret metadata: name: superuser-secrets namespace: cnpg-cluster type: kubernetes.io/basic-auth stringData: username: \u0026lt;내가 쓸 ID, b64인코딩 없이 원본 그대로\u0026gt; password: \u0026lt;내가 쓸 패스워드, b64인코딩 없이 원본 그대로\u0026gt; 위와 같은 Secret을 먼저 secret.yaml이란 이름으로 생성 후,\n1 cat secret.yaml | kubeseal --controller-namespace=sealed-secrets-system --controller-name=sealed-secrets -oyaml \u0026gt; sealed-superuser-secrets.yaml 로 Sealed Secret으로 변경 후, Sealed Secret을 사용합니다!\n이후 프로비저닝을 기다리고, (시간이 좀 걸립니다.) 192.168.0.206으로 방금 설정한 ID/Password를 통해 로그인하여 DB를 사용할 수 있습니다!\n정상적으로 설치되었다면, 다음날 알림을 걸어 꼭!! S3 해당 폴더에 정상으로 백업되는지 확인해 주세요!!!\n3.복구 하루가 지나 정상 백업이 되었다면, 꼭! 복원이 되는지 확인 하시길 바랍니다. 날아가고 확인하면 너무 늦어요\u0026hellip;\n제 복원 설정 파일을 공유 드립니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 # https://cloudnative-pg.io/documentation/1.17/quickstart/ apiVersion: postgresql.cnpg.io/v1 kind: Cluster metadata: namespace: cnpg-cluster name: cnpg-cluster spec: instances: 3 superuserSecret: name: superuser-secrets primaryUpdateStrategy: unsupervised bootstrap: #추가 recovery: source: clusterBackup storage: size: 10Gi pvcTemplate: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: longhorn-ssd volumeMode: Filesystem externalClusters: #추가 - name: clusterBackup barmanObjectStore: serverName: cnpg-cluster destinationPath: s3://lemon-backup/postgres-backup s3Credentials: accessKeyId: name: aws-secrets key: ACCESS_KEY_ID secretAccessKey: name: aws-secrets key: ACCESS_SECRET_KEY wal: compression: gzip 신규 클러스터를 위와 같이 bootstrap, externalClusters 옵션을 추가하면 클러스터가 처음 시작될 때, 기존 S3 파일을 이용해 자동으로 데이터를 복구합니다.\n접속/ 확인 후 복구가 끝났으면 bootstrap, externalClusters 옵션을 지운 후 기존의 백업 옵션을 다시 추가해 주시면 됩니다.\n이때 주의할 점으로, Postgres의 메이저 버전이 다른 경우 복구가 정상적으로 되지 않을 수 있습니다.\n예를 들어 제가 1.16버전 Operator를 사용하다 (Postgres 15), Operator를 1.21(기본값으로 Postgres 16 Operator)까지 업데이트를 했다 하더라도 기생성된 클러스터는 수동 업그레이드를 해 주지 않는 이상 PG 15를 유지합니다.\n이 떄, 만약 장애가 생겨 복구를 시도하는 경우 Operator 버전이 1.21 버전이므로 PG16이 프로비저닝 될 것이고, 이 경우 기존 S3에 저장된 데이터가 PG15 데이터이므로 복구가 안 될 수 있습니다.\n이런 경우는 Operator를 설치 당시로 다운그레이드하여 동일 버전의 PG를 올린 후 복구 -\u0026gt; 업데이트를 진행하거나, Image를 설정하여 동일 메이저 버전을 사용하는 법이 있습니다.\n가능하다면, 신규 클러스터를 하나 만들어 정상 복구가 되는지 꼭 확인하고, 이후 다음 내용을 진행하는걸 꼭! 추천드립니다!!\n4. 클러스터 업데이트 마이너 업데이트의 경우는 자동으로 진행되나, 메이저 버전 업데이트시에는 다음과 같이 진행하면 보?통 별문제 없을겁니다. (15 -\u0026gt; 16 진행해 봄)\n온라인 업데이트의 경우는 The Current State of Major PostgreSQL Upgrades with CloudNativePG 를 참고하시고,\n오프라인 업데이트 (다운타임이 발생해도 문제X) 라면\n기존 클러스터에서 pg_dumpall로 데이터 덤프 신규 클러스터 생성 1에서 덤프뜬 데이터를 2에 부음 기존 클러스터를 바라보던 어플리케이션을 신규 클러스터를 바라보도록 변경 테스트 후 기존 클러스터 삭제 5. 미치며 저는 현재 CNPG를 이용해 약 7~8개월간 서버를 정상적으로 운영하고 있습니다.\n제가 청소하다 선을 자주 빼먹는걸 감안하면(\u0026hellip;) 일반적인 상황에서 충분히 견고한 Postgres DB로 사용될 수 있고, 제가 실수로 클러스터 전원을 전부 날렸을 때도 기존 S3에서 간단히 데이터를 복구할 수 있어 신뢰성이 꽤 높은 시스템이라는 생각을 하였습니다.\n물론, 돈의 여유가 있으면 관리형 RDB를 쓰는게 가장 좋은 선택이겠습니다만, 기술증명 겸 이러한 시스템이 있다는 것도 알아주셨으면 감사하겠습니다!\n이 시점에서, 서버 등에 필요한 시스템이 모두 갖춰져 개발을 시작할 수 있게 되셨을 겁니다. 간단한 것부터 하나씩 만들어 봐요.\n그리고 쿠버네티스 내에서 해당 DB는 cnpg-lb-rw.cnpg-cluster 라는 이름으로 접근할 수 있습니다. 예를 들면 jdbc:postgresql://cnpg-lb-rw.cnpg-cluster:5432/my_app 처럼요.\n긴 글 읽느라 수고 많으셨습니다! 다음엔 nvidia-device-plugin을 이용하여 K3S에서 GPU 사용 법을 알아보겠습니다!\n","date":"2023-12-12T22:09:10+09:00","image":"https://blog.lemondouble.com/p/a4ded8cced5f045c/cover_hu_5527b1119fc5ce0d.png","permalink":"https://blog.lemondouble.com/p/a4ded8cced5f045c/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 9. CloudNativePG로 쿠버네티스에서 데이터베이스 운용하기"},{"content":"1. Private Registry는 좋은데, 관리하기가 너무 까다로워요! CLI를 엄청 좋아하는 사람도 있지만.. 저는 커맨드 외우는걸 정말 못 하는 고로.. GUI와 기록을 선호합니다.\nDocker Registry도 CLI로도 관리가 가능하지만, 좀 더 편하게 하기 위해 docker-registry-browser 를 사용합니다.\n이런 느낌의 GUI를 사용할 예정입니다.\n2. 설치 기존 docker-registry-system에 같이 설치합니다.\n기존 파일 밑에 \u0026mdash; 추가 후 작성해 주세요.\nmodules/docker-registry-system/deployment.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 ... 기존 deployment --- apiVersion: apps/v1 kind: Deployment metadata: name: docker-registry-ui-deployment namespace: docker-registry-system spec: replicas: 1 selector: matchLabels: app: docker-registry-ui-app template: metadata: labels: app: docker-registry-ui-app spec: nodeSelector: node-type: worker containers: - name: docker-registry-ui-pod image: klausmeyer/docker-registry-browser:1.7.0 imagePullPolicy: Always ports: - name: http containerPort: 8080 envFrom: - secretRef: name: docker-registry-ui-secrets-sealed-secrets modules/docker-registry-system/service.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ... 기존 service --- apiVersion: v1 kind: Service metadata: name: docker-registry-ui-service namespace: docker-registry-system spec: selector: app: docker-registry-ui-app type: ClusterIP ports: - name: http protocol: TCP port: 80 targetPort: 8080 modules/docker-registry-system/ingress.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 --- apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: docker-registry-ui-ingress namespace: docker-registry-system spec: tls: certResolver: le routes: - kind: Rule match: Host(`UI를 보여줄 URL 예시:docker-ui.lemon.com`) services: - name: docker-registry-ui-service port: 80 추가로 Basic Auth 설정을 진행하려면 Sealed Secrets를 통한 비밀 관리 + Traefik Basic Auth 설정 을 참고해 주세요.\nmodules/docker-registry-system/sealed-docker-registry-ui-secrets.yaml\nsealed secrets GUI에 들어가 다음 Secret을 추가합니다. (참고 Docs Link)\nBASIC_AUTH_USER : registry 로그인에 사용하는 유저명 BASIC_AUTH_PASSWORD : 패스워드 DOCKER_REGISTRY_URL : https://\u0026lt;내가 등록한 Registry 주소\u0026gt; ENABLE_DELETE_IMAGES : true SECRET_KEY_BASE : openssl rand -hex 64 실행 후 나온 값 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: name: docker-registry-ui-secrets namespace: docker-registry-system annotations: {} spec: encryptedData: BASIC_AUTH_USER: afdfads... BASIC_AUTH_PASSWORD: fadvbads.. DOCKER_REGISTRY_URL: dfd.. ENABLE_DELETE_IMAGES: afdgd... SECRET_KEY_BASE: dffdf... 사용법은 직관적이므로, 직접 사용해 보는 것을 추천드립니다!\n3. Registry GC Docker Registry UI에서 이미지를 지우더라도 (혹은 CLI로 이미지를 지우더라도) 즉시 디스크 용량이 줄어들진 않습니다.\n이는 GUI/CLI에서 이미지를 지울 때, 실제로 물리적 삭제가 일어나는 것이 아닌 해당 이미지의 Manifest만을 지우기 때문입니다.\n실제 데이터 삭제는 Registry GC를 명시적으로 수행할 때 일어납니다.\n더 자세한 이야기를 할 수 있지만, 오늘은 일단 사용에 초점을 맞추기에 자세한 이야기는 생략합니다.\n(관련 내용이 좀 더 궁금하시면 투명 셀로판지 이론을 통한 Overlay FS 사용 방법과 유니온 마운트 라는 글을 먼저 읽은 후, Docker registry와 이미지 삭제 등의 키워드로 검색해 보시면 좋을 것 같습니다.)\nRegistry GC 진행하기 !! 주의! GC 진행 도중에는 레포지토리에서 Pull/Push가 일어나지 않아야 합니다. 이를 먼저 확인하고 작업해 주세요!\nkubectl get pod -n docker-registry-system 을 이용하여, Registry의 Pod을 찾아 줍니다. kubectl exec -it pod/\u0026lt;registry-pod-name\u0026gt; -n docker-registry-system -- /bin/registry garbage-collect /etc/docker/registry/config.yml 을 실행하여 GC를 실행합니다. 이후 Longhorn Dashboard에서 Volume 선택 -\u0026gt; Trim Filesystem을 눌러 Actual Size를 재계산해줍니다. 4. 마치며 이를 통해 Docker UI, GC 방법에 대해 알아 보았습니다.\n이제 Private Registry 및 관리 방법이 생겼으니, 마음 편하게 서버를 개발(?) 하셔서 원하시는 서버를 집어넣을 수 있습니다!\n다음 시간에는 CloudNativePG를 이용하여 쿠버네티스 위에서 데이터베이스를 굴리는 방법에 대해 알아보겠습니다.\n","date":"2023-12-10T11:39:46+09:00","image":"https://blog.lemondouble.com/p/a68cc3e1f5e8a95b/cover_hu_dc715f367061dff4.png","permalink":"https://blog.lemondouble.com/p/a68cc3e1f5e8a95b/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 8-1. Docker Registry UI 설정 및 Registry GC 사용법"},{"content":"1. 서론 이왕 인프라 갖춘거 Private Docker Registry도 직접 만들어 봅시다.\n저의 경우는, 이후 이 클러스터에 별도로 개인 서버들을 올려 사용하고 있어 해당 컨테이너를 관리하기 위해 Private Registry를 사용하고 있습니다.\n만약 Dockerhub에 돈을 지불할 의향이 있거나, Public Image로도 충분하다면 이 가이드를 따르실 필요는 없습니다.\n저희는 Private Repo를 사용할 것이기 떄문에, ID,Password 설정도 동시에 진행합니다!\n2. 설치 이번에도 ArgoCD로 빠르게 설치해봅시다!\nmodules/docker-registry-system/pvc.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: longhorn-docker-registry-pvc namespace: docker-registry-system spec: accessModes: - ReadWriteOnce storageClassName: longhorn-ssd resources: requests: storage: 300Gi 저의 경우 ML 서버를 운영해 이미지 사이즈가 큰 관계로, 넉넉하게 300Gi정도 줬는데 필요하시다면 수정하셔서 사용하시면 됩니다.\nmodules/docker-registry-system/service.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Service metadata: name: registry-service namespace: docker-registry-system spec: selector: app: registry type: LoadBalancer ports: - name: docker-port protocol: TCP port: 5000 targetPort: 5000 loadBalancerIP: 192.168.0.202 modules/docker-registry-system/deployment.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 apiVersion: apps/v1 kind: Deployment metadata: name: registry namespace: docker-registry-system spec: replicas: 1 selector: matchLabels: app: registry template: metadata: labels: app: registry name: registry spec: nodeSelector: node-type: worker containers: - name: registry image: registry:2 ports: - containerPort: 5000 env: - name: REGISTRY_HTTP_ADDR value: 0.0.0.0:5000 - name: REGISTRY_AUTH value: htpasswd - name: REGISTRY_AUTH_HTPASSWD_REALM value: docker-registry - name: REGISTRY_AUTH_HTPASSWD_PATH value: /auth/registry.password - name: REGISTRY_STORAGE_DELETE_ENABLED value: \u0026#34;true\u0026#34; volumeMounts: - name: lv-storage mountPath: /var/lib/registry subPath: registry - name: docker-registry-account-htpasswd mountPath: /auth readOnly: true volumes: - name: lv-storage persistentVolumeClaim: claimName: longhorn-docker-registry-pvc - name: docker-registry-account-htpasswd secret: secretName: docker-registry-account-htpasswd docker-registry-account-htpasswd 라는 Secret을 통해 Registry에서 사용할 ID, Password를 지정합니다. 해당 내용은 뒤에서 다루겠습니다.\nmodules/docker-registry-system/sealed-docker-registry-account-htpasswd.yaml\nSecrets 설정이 조금 복잡합니다. Step By Step으로 따라와 봅시다!\nControl Node에서 htpasswd -Bbn \u0026lt;id\u0026gt; \u0026lt;password\u0026gt; 로 Docker Registry에서 사용할 id/password String을 얻습니다. 기존 7-1에서 설정한 kubeseal-webgui를 이용하여 해당 Secret을 암호화합니다. 해당 Secret을 sealed-docker-registry-account-htpasswd.yaml 로 붙여넣기 합니다. 1 2 3 4 5 6 7 8 9 apiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: name: docker-registry-account-htpasswd namespace: docker-registry-system annotations: {} spec: encryptedData: registry.password: dfadf... modules/docker-registry-system/ingress.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: docker-registry-ingress namespace: docker-registry-system spec: tls: certResolver: le routes: - kind: Rule match: Host(`\u0026lt;사용할 subdomain\u0026gt;`) services: - name: registry-service port: 5000 3. 정상 작동 확인 정상 작동이 확인되었다면, 이후 배포합니다!\n배포가 되었다면, 정상적으로 id/password로 로그인이 이뤄지는지 확인합니다.\ndocker가 깔려있는 곳에서 터미널로 다음을 실행합니다.\n1 docker login \u0026lt;my-registry.lemon.com\u0026gt; 이후 id/password를 입력하고, 정상 로그인이 되는지 확인합니다.\n4. Private Registry에서 이미지 받아오는 법 이제 Id/Password가 있는 Private Registry에서 이미지를 Pull 해오는 방법을 알아야겠죠? 그래야 올린 이미지를 쓸 수 있을테니까요.\n제가 사용하는 방법을 일단 올려드립니다. 더 깔끔한 방법이 있을 것 같은데, 혹시 있다면 알려주세요!\n만약 auth-server-system 이라는 namespace에서\nkubectl create secret docker-registry regcred --docker-server=\u0026lt;your-registry-server\u0026gt; --docker-username=\u0026lt;your-name\u0026gt; --docker-password=\u0026lt;your-pword\u0026gt; --docker-email=\u0026lt;your-email\u0026gt; 을 이용하여 default Namespace에 regcred라는 secret을 만듭니다. kubectl get secret regcred --output=yaml \u0026gt; secret.yaml 을 이용하여 secret.yaml에 secret 정보를 저장합니다. 해당 Secret은 어딘가에 백업해 둡니다. kubectl delete secret regcred 를 입력하여 default namespace에 만든 secret을 삭제합니다. 이후 2의 Secret에 Namespace를 수정하여 내가 사용할 namespace로 변경합니다. (예시 : auth-server-system) 1 2 3 4 5 6 7 8 apiVersion: v1 data: .dockerconfigjson: ddfadf... kind: Secret metadata: name: regcred namespace: auth-server-system # 매번 여기를 변경 type: kubernetes.io/dockerconfigjson 이후 cat secret.yaml | kubeseal --controller-namespace=sealed-secrets-system --controller-name=sealed-secrets -oyaml \u0026gt; sealed-regcred.yaml 커맨드를 입력하여 Sealed Secret을 생성합니다. Sealed regcred를 배포시 같이 배포하고, Deployment에 다음과 같이 ImagePullSecret을 추가합니다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion: apps/v1 kind: Deployment metadata: name: auth-server namespace: auth-server-system spec: replicas: 1 selector: matchLabels: app: auth-server template: metadata: labels: app: auth-server name: auth-server spec: nodeSelector: node-type: worker containers: - name: auth-server image: registry.lemon.com/auth_server_repository:2023.12.10.1 imagePullPolicy: Always ports: - name: http containerPort: 80 imagePullSecrets: - name: regcred # 이걸 추가 5. 마치며 축하합니다! 이제 Private Docker Registry 구축을 완료해,\nId/Password를 이용해 내 이미지들을 안전하게 지킬 수 있게 되었습니다!\n이제 다음엔 Web UI를 통해 이미지를 관리하는 법을 알려보고, Docker GC 방법에 대해 알아보겠습니다.\n","date":"2023-12-10T10:08:50+09:00","image":"https://blog.lemondouble.com/p/be354d4b5f50a3ee/cover_hu_2cb4f5e01c37beca.png","permalink":"https://blog.lemondouble.com/p/be354d4b5f50a3ee/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 8. Private Docker Registry 및 접속 ID/PW 설정하기"},{"content":"1. 서론 Sealed Secrets 다 괜찮은데, 문제가 하나 있다면 매번 설정할 떄마다 Secrets.yaml 만들고\u0026hellip; CLI로 Sealed Secrets로 변경한 뒤에.. Git에 추가하는게 끝내주게 귀찮다는 점입니다.\n이걸 좀 더 편하게 하기 위해 kubeseal-webgui를 이용해 간단히 Web UI를 만들어 볼 예정입니다.\n2. 설치 ArgoCD를 이용해 또 간단히 설치해 봅시다!\nmodules/sealed-secrets-system/kubeseal-webgui.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # https://github.com/Jaydee94/kubeseal-webgui apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: kubeseal-webgui namespace: argocd spec: destination: namespace: sealed-secrets-system server: \u0026#39;https://kubernetes.default.svc\u0026#39; source: repoURL: \u0026#39;https://jaydee94.github.io/kubeseal-webgui\u0026#39; targetRevision: 5.1.4 chart: kubeseal-webgui helm: parameters: - name: api.url value: https://\u0026lt;web ui를 사용할 subdomain 예시:seal.lemon.com\u0026gt; - name: sealedSecrets.autoFetchCert value: \u0026#39;true\u0026#39; - name: sealedSecrets.controllerName value: sealed-secrets - name: sealedSecrets.controllerNamespace value: sealed-secrets-system sources: [] project: default modules/sealed-secrets-system/ingress.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: sealed-secrets-ingress namespace: sealed-secrets-system spec: tls: certResolver: le routes: - kind: Rule match: Host(`\u0026lt;web ui를 사용할 subdomain 예시:seal.lemon.com\u0026gt;`) services: - name: kubeseal-webgui port: 8080 필요하다면 Basic Auth는 이전 글을 참고하여 추가합니다.\n이후 해당 UI를 통해 간단히 Sealed Secrets를 만들 수 있습니다!\n","date":"2023-12-10T00:41:05+09:00","image":"https://blog.lemondouble.com/p/567b4c177bdb8401/cover_hu_a03276799be25e54.png","permalink":"https://blog.lemondouble.com/p/567b4c177bdb8401/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 7-1. Web UI를 통해 간편하게 Sealed Secrets 만들기"},{"content":"1. Sealed Secrets? 저희는 지금 GitOps를 하고 있습니다. 즉 Git에 클러스터와 관련된 모든 설정을 올리고 있습니다.\n그런?데? 쿠버네티스 Secret은? 암호화 기능이 없습니다. base64로 디코딩하면 누구나 평문으로 내용을 볼 수 있습니다.\n그러면 Git에?올리면?안?되겠죠?\n이런 문제를 해결하기 위해 나온 것이 Sealed Secrets입니다.\n클러스터는 암/복호화 키를 가집니다. 유저는 Git에 파일을 올리기 전, 해당 클러스터가 가지고 있는 키로 암호화를 합니다. Git에 암호화된 Secret이 올라갑니다. 누구나 볼 순 있겠지만, 복호화는 불가능합니다. 따라서 안전합니다. 이후 ArgoCD등으로 클러스터에 해당 Secret을 다시 불러오면, Sealed Secret 컨트롤러가 암호화를 풀어 자동으로 복호화 해 줍니다. Sealed Secrets Github : Link\n2. Sealed Secrets 추가하기 ArgoCD를 통해 순식간에 배포를 해치워 봅니다!\napps/enabled/sealed-secrets-system.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: sealed-secrets-system namespace: argocd spec: destination: namespace: sealed-secrets-system server: \u0026#39;https://kubernetes.default.svc\u0026#39; source: path: modules/sealed-secrets-system repoURL: \u0026#39;git@github.com:\u0026lt;YourOrganizationName\u0026gt;/\u0026lt;YourRepositoryName\u0026gt;.git\u0026#39; targetRevision: HEAD project: default modules/sealed-secrets-system/sealed-secrets.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: sealed-secrets namespace: argocd spec: destination: namespace: sealed-secrets-system server: \u0026#39;https://kubernetes.default.svc\u0026#39; source: repoURL: \u0026#39;https://bitnami-labs.github.io/sealed-secrets\u0026#39; targetRevision: 2.13.3 chart: sealed-secrets helm: releaseName: sealed-secrets project: default 이후 배포를 진행합니다.\n이후로는, 다음 커맨드를 이용하여 kubeseal 커맨드라인 도구를 설치합니다.\n1 2 3 4 KUBESEAL_VERSION=\u0026#39;0.23.0\u0026#39; #2023.12 기준 최신버전 wget \u0026#34;https://github.com/bitnami-labs/sealed-secrets/releases/download/v${KUBESEAL_VERSION:?}/kubeseal-${KUBESEAL_VERSION:?}-linux-arm64.tar.gz\u0026#34; tar -xvzf kubeseal-${KUBESEAL_VERSION:?}-linux-arm64.tar.gz kubeseal sudo install -m 755 kubeseal /usr/local/bin/kubeseal secret.yaml\n1 2 3 4 5 6 7 apiVersion: v1 kind: Secret metadata: name: mysecret namespace: mynamespace data: users: bGVtb246JGFwcjEkL0ZYczZGam0kMkpJZWNlQy45UWc5QjV5NUw2TzVoMAoK 다음과 같은 커맨드로, Sealed Secrets를 생성 후 등록할 수 있습니다.\n1 cat secret.yaml | kubeseal --controller-namespace=kube-sealed-secrets-system --controller-name=sealed-secrets -oyaml \u0026gt; sealed-secrets.yaml 3. Sealed Secrets 이용하여 Basic Auth 등록해보기 Basic Auth는 엄청 엄청 간단히 말하면, ID Password 로그인입니다.\nLonghorn Dashboard를 외부 인터넷에서도 접근할 수 있게 Ingress를 설정하고, Id/Password 로그인을 붙여 봅시다! (Control01에서 진행)\napt install apache2-utils 실행하여 htpasswd 커맨드를 사용할 수 있도록 합니다. htpasswd -nb \u0026lt;id\u0026gt; \u0026lt;password\u0026gt; | openssl base64 를 이용하여, id,password가 담긴 Secret String을 얻습니다. 아무 텍스트 편집기로 secret.yaml을 생성하고, 다음을 참고하여 Secret을 추가합니다. 1 2 3 4 5 6 7 apiVersion: v1 kind: Secret metadata: name: longhorn-system-basic-auth namespace: longhorn-system data: users: \u0026lt;2에서 얻은 String\u0026gt; 4.cat secret.yaml | kubeseal --controller-namespace=sealed-secrets-system --controller-name=sealed-secrets -oyaml \u0026gt; sealed-secrets.yaml 를 입력하여 sealed-secrets.yaml 을 얻습니다. 5. 다음을 참고하여 ingress를 등록합니다.\nmodules/longhorn-system/ingress.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: longhorn-dashboard namespace: longhorn-system spec: tls: certResolver: le routes: - kind: Rule match: Host(`\u0026lt;원하는 subdomain, 예시:longhorn.lemon.com\u0026gt;`) middlewares: - name: basic-auth namespace: longhorn-system services: - name: longhorn-frontend port: 80 --- apiVersion: traefik.containo.us/v1alpha1 kind: Middleware metadata: name: basic-auth namespace: longhorn-system spec: basicAuth: secret: longhorn-system-basic-auth modules/longhorn-system/sealed-basic-auth-secret.yaml 에는, 방금 생성한 sealed-secrets.yaml을 복사 붙여넣기 해 줍시다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: bitnami.com/v1alpha1 kind: SealedSecret metadata: creationTimestamp: null name: longhorn-system-basic-auth namespace: longhorn-system spec: encryptedData: users: adfasdflavasdlfj... template: metadata: creationTimestamp: null name: longhorn-system-basic-auth namespace: longhorn-system 이후 배포를 진행해 줍니다. 이후 접속하면 정상적으로 id/password 확인 창이 뜨며, 로그인을 하지 않으면 해당 페이지가 보이지 않습니다! 3. 백업 및 복구 이제 우리는 Git에 Secret 같은 민감 정보도 저장할 수 있게 되었습니다!\n근데 다 좋은데, 클러스터가 박살나서 복호화 키를 잃어버리면 우리도 비밀을 풀 수 없게 되어버리겠죠?\n따라서, 암/복호화 키를 반드시 백업해야 합니다!\n백업 방법 kubectl get secret -n sealed-secrets-system -l sealedsecrets.bitnami.com/sealed-secrets-key -o yaml \u0026gt;master.key 를 입력하여 mater.key를 어딘가에 꼭 백업해 둡니다. 복구 방법 방금 백업했던 master.key를 클러스터에 복사하고, kubectl apply -f master.key 를 입력하여 master key를 등록합니다. sealed-secrets-system에 존재하는 Pod 하나를 죽여서 새 Key를 읽게 만듭니다. (ArgoCD에서 간편하게 가능) 4. 마치며 저희는 이제 Sealed Secrets를 통해 Git에 모든 데이터를 저장할 수 있게 되었고,\nTraefik Basic Auth를 이용해 내부 서비스를 외부에 (최소한의 보안을 갖춰) 공개할 수 있게 되었습니다!\n그럼 다음 시간에는, Private Docker Registry를 구축하는 법을 알아보겠습니다!\n","date":"2023-12-10T00:41:05+09:00","image":"https://blog.lemondouble.com/p/1379182f14a62e9b/cover_hu_cfb280d4d1a0ea4b.png","permalink":"https://blog.lemondouble.com/p/1379182f14a62e9b/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 7. Sealed Secrets를 통한 비밀 관리 + Traefik Basic Auth 설정"},{"content":"1. 신규 디스크 추가 및 마운트 해당 내용은 일반적인 SSD / HDD 마운트와 동일합니다! 뭔가 잘 안 된다면, 우분투 하드디스크 마운트 등을 검색하셔서 따라해도 무방합니다! SSD, HDD를 라즈베리 파이에 연결합니다.\nsudo fdisk -l 커맨드를 입력하여, 마운트한 하드 디스크의 이름을 찾습니다. 제 경우 /dev/sda입니다.\nsudo wipefs -a /dev/\u0026lt;방금 확인한 디스크\u0026gt; 를 입력하여 주의하여 디스크를 포맷합니다. (예시 : sudo wipefs -a /dev/sda) sudo mkfs.ext4 /dev/\u0026lt;방금 확인한 디스크\u0026gt; 를 입력하여 디스크를 ext4 포맷으로 만듭니다. sudo blkid -s UUID -o value /dev/\u0026lt;방금 확인한 디스크\u0026gt; 를 입력하여, 해당 디스크의 고유 ID (680dfccb-9d8f-431c-ab3b-8c1e6c86e04f 형태)를 받아옵니다. sudo mkdir /storage-ssd (위치는 자유, 저는 편의상 Root에 /storage-ssd 라는 폴더에 마운트했습니다.) 로 마운트할 폴더를 생성합니다. sudo 권한으로 /etc/fstab을 열어 제일 마지막 줄에 UUID=7cf3fc21-74d6-4c01-a835-f8bd36bc3f7b /storage-ssd ext4 defaults 0 0를 추가합니다. (6번에서 생성한 폴더에 마운트) sudo mount -a를 통해 방금 등록한 디스크를 마운트합니다. 2. 신규 디스크 Longhorn 등록 이제 새로운 디스크를 Longhorn에 등록해 봅시다!\n아까 설정한 로드밸런서 ip (http://192.168.0.201/) 를 브라우저로 들어갑니다. Node -\u0026gt; 해당 Node 선택 -\u0026gt; Edit Node and disks를 선택합니다. disk tag에 StorageClass시 설정한 diskSelector 과 같은 태그를 입력해 해당 디스크가 해당 StorageClass에 속한다는 것을 알려줍니다. 이후 Path를 위에서 설정한 Mount 폴더로 지정합니다. 이후 저장을 누르고, 정상적으로 볼륨이 인식되는지 확인합니다.\n3. 마치며 수고하셨습니다! 이제 Longhorn을 이용해 이후의 어플리케이션에 안정적으로 분산 스토리지 시스템을 공급할 수 있게 되었습니다!\n이 다음으론 Sealed Secrets를 통해 패스워드 등의 민감정보를 Git을 통해 관리하는 법을 알아보고, 그 다음 Private Docker Registry를 구현해 볼 예정입니다.\n감사합니다!\n","date":"2023-12-09T10:48:57+09:00","image":"https://blog.lemondouble.com/p/615befc538a81902/cover_hu_e577cee53da60152.png","permalink":"https://blog.lemondouble.com/p/615befc538a81902/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 6. 분산 스토리지 추가 및 Longhorn을 통한 제공하기"},{"content":"1. GitOps 시작 이제 ArgoCD를 설치하였으니, 리포지토리 구조를 정리해 봅시다.\n저희는 App On Apps 패턴을 이용하여 간결한 배포 방법을 만들어 볼 것입니다.\n이해가 되지 않더라도 일단 따라와 주세요.\n지난 시간에 만든 빈 레포지토리를 다음과 같은 구조로 만들어 주세요.\n1 2 3 4 5 /apps ㄴ/disabled ㄴ/enabled /init /modules 이후, init 폴더에 이전에 작업했던 ip_address_pool.yaml, (argoCD의) ingress.yaml, traefik-update.yaml, values.yaml을 전부 추가합니다.\nGit에 최초 설정 당시의 파일들을 저장해 둠으로써, 이후 클러스터를 복구해야 하는 상황에서 빠르게 해당 설정 파일들을 kubectl -f 파일명 으로 적용하기 위해서입니다.\n아래와 같이 파일들을 추가하고, README.md를 하나 추가해서 지금까지 어떤 작업을 했는지 적어두면, 클러스터가 문제가 생겼을 때 복구시 큰 도움이 됩니다. README.md는 직접 작성해 주세요!\n1 2 3 4 5 /init ㄴ ip_address_pool.yaml ㄴ ingress.yaml ㄴ traefik-update.yaml ㄴ values.yaml 이후, init 폴더 안에 다음 yaml 파일을 참조해 root-app.yaml을 추가합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: \u0026lt;좋아하는 아무 이름 (제 경우는 lemon-k8s-apps)\u0026gt; namespace: argocd spec: destination: namespace: argocd server: \u0026#39;https://kubernetes.default.svc\u0026#39; source: repoURL: \u0026#39;git@github.com:\u0026lt;YourOrganizationName\u0026gt;/\u0026lt;YourRepositoryName\u0026gt;.git\u0026#39; targetRevision: HEAD path: apps/enabled project: default syncPolicy: syncOptions: - CreateNamespace=true 하나하나 뜯어보면\nArgoCD가 관리하는 어플리케이션을 하나 추가 어플리케이션은 Github의 내 레포지토리를 참고 Path는 위에서 설정한 apps/enabled 안에 있는 모든 yaml 파일을 실행 입니다.\n이후 kubectl apply -f root-app.yaml 하여 root app을 설치합니다.\n이후, ArgoCD로 돌아가 내 새로운 첫 어플리케이션이 잘 떴는지 확인합니다. Sync가 실패하고, 리포지토리에 해당 Path가 없다고 나온다면 정상 연결된 것입니다.\n2. Longhorn 설치 메인 USB (OS가 깔려있는 USB) 이외에 최소한 하나 이상의 스토리지용 USB, SSD, HDD가 꽂혀 있다고 가정합니다. 2023.12 기준 ArgoCD로 최초 설치시 Longhorn의 초기 Batch Job이 실패하는 버?그가 있어 수동으로 한번 설치 후, ArgoCD로 이전하는 작업을 진행합니다.\nLonghorn은 분산 스토리지 관리자로써, 다음과 같은 기능을 제공합니다. (읽기 귀찬다면, 간단히 S3와 비슷한 기능들을 제공합니다!)\n분산 스토리지 저장 내 파일을 설정에 따라 N개의 스토리지에 분산 저장합니다.\n만약 어떤 재해로 인해 한 스토리지가 파괴되더라도 해당 스토리지에 있었던 데이터들을 다른 스토리지로 자동으로 이전하여 최소 N개의 복재본이 유지되도록 합니다.\n이를 통해, 만약 SSD 하나가 뻑나더라도 다른 스토리지에 동일한 데이터가 있으므로, 프로그램 및 시스템이 정상 운용되도록 할 수 있습니다.\nStorage Tiering 만약 어떤 시스템은 I/O가 굉장히 헤비한 워크로드라 높은 스토리지 성능을 필요로 하고,\n어떤 시스템은 자주 읽지 않는 파일이 엄청 많아, 스토리지 성능보단 용량이 필요할 수 있습니다.\nLonghorn은 StorageClass를 지원하여, 이를 통해 단계별로 스토리지를 나눠 사용할 수 있습니다.\n예를 들어 I/O가 중요한 경우는 NVME SSD만 모여있는 스토리지 클래스를, 단순 파일 저장이 많이 필요한 경우는 HDD만 있는 스토리지 클래스를 할당하는 식으로 사용합니다.\n따라서! 다음 명령어를 통해 Longhorn을 설치합니다.\n1 2 3 helm repo add longhorn https://charts.longhorn.io helm repo update helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace --set service.ui.loadBalancerIP=\u0026#34;192.168.0.201\u0026#34; --set service.ui.type=\u0026#34;LoadBalancer\u0026#34; 정상적으로 설치되었다면, 브라우저에서 http://192.168.0.201로 들어가서 Longhorn UI가 나오는 것을 확인합니다.\n3-1. ArgoCD를 이용해 설치하기 - 설정파일 추가 이제 설치가 한번 완료되었으니, 기존 설치했던 Longhorn을 ArgoCD로 옮겨보도록 합시다.\n폴더 구조 및 배포 전략은 다음과 같습니다. 아래와 같은 구조로, root-app을 배포하면 하위 앱들이 자동으로 뜨도록 합니다.\n이것도 마찬가지로, 일단 따라해 보도록 합니다. 아래 파일을 추가합니다.\napps/enabled/longhorn-system.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: longhorn-system namespace: argocd spec: destination: namespace: longhorn-system server: \u0026#39;https://kubernetes.default.svc\u0026#39; source: path: modules/longhorn-system repoURL: \u0026#39;git@github.com:\u0026lt;YourOrganizationName\u0026gt;/\u0026lt;YourRepositoryName\u0026gt;.git\u0026#39; targetRevision: HEAD project: default modules/longhorn-system 을 자동 배포하는 어플리케이션을 추가합니다.\nmodules/longhorn-system/longhorn.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: longhorn namespace: argocd spec: destination: namespace: longhorn-system server: \u0026#39;https://kubernetes.default.svc\u0026#39; source: repoURL: \u0026#39;https://charts.longhorn.io\u0026#39; targetRevision: 1.5.3 chart: longhorn helm: parameters: - name: service.ui.loadBalancerIP value: 192.168.0.201 - name: service.ui.type value: LoadBalancer project: default helm.paramters 를 통해 위와 같이, values.yaml에 들어갈 값을 추가할 수 있습니다.\nmodules/longhorn-system/longhorn.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: longhorn-ssd provisioner: driver.longhorn.io allowVolumeExpansion: true reclaimPolicy: \u0026#34;Delete\u0026#34; volumeBindingMode: Immediate parameters: numberOfReplicas: \u0026#34;2\u0026#34; fsType: \u0026#34;ext4\u0026#34; diskSelector: \u0026#34;ssd\u0026#34; --- kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: longhorn-hdd provisioner: driver.longhorn.io allowVolumeExpansion: true reclaimPolicy: \u0026#34;Delete\u0026#34; volumeBindingMode: Immediate parameters: numberOfReplicas: \u0026#34;1\u0026#34; fsType: \u0026#34;ext4\u0026#34; diskSelector: \u0026#34;hdd\u0026#34; ssd, hdd 스토리지를 추가합니다. 만약 USB 등을 사용하고 계시다면, 이름을 longhorn-usb 등으로 변경하셔도 됩니다.\nnumberOfReplicas : 복제본의 개수입니다. 저의 경우 SSD는 어플리케이션에 필요한 데이터 저장용이니 안정성을 위해 최소 2개, HDD는 단순 파일 저장용이라 1개로 설정했습니다. fsType : 파일시스템의 타입입니다. ext4를 사용할 예정입니다. 가이드를 따라오신다면, 동일하게 설정하면 됩니다. diskSelector : 이후 Disk 등록시 사용합니다. 기억해 두도록 합니다.\n작업을 완료하셨다면, Git commit 후 Push합니다!\n3-2. ArgoCD를 이용해 설치하기 - 실제 배포하기 이제 초기 설정했던 ArgoCD에 들어가 직접 배포를 진행합니다.\n저희가 알아야 하는 것은 Sync, Refresh 버튼입니다.\nSync : Git에서 새로운 클러스터 상태를 가져온 후, 실제 클러스터 상태를 Git과 동기화합니다. Refresh : Git에서 새로운 클러스터 상태를 가져옵니다. 클러스터를 동기화하진 않습니다. 따라서, Sync를 누르면 ArgoCD가 배포를 시작합니다.\n이후 Longhorn-system -\u0026gt; Longhorn을 차례차례 타고 가 Sync 버튼을 눌러 배포를 진행합니다.\n마치며 고생하셨습니다! 드디어 지금까지 중구난방으로 있던 Yaml 파일들을 정리하고, GitOps를 향한 첫 발을 내딛었습니다.\n이제 다음 시간에는 실제 SSD, HDD를 Longhorn을 통해 등록해 보겠습니다.\n","date":"2023-12-09T09:16:13+09:00","image":"https://blog.lemondouble.com/p/107eed6a453d6238/cover_hu_e81655c3b9d6606a.png","permalink":"https://blog.lemondouble.com/p/107eed6a453d6238/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 5. ArgoCD GitOps 설정 및 Longhorn 설치"},{"content":"만약, 시스템을 Git을 통해 관리하면 어떨까요? 내가 필요한 컨테이너 종류, 개수 등을 전부 Git에 선언하고 Git에 선언되어 있는 그대로 클러스터가 운영된다면, 굉장히 편하지 않을까요?\n굳이 클러스터 이곳저곳을 확인하지 않더라도, Git에 올라와 있는 설정값만 확인하면 될 테니까요.\n또한, 만약 배포 도중 문제가 생겼다면 바로 Revert해버릴 수 있지 않을까요? 또, 여러 사람이 같은 클러스터를 관리하면서 생기는 문제도 우리가 항상 하는 PR을 통해 관리할 수 있을 거구요.\n마지막으로, 만약 클러스터에 갑작스럽게 문제가 생겨 모든 노드가 다운되어 버렸더라도, 선언한 상태값이 Git에 있으므로 다른 클러스터를 통해서라도 빠르게 복구를 진행할 수 있을 겁니다.\n그러한 방법을 GitOps라고 부르며, ArgoCD는 Git에 선언된 상태와 클러스터의 상태를 똑같이 맞춰주는데 도움을 주는 툴입니다.\n이후 배포해야 할 시스템이 정말 정말 많아서, 제일 먼저 이러한 GitOps를 도와주는 ArgoCD를 먼저 설치합시다.\n또한 ArgoCD 외부 접속이 가능하도록, Traefik의 LetsEncrypt 설정 등을 같이 진행해 봅시다.\nTraefik Letsencrypt Certification 적용 참고한 글 : HTTPS on Kubernetes Using Traefik Proxy Control Node에서 다음을 참고하여 traefik-update.yaml을 하나 만들어 줍니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # https://traefik.io/blog/https-on-kubernetes-using-traefik-proxy/ apiVersion: helm.cattle.io/v1 kind: HelmChartConfig metadata: name: traefik namespace: kube-system spec: valuesContent: |- ports: web: redirectTo: port: websecure priority: 10 additionalArguments: - \u0026#34;--log.level=INFO\u0026#34; - \u0026#34;--certificatesresolvers.le.acme.email=yourMail@gmail.com\u0026#34; - \u0026#34;--certificatesresolvers.le.acme.storage=/data/acme.json\u0026#34; - \u0026#34;--certificatesresolvers.le.acme.tlschallenge=true\u0026#34; - \u0026#34;--certificatesresolvers.le.acme.caServer=https://acme-v02.api.letsencrypt.org/directory\u0026#34; 이후 kubectl apply -f traefik-update.yaml을 이용해 클러스터에 적용합니다.\n이후, IngressRoute에서 CertifiactionResolver로 le(letsEcnrypt)를 사용할 수 있게 됩니다. (무슨 이야긴지 모르겠다면, 진행하다 보면 이해하게 됩니다.)\nArgocD 설치 ArgoCD를 설치하고, Github SSO를 연동해 봅시다.\nDNS 설정을 진행합니다. ArgoCD를 외부에서 접근시 사용할 서브도메인을 현재 IP 주소와 연결해 줍니다.\n내 IP 보기 에 들어가서 내 IP를 확인합니다. DNS를 구매한 곳에 가서 A 레코드로 IP4 Address에 방금 확인한 내 IP를 입력합니다. Github Organization을 하나 생성하고, 적절한 Role을 하나 만듭니다. 저의 경우는 개인용 Organization에 Admin이란 Role을 추가했습니다.\nOrignization -\u0026gt; Settings -\u0026gt; Developer Settings -\u0026gt; OAuth Apps에 가서 새로운 OAuth 앱을 하나 추가합니다.\n이때, Authorization callback URL을 https://\u0026lt;방금 설정한 도메인\u0026gt;/api/dev/callback 으로 설정합니다. 나머지는 자유롭게 바꾸셔도 상관없습니다.\n이후 Client ID, Secret 을 발급 후 기억합니다.\nvalues.yaml 파일 생성 다음 파일을 참고하여 values.yaml 파일을 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 configs: params: # Traefik이 Https 연결 해 주므로 Insecure 모드로 가동 server.insecure: true rbac: # Built-in Policy : https://github.com/argoproj/argo-cd/blob/master/assets/builtin-policy.csv policy.default: role:readonly policy.csv: | g, \u0026lt;Github Organization 이름\u0026gt;:\u0026lt;생성한 Role 이름\u0026gt;, role:admin server: config: url: https://\u0026lt;설정한 도메인 이름\u0026gt; dex.config: | connectors: - type: github id: github name: github config: clientID: \u0026lt;방금 발급받은 Client ID\u0026gt; clientSecret: \u0026lt;방금 발급받은 Client Secret\u0026gt; orgs: - name: \u0026lt;Github Organization 이름\u0026gt; 예를 들어 다음과 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 configs: params: # Traefik이 Https 연결 해 주므로 Insecure 모드로 가동 server.insecure: true rbac: # Built-in Policy : https://github.com/argoproj/argo-cd/blob/master/assets/builtin-policy.csv policy.default: role:readonly policy.csv: | g, LemonLemon:Admin, role:admin server: config: url: https://argo.lemon.com dex.config: | connectors: - type: github id: github name: github config: clientID: vadf45a6sdfad clientSecret: adsfvasd45f6ads4f56asd7 orgs: - name: LemonLemon ingress.yaml 파일 생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: argocd-ingress namespace: argocd spec: entryPoints: - websecure routes: - kind: Rule match: Host(`\u0026lt;설정한 도메인 이름\u0026gt;`) priority: 10 services: - name: argo-cd-argocd-server port: 80 - kind: Rule match: Host(`\u0026lt;설정한 도메인 이름\u0026gt;`) \u0026amp;\u0026amp; Headers(`Content-Type`, `application/grpc`) priority: 11 services: - name: argo-cd-argocd-server port: 80 scheme: h2c tls: certResolver: le 아래는 예제입니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 apiVersion: traefik.containo.us/v1alpha1 kind: IngressRoute metadata: name: argocd-ingress namespace: argocd spec: entryPoints: - websecure routes: - kind: Rule match: Host(`https://argo.lemon.com`) priority: 10 services: - name: argo-cd-argocd-server port: 80 - kind: Rule match: Host(`https://argo.lemon.com`) \u0026amp;\u0026amp; Headers(`Content-Type`, `application/grpc`) priority: 11 services: - name: argo-cd-argocd-server port: 80 scheme: h2c tls: certResolver: le values.yaml과 ingress.yaml을 작성했다면, 다음 커맨드를 입력하여 argocd를 설치합니다. 1 2 3 4 5 kubectl create namespace argocd helm repo add argo https://argoproj.github.io/argo-helm helm install -n argocd argo-cd argo/argo-cd -f values.yaml kubectl apply -f ingress.yaml 잠깐 기다린 후, 설정한 도메인 (제 경우는 https://argo.lemon.com) 로 들어가면 정상적으로 ArgoCD가 반겨줍니다.\n이후 Log in Via Github 버튼을 클릭해 로그인합니다.\n로그인이 성공했다면 정상적으로 설치된 경우입니다!\n※ Tip : 401 에러가 난다면, 한번 argocd 네임스페이스의 argocd-server 팟을 한번 죽여보세요\nArgoCD Git Private Repo 연결 만약 클러스터를 Public Repo로 사용한다면 이 설정은 생략해도 되겠지만, 혹시 모를 사고를 방지하기 위해 Git에서도 private Repo를 사용하고, ArgoCD에서도 연결해서 사용해 봅시다.\nGithub에서 새로운 레포지토리 (빈 레포지토리) 를 하나 추가한 후, 해당 레포지토리를 ArgoCD와 연동해 봅니다.\nArgoCD 로그인 후 Settings -\u0026gt; Repository -\u0026gt; Connect Repo를 누릅니다. connection method via ssh, Name은 아무거나, Proejct는 default, Repository URL은 git clone시 ssh 주소를, SSH private key data는 Control01 노드의 ~/.ssh/id_rsa 를 복사해서 붙여넣어 줍니다. Git을 연결합니다. 마치며 오늘은 Traefik Ingress 설정, ArgoCD 설치 및 Git 연동까지를 진행해 보았습니다.\n다음 포스트에서는 지금까지 어질러 놨던(?) yaml 파일을 정리하고, App of Apps를 이용하여 리포지토리 구조를 정리하고, 만약 장애가 나더라도 클릭 한 번으로 복구할 수 있는 시스템을 만들어 볼 예정입니다.\n언제나 그렇듯, 틀린 내용이 있다면 알려 주세요!\n","date":"2023-12-06T05:21:00+09:00","image":"https://blog.lemondouble.com/p/f10efbc0bcc4c537/cover_hu_7d17ba20ad4b15cc.png","permalink":"https://blog.lemondouble.com/p/f10efbc0bcc4c537/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 4. Traefik https 인증 및 ArgoCD 설정"},{"content":"Helm 설치 참고한 글 : Installing Helm 다음 커맨드를 차례로 입력하여 Helm을 설치합니다.\n1 2 3 curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 chmod 700 get_helm.sh ./get_helm.sh 이후, kubeconfig 파일을 설정해 줍니다.\n1 2 cp /etc/rancher/k3s/k3s.yaml ~/.kube/config chmod 600 ~/.kube/config 이후 bashrc 제일 밑에 다음 한 줄을 추가한 후, source ~/.bashrc 로 Shell 설정을 즉시 적용시킵니다.\n1 export KUBECONFIG=~/.kube/config Helm은 쿠버네티스의 패키지 매니저와 비슷한 역할을 합니다.\n예를 들어서, 제가 Kafka를 클러스터 모드로 설치하려고 한다고 가정해 봅시다.\n제가 일일히 Kafka를 쿠버네티스에 띄우려면, Kafka Container 이미지를 가져와서 클러스터 옵션을 키고, 내부 네트워크 연결을 해 주고\u0026hellip; 하는 작업을 전부 해 줘야 할 것입니다.\n또한, Kafka 버전 업그레이드를 한다고 상상을 해 봅시다.\n우리가 작성한 Kafka 클러스터 Yaml 파일에서 컨테이너 이미지 버전을 올리고.. 만약 클러스터 설치를 하면서 다른 리소스를 가져다 썼다면 호환이 되는지 확인하고\u0026hellip; 등등등의 복잡한 작업이 기다리고 있을 것입니다.\n하지만 누가 이 작업을 미리 해 두었다면?\n그것을 Helm Chart 라고 부르고, 저희는 Helm install Kafka 만 입력하면 이미 네트워크, 클러스터 설정이 완료된 카프카 클러스터를 얻을 수 있게 됩니다.\n근데 만약 모두 같은 Helm Chart를 사용한다면, 실험용으로 Kafak 컨테이너 한 대만 띄워보기를 원하는 우리같은(?) 사람과, Kafka 수백대가 클러스터를 이뤄야 하는 큰 회사가, 서로 같은 설정을 사용할 수 밖에 없을 것입니다.\n이런 경우를 대비해, Helm및 Chart는 Values.yaml 이라는 추가 설정 파일을 제공합니다.\n즉, 만약 내가 컨테이너 한 대만 필요하다면 차트의 values.yaml 을 잘 읽고, kafka.container-count : 1 과 같이 설정하면 해당 변수가 주입됨으로써, Helm Chart를 내가 필요한 설정을 넣어 유연하게 사용할 수 있는 식입니다. (마치 서버의 환경 변수 같이요!)\nMetalLB 설치 참고한 글 : Installation With Helm 이제 Helm을 사용해 로드밸런서인 MetalLB를 설치해 봅시다!\n다음 커맨드를 순차적으로 입력합니다. 별다른 말이 없다면 모든 커맨드는 Control Node에서 입력합니다.\n1 2 helm repo add metallb https://metallb.github.io/metallb helm upgrade --install metallb metallb/metallb --create-namespace --namespace metallb-system --wait 이후 ip-address-pool.yaml 이란 파일을 하나 생성해, 다음과 같이 작성하고 kubectl apply -f ip-address-pool.yaml 을 입력하여 적용시킵니다.\n아래는, 로드밸런서 등의 IP가 필요한 서비스에 192.168.0.200~192.168.0.250 사이의 IP를 할당해 주겠다는 뜻입니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: ipaddresspool namespace: metallb-system spec: addresses: - 192.168.0.200-192.168.0.250 --- apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: default namespace: metallb-system spec: ipAddressPools: - ipaddresspool 이제, 정상적으로 설정되었다면 LoadBalancer를 사용하는 서비스에 IP가 할당됩니다.\nkubectl get svc -n kube-system 를 입력해 traefik의 EXTERNAL-IP 부분이 192.168.0.xxx 를 할당받았는지 확인합니다.\n아마 별도로 설정하지 않았다면, 192.168.0.200을 할당받았을 것입니다.\n공유기 포트 포워딩 설정 Traefik의 IP를 확인했다면, (다음 시간에 해야 할 일을) 하나만 미리 해 두도록 합니다.\n공유기의 포트포워딩 설정을 찾아, 80, 443번 포트를 위에서 확인한 Traefik의 IP로 포트포워딩 해 줍니다.\nMetalLB는 어떻게 작동하나요? MetalLB는 ARP 모드와 BGP 모드 두가지로 작동할 수 있으나, 저희는 ARP 모드를 사용하므로 ARP 모드의 작동 방법에 대해 아주 간단히(?) 알아보도록 하겠습니다.\n이미지를 보기 전에, 배경 지식이 약간 필요합니다.\nMac Addr는 하드웨어에 박혀있는 주소로, 공장 출고시부터 있으며 고유합니다. 공유기(라우터)의 포트포워딩은, 특정한 포트로 외부 요청을 받았을 시 내부망의 특정 IP로 변환해 요청을 하는 역할을 해 줍니다. 예를 들어 위에 설정한 80번 포트, 443번 포트에 192.168.0.200으로 설정을 해 두었다면, http(80포트), https(443포트) 에 대해 내부망에서 192.168.0.200 을 가진 노드를 연결해 줍니다. 이후의 네트워크 요청 흐름은 다음과 같습니다.\nhttps:// 로 서버에 접근하면, 443 포트를 외부에서 요청하게 됩니다. 저희가 위에서 설정한 포트 포워딩 설정으로 인해, 라우터는 192.168.0.200에게 트래픽을 전달해야 합니다. 그런데 라우터는 현재 192.168.0.200 주소를 모르므로 내부망의 모두에게 192.168.0.200 나와라 는 요청을 뿌립니다. (ARP Request) 그러면 Traefik이 있는 Node가 헉 전데염;; 하고 자기 Mac Address를 담은 응답을 보냅니다. 이로 인해 라우터는 외부에서 들어온 트래픽을 AA-AA-AA-AA로 보내면 된다는걸 알게 됩니다. 따라서 이제 서로 트래픽 전달이 가능하니, 통신이 가능하게 됩니다. 헷갈리는 점 그러면? 헷갈리는 부분이 생깁니다. 하나하나 대략 정리해보면\n아니 그러면 한 노드가 IP가 여러개를 가질 수 있나?\n예를 들어, 아까의 예시라면 AA-AA-AA-AA는 이미 DHCP 서버로부터 내부 IP 하나를 할당받았을 것입니다. 그러면 여기에 추가로 IP를 할당받는 건가? 라고 생각하면 맞습니다! MetalLB 시스템에선 한 노드가 여러개의 내부 IP를 할당받을 수 있습니다. 만약 Traefik이 있는 노드가 죽으면 어떻게 돼요?\n당연히? 잠깐 동안 접근이 불가능하게 다운됩니다. 대신 Traefik이 다른 노드에 뜰 거고, 그 뒤 새로 ARP Request/Response를 진행하면 새로운 Mac Address(Node)로 포워딩되므로 장애가 복구되게 됩니다. ARP Response는 누가 해주는거에요?\nMetalLB 컨테이너가 ARP Response 및, 해당 주소로 들어온 트래픽을 Pod (여기서는 Traefik) 으로 운반해주는 역할을 합니다. 마치며 해당 내용은 문서를 찾아보며 한 독자연구입니다. (특히 MetalLB 쪽은)\n혹시 틀린 게 있다면 언제든지 수정해주시면 감사하겠습니다!\n","date":"2023-12-06T05:20:00+09:00","image":"https://blog.lemondouble.com/p/06a98c88458a5fb7/cover_hu_60e6fa7122007133.png","permalink":"https://blog.lemondouble.com/p/06a98c88458a5fb7/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 3. Helm 설치 및 MetalLB로 로드 밸런서 설정하기"},{"content":"하드웨어 위 그림을 참고하여, 하드웨어 구성을 시켜 줍니다.\n컴퓨터를 사용하는 집이라면 아마 높은 확률로 라우터와 기본 데스크탑은 설정되어 있을테니, 위 도식에서 초록색 부분만 추가하시면 됩니다.\n인터넷에서 스위치허브 (Unmanaged Switch)를 구매하셨다면, 단순히 라즈베리 파이의 랜선을 전부 스위치허브에 연결하고, 공유기에서 인터넷 선을 하나 들고와서 스위치에 연결하면 됩니다.\n라즈베리 파이 설정 이전의 제가 발행한 글인 내가 보려고 올리는 라즈베리 파이 초기 세팅 과, SD카드 세개 날려먹고 화나서 쓰는 라즈베리파이 USB Boot 세팅 방법, 혹은 타 블로그의 글을 참고하여 라즈베리 파이에 우분투 서버 22.04를 설치해 줍니다.\nSD카드로도 클러스터 구축은 가능하나, USB Boot 설정을 한 후 USB로 부팅하는것을 강력하게 권장 드립니다.\n여기까지 오셨다면, 라즈베리 파이 Ubuntu 설치 + SSH 연결이 가능하다고 가정합니다.\n이후, sudo apt update -y \u0026amp;\u0026amp; sudo apt upgrdae -y \u0026amp;\u0026amp; sudo apt install linux-modules-extra-raspi 로 필요한 종속성 설치를 진행합니다. (라즈베리 파이 + Ubuntu 22.04에 K3S 설치해서 쓰기 참고)\nDHCP 서버 설정 이후, 최초 한번 모든 파이를 켜 주고 나면 파이가 내부 IP를 할당받게 됩니다. (설정을 바꾸지 않았다면, 192.168.0.X 형태)\n별도로 설정하지 않았다면, 이 내부 주소는 파이의 전원이 켜질 때마다 남는 IP 중 하나 를 가져와서 설정하게 되는데, 이 경우 IP가 유동적으로 바뀌어 매번 별도로 노드의 IP를 공유기 관리자 페이지 등에서 확인해야 하는 번거로움이 있습니다.\n(정확히는, 바뀔 수도 안 바뀔 수도 있습니다. 대부분 그대로 유지하려고 하긴 하지만, 만일을 위해 미리 세팅해 둡니다.)\n위와 같이 DHCP 서버 설정 (Iptime 기준) 과 비슷한 메뉴를 찾아, 최초 연결된 파이의 Mac Addr (1A-2B-3C\u0026hellip;) 를 입력한 후, 특정 IP를 할당해 줍시다.\n저는 편의상 192.168.0.20(Control Node), 192.168.0.21~30(Worker Node)로 설정하였으나, 해당 IP는 편하신 대로 설정하면 됩니다.\n이후 Windows 사용자라면 Putty를 이용해, Linux나 Mac 사용자라면 ssh CLI를 통해 연결 준비를 합니다.\nSupabase 가입 및 Postgres 접속 권한 가져오기 Control Node를 세대나 할당하기엔 자원이 너무 아깝고, 1대만 할당하자니 만약 Control Node가 죽었을 시 복구가 어려워서 선택한 차선책으로,\n저희는 만약 Control Node가 죽더라도 State를 외부 SaaS에 보관하여, 만약 Control Node가 죽더라도 Control Node를 교체하면 외부 Storage에서 상태를 가져와 사용할 수 있도록 구축할 예정입니다.\n이를 위해서 무료 DB를 제공하는 Supabase 를 사용합니다.\nSupabase 에 가입한 후, Postgres 접속 권한을 설정 후 가져옵니다.\n설정할 수 있다면, Supabase는 PgBouncer도 지원하니, PgBouncer도 설정하면 좋습니다.\n(동시 연결 수 제한 떄문에, 이후 Portainer를 설치하면 정상 작동은 하나 PgBouncer 없이는 에러 메세지가 많이 뜹니다.)\nMatser Node 설정하기 위 설정을 따라와 Ubuntu가 설치되어 있다고 가정합니다.\n다음을 준비합니다.\nMater Node로 사용할 라즈베리 파이의 IP (위 DHCP 서버 설정에서 설정한 IP) 예시 : 192.168.0.20 형태, 이후 Worker Node 연결에 필요하므로, 반드시 기록해 둡니다. 쿠버네티스 클러스터가 서로 Join할때 사용할 랜덤 토큰을 준비합니다. 저는 특문 없이 알파벳 대소문자+숫자 조합으로 60글자짜리 Random String을 사용했습니다. 이후 Worker Node 연결시 필요하므로, 반드시 기록해 둡니다. 예시 : jUofPu4hMegXLDKgCn6FbsfWcL8mFQu3DYEiyDjQgh8447cMAqwfgYwTeNU4 위에서 준비한 Postgres 엔드포인트를 준비합니다. 예시 : postgres://postgres:postgresPassword@db.mydb.supabase.co:5432/postgres 1 curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644 --disable servicelb --token \u0026lt;2,myToken\u0026gt; --node-taint CriticalAddonsOnly=true:NoExecute --bind-address \u0026lt;1,myControlNodeIP\u0026gt; --disable local-storage --datastore-endpoint \u0026lt;3, postGresEndpoint\u0026gt; 1,2,3에 맞춰 다음과 같이 설정합니다. 아래는 예시입니다.\n1 curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644 --disable servicelb --token mytoken --node-taint CriticalAddonsOnly=true:NoExecute --bind-address 192.168.0.20 --disable local-storage --datastore-endpoint postgres://postgres:postgresPassword@db.mydb.supabase.co:5432/postgres 하나하나 옵션을 까보면 다음과 같습니다.\ncurl -sfL https://get.k3s.io | sh -s - : K3S를 설정합니다. --write-kubeconfig-mode 644 : KubeConfig 파일의 접근 권한을 644로 설정합니다. 추후 설정에 필요합니다. --disable servicelb : K3S의 기본 Service Load Balancer (klipper-lb) 를 비활성화합니다. 이후 MetalLB라는 로드밸런서를 별도로 깔아 사용할 예정입니다. --token \u0026lt;mytoken\u0026gt; : K3S 클러스터가 서로 Join할때 쓰는 토큰을 설정합니다. --node-taint CriticalAddonsOnly=true:NoExecute : Control Node에 Taint를 추가합니다. 즉, 안정성을 위해 정말 중요한 컨테이너를 제외하곤 Control Node에서 실행하지 않도록 합니다.. 만약 노드가 한대거나, 상관없이 Control Node에서도 컨테이너를 실행하고 싶다면 해당 옵션을 제거해 주세요. --bind-address \u0026lt;myControlNodeIP\u0026gt; : 마스터 노드를 특정 IP에 종속시키는 플래그입니다. --disable local-storage : K3S의 기본 Local Storage를 비활성화합니다. 이후 Longhorn Storage를 별도로 설치해 사용할 예정입니다. --datastore-endpoint \u0026lt;postGresEndpoint\u0026gt; : 내장 Etcd 대신 외부 Database를 상태 저장소로 사용합니다. 위 커맨드를 Master Node에서 실행하면 쿠버네티스가 설치되기 시작합니다.\n이후, 편의를 위해 Hosts 파일을 수정합니다.\nsudo vi /etc/hosts 를 실행 후 (어떤 텍스트 편집기로 열어도 괜찮습니다., 아래와 같은 줄을 추가합니다.\n1 2 3 4 5 192.168.0.20 control01 control01.local 192.168.0.21 worker01 worker01.local 192.168.0.22 worker02 worker02.local 192.168.0.23 worker03 worker03.local 저의 경우 192.168.0.20을 control01이란 닉네임으로, 21을 worker01, 22를 worker02라는 닉네임으로 사용했습니다.\n이렇게 설정하면, 이후 control Node에서 ssh 접속시 ssh worker01 식으로 간단히 접속이 가능해집니다.\n마지막으로, Master Node에서 쉽게 Worker Node로 접근하기 위해, SSH 설정을 진행합니다.\nssh-keygen 명령어를 입력하고 아무것도 치지 말고 엔터를 입력해 ssh key를 만듭니다. 이후 ~/.ssh/id_rsa.pub 파일을 잘 복사해 둡니다.\nWorker Node 설정하기 위 설정을 따라와 Ubuntu가 설치되어 있다고 가정합니다.\n먼저 SSH 접속부터 설정합니다.\n~/.ssh/authorized_keys에, 한 줄 줄바꿈 후 방금 복사한 id_rsa.pub의 내용을 붙여넣기 합니다.\n이후 control Node에서 ssh worker01 등을 입력하여 연결이 잘 되는지 확인합니다.\n만약 연결이 잘 되지 않는다면, ssh authorized_keys 등록 등으로 검색하여 해결합니다.\n이후 K3S를 설치합니다.\n다음을 준비합니다.\n위에서 설정한 Master Node의 IP 위에서 설정한 Master Node의 Token 이후 아래 커맨드를 입력하여 Worker Node를 Join시킵니다.\n1 curl -sfL https://get.k3s.io | K3S_URL=https://\u0026lt;1,myControlNodeIP\u0026gt;:6443 K3S_TOKEN=\u0026lt;2,myToken\u0026gt; sh - 1,2에 맞춰 위와 같이 설정합니다. 아래는 예시입니다.\n1 curl -sfL https://get.k3s.io | K3S_URL=https://192.168.0.20:6443 K3S_TOKEN=mytoken sh - 이후 Node Join을 확인합니다.\n노드 Join 확인 및 Label 생성 이후 Control Node에서 kubectl get nodes 를 입력하여 node들이 정상 Join한 것을 확인합니다.\n이후, ROLES 부분을 수정하기 위해 Control Node에서 각 Node에 대해 아래 커맨드를 설정합니다.\n만약 Node가 적다면 적은 개수만큼, 더 많다면 더 많이 해 주시면 됩니다.\n1 2 3 kubectl label nodes worker01 kubernetes.io/role=worker kubectl label nodes worker02 kubernetes.io/role=worker kubectl label nodes worker03 kubernetes.io/role=worker 이후, 컨테이너 생성시 특정 Pod (Worker Node) 에만 뜨도록 설정을 위한 Label을 추가합니다. (위와 아래 둘 다 해 줘야 합니다!)\n1 2 3 kubectl label nodes worker01 node-type=worker kubectl label nodes worker02 node-type=worker kubectl label nodes worker03 node-type=worker 최종 점검을 진행합니다.\n아래는 제 경우의 출력입니다. 여러분은 아직 ROLES 부분이 다를 수 있습니다. (kubectl get nodes)\n1 2 3 4 5 NAME STATUS ROLES AGE VERSION control01 Ready control-plane,master 307d v1.27.4+k3s1 worker02 Ready worker 307d v1.27.4+k3s1 worker01 Ready worker 307d v1.27.4+k3s1 worker03 Ready worker 307d v1.27.4+k3s1 또한, 추가한 라벨도 확인합니다. (kubectl get nodes --show-labels)\n여러 개의 라벨이 있는게 당연하니, 우리가 추가한 node-type=worker 가 잘 있는지만 확인하면 됩니다.\n마치며 축하합니다! 하드웨어 및 쿠버네티스 설치를 성공적으로 마쳤습니다.\n긴 설정인 만큼, 진행이 막히는 부분이 있으면 편하게 댓글로 알려주세요!\n","date":"2023-12-03T22:20:29+09:00","image":"https://blog.lemondouble.com/p/f4efced2e8d5b1d8/cover_hu_952c2074be0c120d.png","permalink":"https://blog.lemondouble.com/p/f4efced2e8d5b1d8/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 2. 클러스터 초기 설정 편"},{"content":"첫 시간은 하드웨어 시간입니다! 간단한 것부터 조금 복잡한 이야기부터, 빠르게 시작해 봅시다!\n1. SBC (Single Board Computer) 역시 제일 중요한건 SBC겠죠?\n개인적으로는 당X마켓에 알림 걸어놓고, Raspberry PI 4B 4GB 모델 or 8GB 모델이 올라올 때마다 하나씩 맞추시는걸 추천합니다.\n레퍼런스가 가장 많고, 소프트웨어 유지보수 및 테스트가 잘 진행되며, 중고로 구입하는 경우 가격적 디메리트도 어느정도 상쇄가 가능합니다.\n참고로 현재 적정가는 8~10만원 사이인 것 같습니다. (2023.12 서울 기준)\n참고로 라즈베리 파이는 ARM 아키텍쳐를 사용하다 보니, 일부 X86만 지원하는 프로그램을 굴리는 데 어려움을 겪을 수 있습니다. (대표적으로 셀프호스트 Sentry 가 있습니다.)\n경제적 여유가 되고, 이런 경우를 피하고 싶다면 Intel NUC나 알리익스프레스발 N100 탑재 Mini PC 도 좋은 선택이 될 수 있지 싶습니다.\n2. 케이스 라즈베리 파이 기준으론, 사진과 같은 타워 케이스를 추천드립니다.\n공간을 효율적으로 사용할 수 있고, 가격이 싸고, 결정적으로 선 정리를 하면 이뻐서 기분이 좋습니다 선 정리 및 관리하기가 용이합니다.\n단, 기본 구매시 딸려오는 쿨링 시스템이 부실하므로 별도의 추가 쿨링이 필요합니다.\n위 사진과 같은 5V 팬을 클러스터 노드 중 하나에 꼽아서 사용하거나, (5V 팬 라즈베리 파이 라고 검색하면 자료가 많이 나옵니다.) 핸디 선풍기 등을 옆에 놔둬주는걸 추천합니다.\n3. 전력 공급기 라즈베리 파이 4B 모델을 기준으로, 한 컴퓨터에는 15W (5V 3A)의 전원 공급이 필요합니다. (Link, A 15W로 검색)\n따라서, 파이 하나당 15W의 전력을 필요로 하고, 만약 4대의 클러스터를 구성한다면 최대 60W 출력이 필요합니다.\n따라서, 예를 들어 위와 같은 충전기의 USB A타입 (아래 4개) 에 병렬 연결하면 요구 용량보다 공급 용량이 적어 충전기가 과부하가 걸리게 됩니다.\n5V 3A를 공급해 주는 충전기가 시중에 많지 않아 (대부분 2.1A 또는 2.4A Max입니다.) 전용 충전기를 사는게 아니라면 어느정도 과부하가 걸리게 되고, 경험상 1A 차이 정도는 큰 문제가 없었지만, 멀티 충전기에 4대를 연결하는 등의 연결은 자제하도록 합시다.\n멀티 충전기에서 5V 3A.. 아니 5V 2.4A를 여러 포트에 공급해 주는 멀티 충전기도 판매하는 곳이 없으므로, 만약 전원 공급 선을 정리하고 싶다면 SMPS(Switching Mode Power Supply) 라는 전력 공급기를 사는 것이 제일 안전한 방법일 것입니다. 이 내용은 나중에 기회가 된다면 포스팅하겠습니다.\n결론적으로, 전력 공급은\n멀티 충전기를 사용한다면, 총 전력량을 잘 보고, 지나치게 과부하되지 않도록 선을 분배하기 (6포트 충전기가 있다고, 6개에 다 꼽으면 안 됩니다!) 아니라면 2.1V짜리 1구 충전기를 여러개 사서 꼽기 정도로 일단 정리해 볼 수 있을 것 같습니다.\n4. SSD와 USB UASP(USB Attached SCSI Protocol) 라즈베리 파이는 별도로 Sata 선이나 NVME 슬롯이 없어 USB를 통해 SSD를 연결하게 됩니다.\n이 때, SATA3 을 사용하는 SSD의 전송 속도는 최대 6Gbps이고, USB 3.0의 전송 속도는 최대 5Gbps 입니다. (물론, 이론적인 속도이며 실제로는 이것보다 훨씬 덜 나옵니다.)\n그런데, USB3.0에서 UASP라는 기능을 지원하지 않는 경우, USB 3.0이 데이터를 주고받는데 비효율적인 방법으로 주고 받아 (자세히 설명하면 너무 길어지므로 생략..), UASP(USB Attached SCSI Protocol) 라는 프로토콜을 지원하는 케이블/커넥터를 사용해야 전체 쓰루풋을 손실 없이 사용할 수 있습니다.\n그런데 몇천원짜리 싼 저렴이 선들은 이 UASP 프로토콜을 지원하지 않아, 구입하시기 전에 UASP 프로토콜을 지원하는 프로토콜인지 확인이 필요합니다.\n저의 경우는 새로텍 FHD-260U3을 사용하고 있으며 (바이럴 아님), Aliexpress의 Ugreen사의 케이블도 괜찮다는 평을 듣고 있는 것으로 알고 있습니다.\n아래는 제 테스트 결과입니다.\n같이 보면 좋은 게시글 : UASP makes Raspberry Pi 4 disk IO 50% faster 그리고 SSD의 경우는, 적당히 핫딜 떴을 때 SATA3 사용하는 SSD로 아무거나 주워오시면(?) 됩니다\n5. 네트워크 장비 네트워크 구성은 이후 시스템 구축시 이야기할 기회가 많으니, 장비만 간단하게 설명하면\n스위치허브로는 저는 적당히 ipTIME H6008-IGMP를 사용하였는데, 1Gbps 이상을 지원하는 스위치허브라면 아무거나 구매하셔도 상관없을 것 같습니다.\n마찬가지로, 랜선도 Cat5e 이상(아무거나 사면 보통 Cat5e 이상입니다.) 으로 사면 문제 없을 것 같습니다.\n마치며 1편에선 안정적인 클러스터를 구축하기 위한 하드웨어에 대해 간단히 알아봤습니다.\n혹시라도 잘못된 부분이나, 수정할 부분이 있다면 알려주시면 감사하겠습니다!\n","date":"2023-12-03T14:54:33+09:00","image":"https://blog.lemondouble.com/p/20ae99fa945edf10/cover_hu_7ae3205a7884dab0.png","permalink":"https://blog.lemondouble.com/p/20ae99fa945edf10/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 1. 하드웨어 편"},{"content":"처음 라즈베리 파이 클러스터를 구축한 지 대략 300일이 지났습니다. 계기는 사실 그렇게 크진 않았습니다.\n저는 매일 아침 GeekNews 라는 뉴스 큐레이팅 서비스를 보며 출근하는데, 그 중 Chick-Fil-A 의 Edge Computing 기술 아키텍처 : Enterprise Restaurant Compute 라는 글이 눈에 보였습니다.\n그 뒤로 칙필레도 쿠버네티스를 굴리는데, 서버 엔지니어로써 나도 관리하는 클러스터 하나쯤 있어야 하지 않을까? 가 모든 일의 시작이었습니다..\n구성 환경 제 옥탑방(\u0026hellip;) 클러스터는 대략 이렇게 생겼습니다. 구성 스펙을 정리해보자면,\nControl Node Raspberry PI 4b+ 8GB Model * 1 Samsung MUF-AB FIT PLUS 64GB USB ARM Worker + Storage Node Raspberry PI 4b+ 8GB Model + 500GB SSD(PNY CS900 500GB) * 3 Sandisk USB Ultra Fit USB 3.1 32GB * 3 GPU X86 Worker Node (For ML) Ryzen 5600x + 64GB DDR4 3200 RAM + GTX3090 + NVME SSD(WD Black) 1TB + WD RED Plus 4TB HDD 과 같이 구성하여 총 라즈베리 파이 4대, X86 GPU 서버 1대로 X86, ARM 혼합 클러스터를 운영하고 있습니다.\n라즈베리 파이의 디스크 I/O는 안정성을 위해 SD카드 대신 USB 디스크를 사용하고 있습니다.\n어? Control Node가 1대밖에 없으면 HA 못하는거 아니에요? 첫 클러스터 구축 시의 제일 큰 고민이었습니다.\n3대 이상을 HA로 구성하면 안전하지만, PI의 연산능력이 그렇게 많지 않다는 걸 감안했을 때, 가능한 낭비되는 컴퓨팅 파워를 줄이고 싶었습니다. 그 결론으로, Master Node는 하나로 둔 뒤, 아래와 같은 안전장치를 마련하고 있습니다.\netcd 대신 External Database를 상태 저장소로 사용하고 있습니다. 현재는 Supabase 제공 Postgresql을 외부 저장소로 사용 중입니다. Master Node는 Taint를 걸어 Job scheduling이 불가하게 설정해 두었습니다. 이를 통해, 작업 할당 도중 마스터 노드가 갑작스럽게 사망하거나 하는 일을 줄였습니다. Mater Node에는 조금 더 신뢰성있는 장비를 사용 중입니다. (USB, 쿨러 등) 그럼에도 불구하고 Matser Node가 여름에 과열로(..) USB가 사망한 적이 있으나, 모든 State가 외부 DB에 존재하므로 10분 이내로 복구할 수 있었습니다.\n현재 소프트웨어적으로 구축한 내용 K3S와 External DB를 이용하여, 만약 Master Node가 다운되어도 복구할 수 있도록 시스템을 구축합니다. ArgoCD와 Github를 이용하여, GitOps 시스템을 구축합니다. Mend Renovate 를 이용하여, 설치한 Helm Chart 및 Private Registry의 새 버전이 나오면 자동으로 PR을 생성하고, 클러스터를 최신 상태로 유지합니다. Longhorn을 이용하여 분산 스토리지를 구현, 만약 SSD 하나가 나가더라도 복구할 수 있는 시스템을 구축합니다. Docker Private Registry를 구축하고, Docker-registry-browser 를 이용하여 업로드된 이미지를 확인할 수 있는 GUI를 구축합니다. Sealed-secrets를 이용하여 Secret을 외부 서비스 없이 Git에 저장하여 관리합니다. 또한 Kubeseal-webgui 를 이용하여 GUI를 이용하여 Secret을 편하게 추가할 수 있는 화면을 만듭니다. kube-prometheus-stack 을 이용하여 모니터링 시스템을 구축하고 관리합니다. CloudNativePG 를 이용하여 HA Postgres 데이터베이스 시스템을 구축하고, 혹시 모를 사고를 방지하기 위해 AWS 등에 자동 백업 시스템을 구축합니다. Portainer 를 이용하여 웹 UI를 기반으로 클러스터 관리를 수행하는 시스템을 만듭니다. MetalLB 설정을 통해 내부망에서만 접근 가능한 엔드포인트, 서비스를 쉽게 구축합니다. nvidia-device-plugin 을 이용하여 쿠버네티스 상에서 컨테이너가 CUDA 등의 디바이스를 사용할 수 있도록 합니다. traefik-forward-auth 라이브러리의 아이디어를 차용하여, 자체 인증 시스템을 구축합니다. 비밀번호 이외에 SSO 로그인을 통해 내부 관리 시스템에 접근 가능하도록 합니다. 앞으로 쓸 글 앞으로는, 제가 구축한 내용을 복기하며 하드웨어부터 소프트웨어, 시스템 구축까지의 내용을 시간 날 때마다 하나씩 추가할 예정입니다. 위는 순서 없이 나열한 내용이며, 글을 적다 보면 내용이 변경될 수 있습니다.\n덧붙여, 해당 구축 가이드는 2023.12 기준으로 제가 직접 따라가며 정상 동작을 확인한 방법입니다.\n서론을 마치며 클러스터를 구축하며 관련 자료를 열심히 찾아봤는데,\n의외로 이런 자료가 그렇게 많지 않단 것을 알게 되었습니다. VladoPortos 님의 Kubernetes with OpenFaaS on Raspberry Pi 4 자료가 도움이 많이 되었지만, 국내 자료가 그리 많지 않은 것이 아쉬웠습니다.\n미약하나마, 이 가이드가 비슷한 길을 개척하려는 분께 도움이 되길 바랍니다.\n","date":"2023-12-03T13:24:05+09:00","image":"https://blog.lemondouble.com/p/d42387934c57aa25/cover_hu_63a855ce346128f9.png","permalink":"https://blog.lemondouble.com/p/d42387934c57aa25/","title":"집에서 라즈베리 파이 클러스터로 데이터센터 차리기 - 서론"},{"content":"IoT는 꽤 재밌는 주제인데도,\n입문 난이도가 높음 + 잘 알려지지 않은 주제라 이걸로 뭘 할 수 있는지도 잘 알려지지 않은 것 같습니다.\n저는 여러 플랫폼 중 HomeAssistant(이하 HASS)라는 플랫폼을 직접 설치해서 사용 중인데요, 왜 이런 플랫폼이 필요하고, 우리의 삶에서 뭘 개선시킬 수 있는지 이야기를 하려고 합니다!\n1. HomeAssitant가 뭔가요? (이하 HA, Hass) 여러 회사의 IoT 디바이스를 한 곳에서 통합 관리하게 해 주는 통합 플랫폼입니다.\nDemo 를 확인해 보세요!\n2. 이걸로 뭘 할 수 있나요? 프로그래밍을 할 수 있다면, 문자 그대로 모든 걸 할 수 있습니다! 여러분의 관심에 따라? 할 수 있는 예시를 몇 개 들어볼겠습니다.\n자동화에 관심이 있다면? 예시 시나리오 : 화장실 불 끄기가 너무 귀찮아요 위와 같이, 스마트 전등과 모션 감지 센서를 설치해서\n아래와 같이 자동화 할 수 있습니다!\n1 2 3 4 5 6 7 8 9 10 // 실제 코드는 아니고, 의사 코드입니다. // 자동화 1. 화장실 등 자동 키기 if(화장실_모션_센서.Occupancy transfer to True){ turn_on (화장실_등) } // 자동화 2. 화장실 등 자동 끄기 if(화장실_모션_센서.Occupancy transfer to False \u0026amp;\u0026amp; 바뀐지_10분_지남){ turn_off(화장실_등) } 실제로 hass 화면에선 다음과 같이 자동화를 하게 됩니다. AI에 관심이 있다면? 예시 시나리오: 음성 비서 만들기 그런데 정말 뭐든 할 수 있는.. 회로 DIY에 관심이 있다면? ESPHome 같은 프로젝트를 이용해 보세요! ESPxx 과 같은, 와이파이와 CPU가 달린 개발 보드를 몇 천원이면 구매할 수 있습니다!\n아래와 같은 환기 등 만들기 시나리오는 어떠세요?\n매번 버스 시간 확인하기 귀찮다면?\n공공 API를 이용해서 버스 알림이를 만들어보는건 어떨까요?\n이외에도, HomeAssistant라는 플랫폼을 거쳐 더 많은 일들을 할 수 있습니다!\n완전히 오픈 소스라, 필요하다면 고칠 수 있단것도 덤이구요!\n한번 즐거운 IoT, 같이 해 보시지 않을래요?\n","date":"2023-05-21T09:40:47+09:00","image":"https://blog.lemondouble.com/p/107dcfce6eca0dd7/cover_hu_9998594ffdd59735.png","permalink":"https://blog.lemondouble.com/p/107dcfce6eca0dd7/","title":"HomeAssistant로 IoT 한번 해 보지 않을래요?"},{"content":"요즘 심심할 때 논문을 하나씩 읽는 취미가 생겼습니다. 그 중, arxiv에서 재밌는 논문을 하나 읽게 되어서, 정리하고 공유해 보려고 합니다.\n논문 제목은 ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models (Arxiv Link) 입니다.\n주제는, 대규모 언어 모델(ChatGPT) 는 상식적인 문제를 잘 풀 수 있는가? 입니다. 꽤 재밌는 주제라서, 제가 공부하는 겸 공유하는 겸, 겸사겸사 읽은 내용을 정리해 보려고 합니다.\n제가 논문을 읽어 본 적도 없고, 학부 기억도 가물가물 해서 틀릴 수 있음을 미리 알려드립니다! (틀린거 있음 알려주세요 헤헤)\n1. 요약 논문은 다음과 같은 질문을 던집니다.\nGPT는 상식에 관한 질문에 효과적으로 답할 수 있는가? GPT는 상식에 대해 잘 알고 있는가? GPT는 특정 질문에 답하기 위해 필요한 상식과 아닌 상식을 잘 구분할 수 있는가? GPT는 주어진 상식을 효과적으로 활용하여 질문에 답할 수 있는가? 그리고 이에 대한 간략한 답은 다음과 같습니다.\n효과적으로 답할 수 있으나, 사회규범, 인과관계, 시간과 관계된 특정 분야에 대해선 잘 답하지 못한다. 대부분의 상식을 잘 알고 있다. 질문에 대답하는데 밀접한(필요한) 상식과 아닌 상식을 잘 구분하지 못 한다. 문맥상에서 추가적인 상식을 입력해 주더라도, 잘 사용하지 못 한다. 2. 상식의 분류 연구진은 상식을 8가지 카테고리로 분류했습니다. 각각 다음과 같습니다.\n1. 일반(General) 상식 (널리 공유되는 상식)\n해는 동쪽에서 뜬다 2. 물리적(Physical) 상식 (물리적 세계에 대한 지식)\n유리컵은 떨어지면 깨진다. 물은 아래로 흐른다 3. 사회적(Social) 상식 (사회규범에 대한 지식)\n도움을 받았으면 감사합니다~ 라고 해야 한다. 4. 과학(Science) 상식 (과학 개념의 원리/지식)\n중력은 모든 물체를 지구 중심으로 끌어당긴다 5. 인과 관계(Event) 상식 (인과 관계 및 순서에 대한 지식)\n물컵을 넘어트리면 -\u0026gt; 물이 쏟아진다 6. 숫자와 관련된(Numerical) 상식 (숫자와 관련된 상식)\n사람 손가락은 2개고 손가락은 10개 7. 전형적인(Prototypical) 상식 ( 개념에 대한 지식)\n제비는 새의 일종이며, 날개가 있다 8. 시간과 관련된(Temporal) 상식\n해외여행은 산책보다 시간이 오래 걸린다. 그리고, 연구진은 각 카테고리별로 상식에 관련된 데이터 셋 11개를 준비했습니다.\n왼쪽부터 데이터셋 이름 / 카테고리 / 예시 질문 입니다.\n3. ChatGPT는 상식 질문에 효과적으로 답할 수 있을까? 연구진은 위 데이터셋마다 100개씩 문제를 뽑아 GPT3, GPT3.5, ChatGPT 모델에 물어봤습니다.\n아래는 GPT3, GPT3.5, ChatGPT의 각 질문에 대한 정답 비율입니다.\n주목할 점은 다음과 같습니다.\n전체적으로 잘 답변함 과학 (Science) 분야는 약 94%로 가장 높은 정답률을 보임. 사회규범(Social), 인과관계(Event), 시간 (Temporal) 에 대한 정답률은 저조 (70% 미만) 전체적으로 GPT 3.5 모델에 비해, 튜닝 모델인 ChatGPT의 정확도가 높음 일부 질문에 대해 GPT 3.5가 정답률이 높은 것으로 나와있으나, 이는 ChatGPT가 \u0026ldquo;주어진 데이터만으로는 정답을 알 수 없습니다\u0026rdquo; 라는 답변이 나와 생긴 착시에 가까움. 4. ChatGPT는 질문에 답변하기 위해 필요한 상식과 아닌 상식을 잘 구분할 수 있을까? 연구진은 위 질문 데이터셋에서, 각 데이터셋마다 20개를 다시 뽑았습니다.\n이후, GPT에게 \u0026ldquo;이 질문에 답변하기 위해 어떤 지식이 필요한지\u0026rdquo; 물어본 뒤, 인간의 답변을 바탕으로 Precision(정확도), Recall(재현율), F1 Score를 구했습니다.\nPrecision? Recall? 어려운 말이 나오니까 한번 정리하고 들어갈게요.\nPrecision : GPT가 낸 정답의 정확도\nGPT의 답변 중 정답은 몇 %인가? 예를 들어 5개의 답변 중, 3개는 정답 2개는 오답이라면 Precision은 60% Recall : 전체 정답 중 GPT가 찾아낸 정답의 개수\n사람이 예측한 정답 중, GPT가 몇 개나 찾아냈는가? 예를 들어 예측 정답이 5개고, GPT가 답변 10개 중 4개는 정답, 6개는 오답이라면 Recall은 80% (4/5) Recall을 계산할 때 오답은 중요하지 않음! F1 Score : Precision과 Recall의 조화평균\nPrecision만 지표로 삼으면? : 확률 제일 높은 답을 딱 하나만 하면 됨 Recall만 지표로 삼으면? : 답변을 10000개씩 쓰면 그 중 몇개는 얻어걸림 하지만 우리가 원하는 건 \u0026ldquo;정확한 답을\u0026rdquo; \u0026ldquo;충분히 많이\u0026rdquo; 해 주는 AI 모델 따라서 Precision과 Recall의 조화평균을 내면 \u0026ldquo;정확도\u0026rdquo; 비슷한 지표가 나옴! 즉, F1 Score가 높음 == 답변을 잘 했다! 어려운 이야기를 했는데! 결론적으로, 다음과 같은 결과를 얻었습니다.\nGPT는 Precision은 높지만 Recall은 높음 전체 데이터셋에서 Precesion은 **55.88%**지만, Recall은 84.42% 즉, 질문을 해결하는데 필요한 지식을 거의 대부분 알려주지만, 정확도가 높진 않음 GPT는 과학 분야에선 비교적 성능이 좋지만, (F1 74~76%), 사회규범, 시간 분야에서 특히 성능이 낮음(F1 50% 이하) 5. ChatGPT는 상식에 박식할까? 연구진은 3. 에서 생성된, 필요한 지식을 기반으로 질문 프롬프트를 수동으로 만들었습니다. 이후, GPT가 생성한 답변이 정답인지 오답인지를 수동으로 평가했습니다.\n위 질문의 경우는 ChatGPT가 생성한 오답의 예시입니다.\nGPT는 blowing into a trash bag and tying it with a rubber band may create a cushion-like surface, but it is unlikely to be durable or comfortable for prolonged use as an outdoor pillow (번역 : 쓰레기 봉투에 바람을 불어 고무줄로 묶으면 쿠션같은 표면을 가지지만, 야외용 배개로 오래 쓰기엔 내구성이 떨어지거나 편하지 않을 것) 이라 했지만, 쓰레기 봉투로 배개를 만드는 건 흔한 관행이므로 오답으로 처리했다고 합니다.\n위 결과 표에 따라, 연구진은 다음과 같은 결과를 얻었습니다.\nGPT는 박식하며, 질문에 답변하기 위한 대부분의 상식을 알고 있음. 전체 데이터셋에서 평균 82.66%의 정확도를 보임 대부분의 데이터셋에서 70% 이상의 정확도를 보임. 단, 사회규범 영역에서 54.92%로 성능이 낮음. 하지만, GPT의 답변 중 오해의 소지가 있거나, 지나치게 일반화되어 필요없는 지식도 들어가 있음 전체 답변의 26.25%에 관련성이 낮고 오해의 소지가 있는 정보가 포함되어 있음. 15% 정도의 설명이 지나치게 일반화되어, 질문 답변에 필요한 구체적 정보가 아님 6. ChatGPT는 응답시 대화 맥락에서 추가된 상식을 활용하여, 답변에 활용할 수 있을까? 연구진은 GPT가 문맥에서 나온 상식을 이용하여 답변에 활용할 수 있는지 확인하기 위해, 4.에서처럼 GPT에게 질문에 답변할 때 필요한 상식을 추론하게 한 후, 같은 질문에 다시 답하게 하여 답변이 평가되는지 확인했습니다.\n위 예시는, 이전에 오답이었던 답변이 추가 설명을 생성한 뒤에도 변경되지 않는 예시를 보여줍니다.\n위 결과 표에 따라, 연구진은 다음과 같은 결과를 얻었습니다.\nGPT는 GPT가 생성한 상식 설명을 대화 맥락 추가하는 것 만으로는, 효율적으로 답변에 활용할 수 없음. 맥락에 설명이 추가되는 경우, 오답이 정답으로 바뀌는 경우 (C-\u0026gt;W) 및 정답이 오답으로 바뀌는 경우 (W-\u0026gt;C)가 모두 존재 Social IQA 데이터셋의 경우, 맥락에 추가된 상식이 잘못되어 오히려 정답이 오답으로 바뀌는 것이(5개) 오답이 정답으로 바뀌는 것보다(1개) 많았음. 연구진은 이미 Model에 생성된 지식이 있어, 단순히 추가 정보를 생성하는 것이 큰 효과가 없는 것으로 추정함. 심지어, 대화 맥락에 \u0026ldquo;GPT가 생성한 상식\u0026rdquo; 이 아닌, \u0026ldquo;유저가 직접 올바른 상식\u0026quot;을 추가해줘도 정답률이 100%가 되지 못 했음.\n사람이 주석을 단 CoS-E, ECQA 데이터셋을 바탕으로 대화 맥락에 정답 (Golden Knowledge) 를 추가함. CoS-E는 오답-\u0026gt;정답이 4개 증가 ECQA는 오답-\u0026gt;정답이 8개, 정답-\u0026gt;오답이 1개 증가 연구진은 복잡한 추론 (예를 들어, 부정 추론) 등을 활용할 수 있는 능력이 부족하다고 추론 부정 추론의 예시 Q: 농구공에 구멍이 났지만, 모양이 그대로라면 틀린 것은? A: 구멍이 뚫렸다, B:미국에서 인기가 있다, C: 공기가 가득 차 있다 CoS-E 데이터셋은 \u0026ldquo;구멍이 뚫린 물체에는 공기가 머무를 수 없다\u0026rdquo; 라고 설명했지만, GPT는 여전히 A를 예측 // TODO : 더 쓰기..\n","date":"2023-03-30T19:15:11+09:00","image":"https://blog.lemondouble.com/p/dedda5393e239315/cover_hu_97ae19b8167b6cd0.png","permalink":"https://blog.lemondouble.com/p/dedda5393e239315/","title":"ChatGPT는 상식 문제를 잘 풀 수 있을까? (논문)"},{"content":"라즈베리 파이는 다 좋은데 SD 카드 부팅이 굉장히 화가 날 때가 많습니다..\n삽질한다고 SD카드 빼다가 어디 긁어먹어서 인식이 안 되면 굉장히.. 심란한 경우가 많습니다.\nSD카드나 USB나 비슷한 기술 쓰는건 알고 있지만..\n경험상 USB가 그나마 안정성이 더 나은 경우가 많았습니다. (파손 이슈)\n그러니 SD카드를 던져버리고 USB로 부팅시킵시다!\n아래는 Ubuntu 22.04 Server 환경에서 작성되었습니다.\n1. SD카드 굽기 일단 USB로 부팅을 시키려면 부트로더를 손봐야 하는데, 그러려면 일단 SD카드로 한번은 부팅을 해야 합니다.. SD카드 굽고, SSH 연결 합시다. 이후 커맨드는 전부 터미널에서 진행됩니다. 2. eeprom의 Booloader 업데이트 EEPROM : 컴퓨터구조 시간에 배웠던 ROM인데, 대충 여기에 쓰고 지울 수 있는 ROM이라 생각하시면 됩니다. 여따가 Bootloader를 올려놓고 쓰는데, 대충 2020년 9월? 이후 버전의 부트로더부터 USB 부팅을 지원합니다. 겸사겸사 업데이트를 해 줍시다. Ref : https://www.raspberrypi.com/documentation/computers/raspberry-pi.html#automaticupdates\nsudo rpi-eeprom-update 커맨드를 쳐서 현재/최신 버전의 Bootloader를 확인합니다. sudo rpi-eeprom-update -a 커맨드를 쳐서 Bootloader 업데이트를 예약합니다. sudo reboot 으로 재부팅하면 부팅하는 동안 부트로더 업데이트가 진행됩니다. sudo rpi-eeprom-update 커맨드를 쳐서 업데이트가 잘 됐는지 확인합니다. Update (2023.05.19) 위 rpi-eeprom-update 는 RPI4 부터 지원합니다. RPI 3B 이하를 사용 중이라면, Booting Raspberry Pi 3 B With a USB Drive 를 참고해 주세요. Raspbian os를 사용하면 해당 가이드를 그대로 따라하면 됩니다. Ubuntu를 사용 중이라면, config.txt는 boot/firmware/config.txt 를 수정하셔야 합니다!!! 3. 부팅 순서 정리 Ubuntu Server에는 raspi-config 프로그램이 없으므로 일단 설치합니다. sudo apt install raspi-config 입력해서 raspi-config 프로그램을 설치합니다. sudo raspi-config 커맨드 입력 후 Advanced Option -\u0026gt; Boot Order 에 들어갑니다. USB Boot을 선택하고, sudo reboot으로 재부팅합니다. 4. 부팅 순서 적용 잘 됐는지 확인 vcgencmd bootloader_config를 입력합니다. 아래에 BOOT_ORDER=0xf14 라고 세팅되어있는지 확인합니다. 세팅되어있지 않다면 뭔가 설정 중 꼬였으니 1~3을 반복합니다. Ref : https://www.raspberrypi.com/documentation/computers/raspberry-pi.html#configuration-properties 5. USB에 OS 구운 뒤 꼽고 재부팅 SD카드를 빼고, USB에 OS를 굽고 재부팅합니다 와! 부팅이 됩니다. 굿! ","date":"2023-01-29T17:31:39+09:00","image":"https://blog.lemondouble.com/p/977ba9d70a2a7029/cover_hu_b222421dfe760fa6.png","permalink":"https://blog.lemondouble.com/p/977ba9d70a2a7029/","title":"SD카드 세개 날려먹고 화나서 쓰는 라즈베리파이 USB Boot 세팅 방법"},{"content":" k get pod \u0026lt;podname\u0026gt; -o 또는 k describe pod \u0026lt;podname\u0026gt;: 상태 정보 자세히 확인 k logs -f \u0026lt;podname\u0026gt; -c \u0026lt;container-name\u0026gt; : 로그 확인 k get pod \u0026lt;podname\u0026gt; --show-lables : Label 정보 확인 1. Pod 하나 이상의 Container의 집합\n실행의 최소 단위\n특징\n여러 Container로 구성되었다면, 항상 동일 Node에 할당 고유 Pod IP 할당 (Pod당 하나) Pod 내의 Container들은 네트워크 공유, 따라서 Localhost로 접근 가능 Volume 공유 Pod이 가질 수 있는 State\nPending : Master Node에 생성 명령은 전달되었지만 생성되지 않은 상태 ContainerCreating : 특정 Node에 할당되어 생성 중 (이미지 다운로드 등..) Running : 정상 실행 중 Completed : 한번 실행되고 종료되는 Pod (배치 작업 등..) 작업이 완료된 경우 Error : Pod에 문제 발생한 경우 CrashLoopBackOff : 지속적으로 에러가 발생해 Crash가 반복되는 상태 spec.restartPolicy\nAlways : 종료시 항상 재시작 (default) Never : 재시작 X OnFailure : 실패한 경우에만 재시작 (정상 종료되면 재시작되지 않음) 상태 확인\nlivenessProbe : Pod이 정상적으로 동작하고 있는지 확인 readnessProbe : Pod이 생성되고 난 후, 트래픽을 받을 준비가 완료되었는지 확인 (READY 0/1 상태로 확인 가능) 2. Service Reverse Proxy 생각하면 쉬움. Pod은 불안정한데 그 앞에 서서 안정적인 Endpoint 제공\nDNS와 비슷하게 Service 이름으로 연결 가능. myservice라는 service를 생성했다면 myservice라는 이름으로 연결 가능\nService의 DNS\n\u0026lt;ServiceName\u0026gt;.\u0026lt;Namespace\u0026gt;.svc.cluster.local 예시 : default Namespace의 myservice -\u0026gt; myservice.default.svc.cluster.local svc.cluster.local은 K8S 기본값 Service는 어떻게 DNS를 가질까?\nk exec client -- cat /etc/resolve.conf 로 Pod의 DNS 설정 확인 -\u0026gt; Nameserver의 IP 확인 가능 k get svc -n kube-system 입력하면 나오는 CoreDNS가 해당 IP CoreDNS라는 서비스 통해 DNS 사용 가능 Service의 종류\nClusterIP : (default) K8S 클러스터 내부에서만 접근 가능. NodePort : Localhost의 특정 Port를 Service의 특정 포트와 연결 (Docker 생각하기) 단, Nodeport는 모든 Node의 특정 포트를 해당 Service로 연결 예를 들어 32000번 Nodeport가 있다면, master:32000, worker01:30002 전부 32000번 포트로 연결됨 LoadBalancer : Load Balancer 생성하여(Cloud 제공, 혹은 Software) Pod에 분산 외부에선 LoadBalancer의 IP만 알면 되니 편하다. (ClusterIP -\u0026gt; Pod의 안정적 서비스 Endpoint, LoadBalancer -\u0026gt; Node의 안정적 서비스 Endpoint) External-IP를 통해서 외부에서 접근 가능 ExternalName : 외부 서비스를 K8S 네트워크 내에서 사용하고 싶을 때 ex : google.com을 ExternalName으로 google-svc 로 설정하면 curl google-svc 로 google.com의 데이터를 불러올 수 있다. 3. Controller Current State와 Desired State를 비교해 일치하도록 Task를 반복\nReplicaSet\nPod을 복제해 일정 개수를 유지시킴. spec.replicas : 2 와 같이 복제할 Pod의 개수를 선언 가능 selector.matchLables 를 통하여 label이 Match되는 Pod을 선택 가능 Deployment\nReplicaSet을 이용하여 일정 개수의 Pod을 생성, 거기에 배포 기능을 추가 (Deployment -\u0026gt; ReplicaSet -\u0026gt; Pod) Rolling Update 지원, 업데이트 되는 Pod의 비율 조절 가능 spec.strategy.type : RollingUpdate 로 Update 전략 설정 spec.strategy.rollingUpdate.maxUnavailable : 25% : (소숫점 내림) 배포중 최대 중단 가능 Pod의 % 지정. 10개에 25%라면 2개 Pod spec.strategy.rollingUpdate.maxSurge : 25% : (소숫점 올림) 배포중 최대 초과 가능 Pod의 % 지정. 10개에 25%라면 3개 Pod Update History 저장 및 Rollback 가능 k rollout undo deployment \u0026lt;Deployment이름\u0026gt; StatefulSet\nPod마다 고유한 식별자 존재 Pod마다 고유한 데이터를 스토리지에 보관 가능 (예시 : Database) Pod생성의 순서 보장 -\u0026gt; 순서에 민감한 어플리케이션 (첫 어플리케이션은 Master, 두번째 이후는 Slave 등) 에 사용 name-\u0026lt;순번\u0026gt; (0 시작) 식으로 Pod 이름 부여 삭제되는 경우 역순 삭제 (숫자가 높은것부터 삭제) DaemonSet\n모든 Node에 동일한 Pod을 실행시키고 싶을 때 -\u0026gt; 모니터링, 로그 수집 등 4. Job Job\n단발성으로 실행되고 끝나는 프로그램 (ML 학습, Batch Job등) spec.backoffLimit : 2 와 같이 실패시 재시도 정책을 정할 수 있음. (2회 재시도) CronJob\n주기적으로 Job을 실행할 수 있도록 Crontab과 같은 기능 제공 spec.schedule : \u0026quot;1/* * * * *\u0026quot; 과 같이 지정 Namespace 클러스터를 논리적으로 나누는 데 사용\ndefault : 기본 Namespace\nkube-system : K8S의 핵심 컴포넌트 (DNS, 네트워크 등..)의 Pod이 존재\nkube-public : 외부로 공개 가능한 리소스를 담는 Namespace\nkube-node-lease : Node가 살아있는지 master에 알리는 용도? 의 namespace\nConfigMap, Secret ConfigMap\nMetadata (설정값) 저장\nKey-value 형태로 값 저장\n다음과 같이 Yaml로 선언 가능\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # game-volume.yaml apiVersion: v1 kind: Pod metadata: name: game-volume spec: restartPolicy: OnFailure containers: - name: game-volume image: k8s.gcr.io/busybox command: [ \u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;cat /etc/config/game.properties\u0026#34; ] volumeMounts: - name: game-volume mountPath: /etc/config volumes: - name: game-volume configMap: name: game-config 이후 다른 Pod에서 Volume 연결/환경변수로 사용 가능 Volume : 1 2 3 4 5 6 7 8 ..... volumeMounts: - name: game-volume mountPath: /etc/config volumes: - name: game-volume configMap: name: game-config Env : 1 2 3 4 ..... envFrom: - configMapRef: name: monster-config Secrets\ntmpfs (메모리를 파일처럼 다룰 수 있게 하는 시스템) 사용해 보안에 이점 있음 조회시 Base64 인코딩 (암호화 X, 단지 인코딩 한번 해 줌) Appendix 1. 특정 Node에서만 Pod 실행하기 예시 : SSD/HDD Node를 구분하여 배치하는 방법\n각 Node에 label 부착\nkubectl label node master disktype=ssd kubectl label node worker disktype=hdd 이후 nodeSelector로 label 선택\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: node-selector spec: containers: - name: nginx image: nginx # 특정 노드 라벨 선택 nodeSelector: disktype: ssd ","date":"2023-01-26T23:15:42+09:00","image":"https://blog.lemondouble.com/p/f13e7c85c56b2c7c/cover_hu_91e84d6d93f2c782.png","permalink":"https://blog.lemondouble.com/p/f13e7c85c56b2c7c/","title":"내가 보려고 쓰는 K8S 리소스 정리"},{"content":"이번에 소개할 프로젝트는 서버의 Uptime을 기록해 주는 Uptime Kuma라는 서비스입니다.\n내 클러스터/서버 등을 모니터링하고, 문제가 생긴 경우 Telegram, Slack, Discord, E-mail\u0026hellip; 등의 여러 방법을 통해 서버가 죽었음을 알려줄 수 있습니다.\n이렇게 생겼어요!\nGithub 주소 ( Link ) 에 가시면 Demo 서비스도 체험해 볼 수 있습니다.\nFly.io에 대한 소개는 이전 글 (Fly.io 소개 및 Fly.io에 올리기 좋은 서비스 추천 (VaultWarden)) 에서 많이 했으니 글을 참고해 주시고, 이번 글에서는 설치법에만 집중하겠습니다.\n1. fly.toml 파일 생성 적절한 위치에 가서, flyctl launch 를 입력하여 fly.toml 파일을 생성합니다. 2. volume 생성 모니터링 기록 저장, ID/Password 저장 등을 위해 Volume이 필요합니다. 그렇게 많은 볼륨이 필요하진 않아서, 저의 경우는 1GB로 생성했습니다. 1 flyctl volume create uptime_kuma_data --region nrt --size 1 3. fly.toml 파일 변경 다음 toml 파일을 참고해 배포 파일을 작성해 주세요. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # 앱 이름은 flyctl launch 할 때 설정한 값입니다. app = \u0026#34;lemon-uptime-kuma\u0026#34; kill_signal = \u0026#34;SIGINT\u0026#34; kill_timeout = 5 processes = [] # 추가 :1 은 Debian Stable Build입니다. # https://hub.docker.com/r/louislam/uptime-kuma [build] image = \u0026#34;louislam/uptime-kuma:1\u0026#34; [env] # 추가, 아까 생성한 Volume을 Mount합니다. [mounts] source=\u0026#34;uptime_kuma_data\u0026#34; destination=\u0026#34;/app/data\u0026#34; [experimental] auto_rollback = true [[services]] http_checks = [] internal_port = 3001 # 변경, 기본 설정시 Kuma는 내부에서 3001 포트를 사용합니다. processes = [\u0026#34;app\u0026#34;] protocol = \u0026#34;tcp\u0026#34; script_checks = [] [services.concurrency] hard_limit = 25 soft_limit = 20 type = \u0026#34;connections\u0026#34; [[services.ports]] force_https = true handlers = [\u0026#34;http\u0026#34;] port = 80 [[services.ports]] handlers = [\u0026#34;tls\u0026#34;, \u0026#34;http\u0026#34;] port = 443 [[services.tcp_checks]] grace_period = \u0026#34;1s\u0026#34; interval = \u0026#34;15s\u0026#34; restart_limit = 0 timeout = \u0026#34;2s\u0026#34; 4. 배포하기! 1 flyctl deploy 커맨드를 입력하여 배포합니다.\n5. 접속하여 ID/Password 설정하기 \u0026lt;앱 이름\u0026gt;.fly.dev 로 접속하거나, 주소를 잘 모르겠으면 https://fly.io/apps 로 접속하여 앱을 선택하고 주소를 가져옵니다.\n이후 ID/Password를 설정합니다.\n6. 모니터링 추가하기 Add New Monitor를 입력한 뒤, 다음과 같이 설정합니다.\n이후 Save 버튼을 누르면..\n모니터링이 정상 작동합니다!\nAppendix 1. TMI 한국어를 지원합니다! Settings -\u0026gt; Appearance 에서 언어 설정을 할 수 있습니다. Appendix 2. Custom Domain 사용하기 저의 경우, 제 도메인이 있어 Integration 하려고 합니다.\n앱 페이지에 가서, IPv6을 가져옵니다. (IPv$는 Shared IPv4라 IPv6을 사용했습니다.)\n자신의 DNS 페이지에 가서 CNAME 유형을 선택하고, 값으론 \u0026lt;앱이름\u0026gt;.fly.dev 를 입력합니다.\n이후 https://fly.io/apps 대시보드에 들어가, kuma 앱 -\u0026gt; Certificates -\u0026gt; Add Certificate를 누른 후, 내가 입력한 커스텀 도메인을 입력합니다.\n잠시 후 내가 생성한 커스텀 도메인으로 접속이 가능한걸 확인할 수 있습니다! ","date":"2023-01-22T20:41:31+09:00","image":"https://blog.lemondouble.com/p/7da6be543113ac58/cover_hu_b7b3dbfb6b2655b1.png","permalink":"https://blog.lemondouble.com/p/7da6be543113ac58/","title":"Fly.io에 올리기 좋은 서비스 (Uptime Kuma)"},{"content":"MSA 서비스를 만들면 서버 간 굉장히 잦은 통신이 일어납니다.\n예를 들어 (아주 간략화한) 송금 서비스가 있다고 가정해 봅시다.\n지갑 서버와 유저정보 서버가 있다면, 송금 API를 호출하면 위와 같은 일이 일어날 겁니다.\n그런데 만약, 정상 유저 확인 API를 외부에서 접근할 수 있다면 어떨까요?\n악의적인 유저가 작정하고 접근하면, 우리 서비스의 정상 유저 비율을 도표로 나타낼 수도 있을 것입니다(..)\n따라서, MSA 구조에서 서버간 요청에선 API를 사용할 수 있지만, 이를 외부 유저가 접근해선 안 되는 요구사항이 생겼습니다.\n어떻게 하면 이 요구사항을 달성할 수 있을까요?\n1. 가장 간단한 방법은, 서버끼리 비밀 Key를 공유하는 방식입니다. 환경 변수 등을 이용해서, 서버끼리 서로 같은 비밀 Key를 공유합니다. 이를 HTTP Custom Header등에 실어서, 서버가 요청할 때 Key를 같이 보냅니다. 서버는 내부 API 요청을 받으면 비밀 Key를 확인하고, 만약 일치하지 않는다면 Reject 합니다. 하지만 이 방법에는 문제가 있습니다.\n만약에 누가 비밀 Key를 들고 날랐다면? 혹은, Git 등의 외부 저장소에 Key가 노출됐다면? 외부에서 당연히 API 요청이 가능할 것입니다. 2. 그러면 Whitelist 등을 관리하는건 어떨까요? 서버의 IP를 모두 알 수 있다면, X-Forwarded-For 헤더 등을 통하여 IP를 확인한 후, 우리가 알고 있는 서버의 IP가 아니라면 차단할 수 있습니다. 그런데 이 방법에도 문제가 좀 있습니다!\n만약 요청이 너무 많이 들어와서, 서버를 증설한다면 어떨까요? 당연히 새 서버의 IP는 알지 못하므로, 차단될 것입니다. 3. 네트워크 설정을 통해 외부 진입을 막아 봅시다! 1, 2의 방법도 좋은 방법이지만, 아예 외부에서 진입하는 경우 응답이 서버까지 도달하지 않으면 좀 더 안전할 것입니다.\n이걸 Private DNS / Public DNS를 이용하여 구현해 봅시다!\nSplit Horizon DNS, 또는 Multiview DNS 라고도 불리는 방법입니다.\n관련 문서 : ( Link )\n그러면 하나하나 차근차근 알아봅시다.\n1. 먼저 Public DNS / Private DNS에 대해 알아 봅시다.\n아주 간단하게 보면, VPC 외부에서 접근할 때 / VPC 내부에서 접근할 때 다른 응답 주소를 줄 수 있습니다. VPC 내부에서 접근할 땐 내부 DNS 서버를, 외부에서 접근할땐 외부 DNS 서버를 사용한다고 생각하면 됩니다! 그러면, 내부 DNS에 없는 레코드를 요청하면 어떻게 될까요?\n위 그림과 같이 Private DNS에 먼저 질의한 후, 없는 경우 Public DNS에 질의합니다.\n( Note : 실제 네트워크 흐름과 정확히 일치하지 않을 수 있습니다. 하지만 중요한 건, Private DNS에 우선권이 있다는 겁니다.)\n2. 그러면 이제 ALB에 대해 알아 봅시다.\nALB (Application Load Balancer)는 들어오는 요청을 평가해서, 적절하게 분배할 수 있습니다.\nHostName을 기준으로 적절하게 응답할 수 있습니다. 예를 들어.. wallet.lemondouble.com 으로 들어오면 wallet server로 연결할 수 있습니다. user.lemondouble.com으로 들어오면 user server로 연결해 줄 수 있습니다. Path를 사용해도 같은 작업을 할 수 있습니다. api.lemondouble.com/wallet 으로 들어오면 wallet server로 연결할 수 있습니다. api.lemondouble.com/user 로 들어오면 user server로 연결해 줄 수 있습니다. 물론, 특정 Path를 포함하고 있다면 Deny하는 것도 가능합니다. 3. 그러면 이 두개를 합치면?\n먼저, Public DNS와 인터넷에 연결된 Public ALB를 만들어 줍니다.\n이 때, Public ALB의 규칙은 다음과 같습니다. 우선순위 1. Hostname이 wallet.lemondouble.com과 일치하고, Path가 /internal* 이면 고정 404 응답 우선순위 2. Hostname이 wallet.lemondouble.com과 일치하면 Wallet Server의 80 포트로 연결 이 경우 ALB 규칙은 우선순위대로 적용되므로\nwallet.lemondouble.com/api/hello 의 경우 우선순위 1 에 Match되지 않고, 우선순위 2 에 매치되므로 wallet Server로 연결됩니다. wallet.lemondouble.com/internal/hello 의 경우 우선순위 1 에 Match되므로, 404 응답을 받습니다. 다음으로, Public DNS와 동일하게 Private DNS와, 인터넷에 연결되지 않은 Private ALB를 만들어 줍니다.\n이 때, Private ALB의 규칙은 다음과 같습니다. 우선순위 1. Hostname이 wallet.lemondouble.com과 일치하면 Wallet Server의 80 포트로 연결 이 경우, /internal* 에 대해 404를 응답하는 로직이 없으므로, 모든 요청은 Wallet Server로 전달됩니다.\n둘을 합치면 다음과 같은 모양이 됩니다. wallet.lemondouble.com/internal/hello API를 호출하였을 때, VPC 외부에서는 404 응답을 받지만, VPC 내부에서는 정상 응답을 받습니다. 4. 그러면 네트워크 설정만 하면 충분할까요? 얼마나 정보 유출에 민감한 어플리케이션을 만드냐에 따라 다르겠지만, 1 과 3의 방법을 합치면 더 좋을 것입니다. 그러면.. 만약 서버끼리의 비밀 Key가 유출되더라도, 네트워크 설정이 막고 있으니 외부 접근이 차단됩니다. 만약 네트워크 설정을 잘못 건드렸더라도, 외부에서는 비밀 Key를 모르므로 외부 접근이 차단됩니다. 실수는 언제나 일어날 수 있으므로, 실수가 일어나더라도 시스템으로 막을 수 있는 방법이 훨씬 좋습니다!! 이상으로 Private (또는 Internal) API를 만드는 법을 알아봤습니다.\nPrivate API를 만드는 방법은 다양합니다. 이외에도 AWS의 API Gateway를 이용할 수도, (잘 모르지만) K8S나 Spring Cloud Gateway 등을 사용할 수도 있을..? 겁니다.\n여러 방법 중에 이런 방법도 있구나~ 하고 봐 주시면 감사하겠습니다. 혹시 틀린 게 있다면 댓글 남겨 주세요!\n감사합니다.\n","date":"2023-01-22T17:12:43+09:00","image":"https://blog.lemondouble.com/p/acf607c351652785/cover_hu_d69c9d14457f410d.png","permalink":"https://blog.lemondouble.com/p/acf607c351652785/","title":"Private DNS와 ALB를 이용해 VPC 외부에선 접속이 불가능한 API 만들기"},{"content":"Github Actions에서 AWS를 사용하려면 보통 다음과 같은 과정을 거칩니다.\nIAM 창에 들어가서 새로운 유저를 발급합니다. Github Actions에서 IAM Role을 추가하고, Programmatic access 방식을 추가해 AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY 를 발급받습니다. 레포지토리에 설정에서, Github Secrets에 방금 발급받은 AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY를 추가합니다. 이후 github actions에서 다음과 같은 워크플로우를 추가한 뒤 AWS 리소스에 접근합니다. 1 2 3 4 5 6 - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: us-east-2 프로젝트가 하나라면, 이런 식으로 관리해도 큰 문제가 없습니다.\n하지만 AWS를 사용하는 프로젝트가 늘어나기 시작하면 몇 가지 고민에 빠지게 됩니다.\n예를 들어 한 레포지토리마다 하나의 IAM User를 발급을 할 것인지? 아니면 슈퍼 유저를 하나 발급해서 ACCESS_KEY, ACCESS_SECRET을 돌려 쓸지?\n슈퍼 유저를 발급하면 혹시라도 관련 키가 유출되었을 경우 큼지막한 AWS 영수증을 받아 볼 수 있어 좋은 Practice라고 보긴 어렵고,\n여러 유저를 발급하자니 유저 수가 많아짐에 따라 어떤 Key가 어디서 사용하는지 관리하기 어려워지는 문제점이 있습니다.\n그나마 최근에 Access Key마다 Description을 붙일 수 있는 기능이 추가되어 관리가 조금 나아졌지만, 그 전엔 어떤 Key를 어디서 쓰는지 몰라서 건드리지 못 하는 일이 비일비재했습니다.\n여기서 한발자국 더 나아가서! Access Key 관리 모범 사례를 준수하려 하면 머리가 더 아파오기 시작합니다.\n모범 사례에 따르면 주기적으로 장기 자격 증명을 교체해줘야 하는데, 주기적으로 새 Key 발급받고 레포지토리에서 Secrets 갈아끼울 생각을 하니 머리가 아픕니다.\n(여러분이 받은 만료되지 않는 ACCESS_KEY, ACCESS_SECRET 를 장기 자격 증명이라고 합니다.)\n내가 매번 Secrets에 Key를 끼워주지 않아도, 알아서 내 리포지토리에만 내 키를 알아서 끼워주면 얼마나 편할까요?\n또한,\n매번 Actions이 돌 때마다 잠깐만 쓸 수 있는 임시 Token을 발급받아서, 혹시 Token이 털리더라도 시간이 지나면 만료되어서 못 쓰게 된다면 얼마나 좋을까요?\nOIDC 연결을 통해 가능합니다! 또한, AWS가 권장하는 보안 모범 사례 (Link) 이기도 합니다!\n한번만 세팅해 두면 Actions 돌리기가 꽤 편해지는 방법을 소개합니다!\n설정 방식은 공식 문서가 있습니다. 혹시 막히는 부분이 있다면 공식 문서를 참고하세요 ( Link ) 1. OIDC 자격 증명 공급자 생성하기 IAM Console로 접속합니다. https://console.aws.amazon.com/iam/ 왼쪽 메뉴에서 작업 증명 공급자 메뉴를 선택한 뒤, 공급자 추가 를 선택합니다. 다음과 같이 설정합니다. 공급자 URL : https://token.actions.githubusercontent.com 대상 : sts.amazonaws.com 이후 지문 가져오기를 누른 후, 공급자를 추가합니다. 2. OIDC 연동 Role 생성 역할 메뉴를 선택하여 새로운 역할을 만듭니다. 웹 자격 증명 을 선택하고, 다음과 같이 1에서 생성한 Github Provider를 선택해 줍니다. 권한의 경우, 지금은 간단한 예시이므로 Managed Policy에서 S3FullAccess 를 사용했습니다. 실제로 사용하실 땐 직접 Policy를 만들어서 사용하실 수도 있습니다. 적당한 이름과 설명을 넣고, Role을 생성합니다. 생성이 끝났습니다! 이후 해당 Role의 ARN을 어디 기록해 둡시다. 3. Github Actions에서 해당 Role 사용하기 다음과 같은 식으로 방금 생성한 Role을 사용할 수 있습니다.\n새로운 Repository를 판 후 .github/workflows/oidc-connect-test.yaml 을 생성합니다.\nAccess key나 Secret 등은 전혀 설정하지 않아도 됩니다!\n아래는 현재 레포지토리를 그대로 S3에 업로드하는 Github Actions입니다.\n다음과 같은 부분을 자신에 맞게 바꿉니다.\nrole-to-assume 부분을 2. 에서 기록한 Role의 ARN으로 변경합니다. 테스트를 위해 버킷을 아무 이름으로 하나 파고, \u0026lt;버킷 이름\u0026gt; 부분을 생성한 버킷 이름으로 교체합니다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 name: OIDC Connect Test on: push: branches: [ main ] jobs: deploy: name: OIDC Connect Test runs-on: ubuntu-latest # These permissions are needed to interact with GitHub\u0026#39;s OIDC Token endpoint. permissions: id-token: write contents: read steps: - name: Checkout uses: actions/checkout@v3 - name: Configure Github Actions AWS credentials uses: aws-actions/configure-aws-credentials@v1 with: role-to-assume: arn:aws:iam::123456461:role/github-actions-role # 아까 생성한 Role을 사용 aws-region: ap-northeast-1 - name: Copy file to S3 run: | aws s3 sync . s3://\u0026lt;버킷 이름\u0026gt; 이후, Actions가 정상적으로 작동하고 실제로 S3에 리포지토리 파일이 올라갔다면 세팅에 성공하신 겁니다! Appendix 1. 보안 강화하기 개인 사용 용이라 현재는 모든 레포지토리가 Credentials를 받아올 수 있지만, 특정 리포지토리만 한정할 수도 있습니다. Role 생성시 아래와 같이 repo를 추가하면, octo-org/octo-repo의 모든 브랜치/PR에서만 AWS를 사용하도록 할 수 있습니다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Federated\u0026#34;: \u0026#34;arn:aws:iam::123456123456:oidc-provider/token.actions.githubusercontent.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRoleWithWebIdentity\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringLike\u0026#34;: { \u0026#34;token.actions.githubusercontent.com:sub\u0026#34;: \u0026#34;repo:octo-org/octo-repo:*\u0026#34; # 이 부분을 추가 }, \u0026#34;StringEquals\u0026#34;: { \u0026#34;token.actions.githubusercontent.com:aud\u0026#34;: \u0026#34;sts.amazonaws.com\u0026#34; } } } ] } Appendix 2. Role은 어떻게 관리하나요? 한번 세팅을 해 두었으니, 앞으로 새로운 Role이 필요하면 2 (OIDC 연동 Role 생성) 부분을 진행해 새로운 Role을 생성하고, role-to-assume 부분만 갈아 껴 주면 됩니다. 끝!\n","date":"2023-01-22T11:18:50+09:00","image":"https://blog.lemondouble.com/p/github-oidc%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%B4-github-actions%EC%97%90-aws-key-%EB%84%A3%EC%A7%80-%EC%95%8A%EA%B3%A0-aws-%EB%A6%AC%EC%86%8C%EC%8A%A4-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-%EB%AC%B4%EB%A3%8C/cover_hu_aa11508ccd80f93a.png","permalink":"https://blog.lemondouble.com/p/github-oidc%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%B4-github-actions%EC%97%90-aws-key-%EB%84%A3%EC%A7%80-%EC%95%8A%EA%B3%A0-aws-%EB%A6%AC%EC%86%8C%EC%8A%A4-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0-%EB%AC%B4%EB%A3%8C/","title":"Github OIDC를 이용해 Github Actions에 AWS Key 넣지 않고 AWS 리소스 사용하기 (무료)"},{"content":"회사나 조직에서 AWS를 사용하다 보면, 다음과 같은 SSO (Single Sign On) 페이지를 통해 로그인 하는 아래와 같은 페이지를 본 적이 있으실 겁니다.\n한번 설정해 보면 세션이 유지되는 동안 매번 id/password를 입력해 주지 않아도 되어서 사용하기도 편하고,\n또한 aws-cli나 aws-sdk를 사용할 때도 AWS_ACCESS_KEY, AWS_ACCESS_SECRET 같은 유출되면 안 되는 정보들을 로컬 컴퓨터에 평문으로 저장하지 않아도 되어서, 보안 상으로도 이점이 있습니다.\n또한 무료 서비스라 (Link) 추가 비용도 전혀 들지 않습니다!\n그러면 한번 설정해 봅시다!\n1. IAM Identity Center 설정하기 1. AWS에서 SSO를 검색해서 IAM Identity Center 를 찾습니다. 2. 활성화를 눌러줍니다! 이 때, Identity Center는 한 리전에서만 활성화 가능하니 주로 쓰는 리전에 활성화 해 주면 추후 관리하기 좀 더 편합니다. 3. AWS Organization을 먼저 설정해 줍니다. 다음과 같은 에러창을 만날건데, AWS Organization을 먼저 생성해 줍니다. AWS Organization도 마찬가지로 무료 서비스입니다. (Link) 4. 그러면 Dashboard가 생성되는데, 추가 설정을 위해 설정으로 이동 버튼을 눌러줍니다. 5. 일단 SSO 로그인 페이지에 별칭을 부여해 봅시다. 기본으로 제공해 주는 로그인 페이지는 aadfbasdf-d.awsapps.com/start 과 같이 자동 생성된 페이지라 기억하기 어렵습니다. 이걸 lemonlogin.awsapps.com/start 처럼 별명을 지정해 봅시다. 딱 한 번만 설정할 수 있으므로, 신중하게 설정합시다! 6. 그룹을 추가합시다! 그룹을\n7.사용자를 추가합시다! 사용자 추가 버튼을 누른 뒤에.. 사용자 정보를 입력해 줍니다. 암호는 이메일로 설정 메일을 받을 수도, 일회용 패스워드를 받을 수도 있습니다. 기본값으로 이메일로 받도록 합시다. 이후 Group을 선택할 수 있는데, 5.에서 설정했던 그룹을 추가해 줍니다.\n이후 이메일을 확인하여 초대를 수락하고, 해당 계정에 Password를 설정한 후 어딘가에 기록해 둡니다!\n8. 권한을 추가합시다! 권한 세트를 선택합니다. 저는 AdministratorAccess를 선택했지만, 다른 값을 선택할수도 있습니다. 사전 정의된 권한 세트에 원하는 Role이 없다면 사용자 지정으로 직접 선택할 수도 있습니다. 이후 나머지 설정을 적어 줍니다. 세션 기간의 경우, 해당 시간이 지나면 자동으로 AWS가 로그아웃 시키는 시간입니다. 기본은 1시간인데, 너무 자주 로그아웃 되면 귀찮아서 저는 4시간 정도로 늘렸습니다. 릴레이 상태 는 로그인 성공 시 Redirect할 URL을 정할 수 있습니다. 예를 들어 Billing 권한이라면 로그인 성공시 Billing 페이지로 리다이렉션되는 식입니다. 필요하다면 문서(Link) 를 참고해서 설정해 주세요. 저는 빈 칸으로 남겨뒀습니다. 9. SSO 로그인 설정하기 이후 AWS 계정 탭에서 내 계정을 선택하고, 사용자 또는 그룹 할당 을 눌러주세요. 위에서 설정한 그룹을 추가하고 위에서 설정한 권한을 추가해 주세요. 10. SSO 로그인 이제 5에서 추가했던 aws 로그인 주소로 가서 로그인 해 봅시다. 별칭.awsapps.com/start 로 들어가서 로그인합니다. SSO 설정에 성공하셨습니다! Appendix 1. 보안 강화하기 IAM Identity Center - 설정에 들어가 SSO 계정에 대한 MFA 설정, 세션 길이 설정 등을 할 수 있습니다. 다른 내용은 그대로 쓰더라도, 이 사용자에게 등록된 MFA 디바이스가 없는 경우 - 로그인 시 MFA 디바이스를 등록하도록 요구 옵션은 키는 것을 추천드립니다. Appendix 2. AWS SSO로 AWS_CLI 사용하기 SSO 설정 이후 위와 같이 aws configure sso를 이용하여 로그인 설정을 해 줍니다. (빨간색으로 밑줄친 줄은 직접 입력합니다.) 설정 이후 aws s3 ls 커맨드 등을 이용하여 정상 세팅되었는지 확인합니다. ","date":"2023-01-21T16:17:06+09:00","image":"https://blog.lemondouble.com/p/1c9eaebbdee2ac98/cover_hu_654fbd85317025b5.png","permalink":"https://blog.lemondouble.com/p/1c9eaebbdee2ac98/","title":"AWS SSO로 회사에서 보던 그 화면 집에서도 쓰기 (무료)"},{"content":"요즘 되게 핫하면서도 어려운 기술이 쿠버네티스인 것 같습니다.\n저는 학생때부터 나름 꽤 오래(?) 홈 서버를 운영했었는데, 운영을 하면서 꽤 많은 힘든 일을 마주했습니다.\n이사를 가서 네트워크를 다시 짜야 되거나, 디스크 하나가 죽어버리거나 하는 경우도 간간히 있고, 그럴 때마다 Git이랑 다른 컴퓨터를 뒤져서 사이트를 살리는게 여간.. 귀찮은 일이 아닙니다.\n뭔가 내 서버가 어떻게 돌아가고 있는지 적어놓고, 언제든지 마음대로 컴퓨터를 추가하고, 디스크를 추가하고 하면 얼마나 좋을까? 하는 차에 K8S는 정말 매력적인 선택지였습니다.\n그래서 K8S 공부를 해 봤는데, 보통 Vagrant 등을 이용해서 짱짱한 컴퓨터 하나에 가상 노드 하나를 놔두고, 가상 VM을 띄워서 실습을 하게 됩니다.\n근데 문제는 그 다음입니다.\n데스크탑 홈 서버를 여러대를 사서 K8S를 돌릴 수는 없으니, 가격 쌈 \u0026amp;\u0026amp; 저전력 PC를 찾게 되고, 보통 라즈베리 파이로 귀결됩니다.\n그런데 강사 선생님이 이쁘게 깔아놓은 설치 스크립트만 쓰다가, ARM에 직접 K8S 깔려면 머리가 터집니다. (사실 저는 터졌습니다. 다른사람은 잘 모르겠고..)\nKubeAdm, MicroK8S 를 몇번의 삽질 끝에 설치 실패했는데, K3S는 되게 간편하게 설치 되어서 경험을 공유하려 합니다.\n아래 환경은 Raspberry pi 4B + Ubuntu Server 22.04에서 진행했습니다.\n1. Master Node 설치하기 K3S 공식 문서를 보면 curl -sfL https://get.k3s.io | sh - 만 실행하면 알잘딱깔센으로 설치해준다고 합니다! bash에 curl -sfL https://get.k3s.io | sh - 를 입력해서 설치를 해 봅니다. 와! 설치가 잘 됩니다. kubectl도 같이 설치해준다고 합니다. kubectl get nodes를 입력해 봅니다. 와! 노드도 잘 뜹니다. 근데 사실 함정입니다. kubectl get nodes를 어? 갑자기 응답이 느립니다. The connection to the server 127.0.0.1:6443 was refused - did you specify the right host or port? 라는 이슈를 만납니다. 아\u0026hellip; 얘도 함정인가? 불안합니다. 정말 다행이도 얘는 Known Issue이고, 해결 방법도 있습니다. (관련 Github Issue) sudo apt install linux-modules-extra-raspi \u0026amp;\u0026amp; reboot를 입력해서 추가로 필요한 의존성을 깔아주고 재부팅 한 후 kubectl get nodes를 하면 잘 됩니다. (공식 문서 Link) 2. 노드 추가하기 Master Node를 설정했으니, Worker Node를 붙여봅시다.\nkubectl get nodes 를 입력하여, Master Node의 이름(Name) 을 확인합니다.\nsudo kubectl get node \u0026lt;master node 이름\u0026gt; -ojsonpath=\u0026quot;{.status.addresses[0].address}\u0026quot; 를 입력하여, 나오는 IP 값을 확인합니다. (1)\nsudo cat /var/lib/rancher/k3s/server/node-token 을 입력하여, 나오는 Token값을 확인합니다. (2)\n새 Worker Node에 접속해서 sudo apt install linux-modules-extra-raspi \u0026amp;\u0026amp; reboot 하여 관련 의존성을 깔아줍니다.\n이후 curl -sfL https://get.k3s.io | K3S_URL=https://\u0026lt;(1)에서 확인한 ip\u0026gt;:6443 K3S_TOKEN=\u0026lt;2에서 확인한 토큰\u0026gt; sh - 을 실행합니다.\n이후 Master Node에서 kubectl get nodes 입력하여 정상적으로 클러스터에 Join되었는지 확인합니다.\n추가 : Worker Node 상태가 NotReady에서 안 넘어가는 경우가 있는데, 이 경우를 대응해 봅시다.\nkubectl get nodes 입력하여 Worker Node의 이름을 기억합니다. sudo kubectl describe node \u0026lt;worker node 이름\u0026gt; -n=kube-system 을 입력하여 현재 노드의 상태를 확인합니다. 저의 경우는 invalid capacity 0 on image filesystem 이라는 오류가 계속 발생해서 Worker Node가 조인이 안 되는 상황이었는데, 재부팅 하니 해결됐습니다 (..) 끝!\n","date":"2023-01-18T11:56:02Z","image":"https://blog.lemondouble.com/p/fe0cac9de367fc18/cover_hu_76e4408caaa3e6f6.png","permalink":"https://blog.lemondouble.com/p/fe0cac9de367fc18/","title":"라즈베리 파이 + Ubuntu 22.04에 K3S 설치해서 쓰기"},{"content":"라즈베리 파이나, 홈 서버를 굴리다 보면 의외로(?) 서버를 초기화해야 하는 경우가 많습니다.\n세팅을 잘못 해서 뭔가 잘 안 된다거나, 뭘 깔았는지도 모르게 뭐가 잔뜩 설치되어 있는 경우 같이요.\n그래서 라즈베리 파이를 초기화하면 해 줘야 하는 일련의 작업들이 있는데,\n문제가 이게 항상 까먹을 때 쯤 초기화를 해서 세팅할 때마다 한참 구글을 뒤져야 합니다. 그래서 그걸 좀 모아두려구요.\n아래 내용은 Raspberry PI + Ubuntu Server 기준입니다. 초기 로그인 패스워드는 ubuntu 입니다.\n1. SSH 접속 가능하게 하기 HDMI 케이블 들고와서 꼽고 키보드 꼽는거 굉장히 귀찮습니다. 부팅 디스크 구울때 ssh (확장자 없음) 이란 빈 파일을 하나 생성해 주면, 부팅시 ssh 접근이 가능하도록 설정됩니다. (기본은 막혀있음) 파이를 랜선을 이용해 공유기에 꼽고, 공유기 관리 페이지 등으로 IP를 딴 다음 SSH 접속합니다. hostname은 ubuntu로 잡힙니다. 2. SSH 로그인 설정 SSH 로그인이 가능하게 ~/.ssh/authorized_keys 파일에 내 공개키를 추가해 줍시다. SSH 키가 없다면 SSH 키 로그인 같은걸 구글에 쳐서 따라해 봅니다! 3. 패스워드 로그인 막기 내부망에서만 열어둔 파이라면 보통 별 상관 없지만.. SSH Key가 있다면 패스워드 로그인은 어지간하면 막아놓는게 보안상 좋습니다. /etc/ssh/sshd_config 에 들어가서 PasswordAuthentication no 로 설정해 줍니다. /etc/ssh/sshd_config.d/50-cloud-init.conf 에 들어가서도 PasswordAuthentication no 로 설정해 줍니다. 이후 sudo systemctl reload sshd 로 ssh 서버를 재시작합니다. 4. Wifi 연결 (선택) 랜선 꼽아서 쓸 거면 안 해도 됩니다. 저는 랜선 빼면 방이 지저분해져서 일단 와이파이 세팅을 합니다. /etc/netplan/50-cloud-init.yaml 파일을 엽니다. (없으면 만듭니다.) 아래 파일을 복사 붙여넣기 합니다. 아래 설정 파일은 SSID (와이파이 이름) = lemon, 패스워드는 lemon1234의 예시입니다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # This file is generated from information provided by the datasource. Changes # to it will not persist across an instance reboot. To disable cloud-init\u0026#39;s # network configuration capabilities, write a file # /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following: # network: {config: disabled} network: ethernets: eth0: dhcp4: true optional: true wifis: wlan0: dhcp4: true optional: true access-points: \u0026#34;lemon\u0026#34;: password: \u0026#34;lemon1234\u0026#34; version: 2 5. Hostname 설정 SSH 창에서 지금 뭐 건들고 있는지 안 보이면 불안합니다\u0026hellip; sudo hostnamectl set-hostname \u0026lt;호스트명\u0026gt; 입력해서 호스트 이름도 알기 쉽게 바꿔줍니다. 6. 재부팅 sudo reboot 무선랜으로 재연결시 내부 IP가 바뀔 수도 있으니, 연결 안 되면 공유기 페이지 보고 내부 ip를 수정합니다. ","date":"2023-01-18T11:13:14Z","image":"https://blog.lemondouble.com/p/088bdb572b7dc509/cover_hu_4b9026c79a6cd6f8.png","permalink":"https://blog.lemondouble.com/p/088bdb572b7dc509/","title":"내가 보려고 올리는 라즈베리 파이 초기 세팅"},{"content":" TIP : 숫자 작성할 때 var number = 1_000L 과 같이 작성 가능 1. 변수 val collections에도 element는 추가할 수 있음 primitive, reference type 구분 없지만, 실제 연산할떈 primitive type으로 변환되어 연산됨 2. Null 처리 Safe Call ( ?. ) : null이 아니면 실행, null이면 실행하지 않음 1 2 3 val str: String? = \u0026#34;ABC\u0026#34; str.length // ERROR str?.length // OK Elvis 연산자 ( ?: ) : 앞의 연산 결과가 null이면 뒤의 값을 사용 1 2 3 4 val str: String? = \u0026#34;ABC\u0026#34; str?.length ?: 0 // str.length 혹은 0 str?.length ?: throw IllegalArgumentException(\u0026#34;null이 들어왔습니다.\u0026#34;) // 혹은 예외 발생 str?.length ?: return 0 // Early return에도 사용 가능 플랫폼 타입 : Kotlin에서 Java 코드를 가져왔을 때 null, nonnull 타입 둘 다 사용 가능 가능하면 Java 코드 직접 확인하거나, Kotlin으로 wrapping하여 사용하자. 1 2 3 4 5 6 // @Nullable, @NonNull 있으면 변환됨, 하지만 없으면 플랫폼 타입 // java 에서 private final String name; \u0026lt;\u0026lt; Annotation 없을 때 val name : String? = javaPerson.name // OK val name : String = javaPerson.name // OK 3. Type 기본 Type 변환 : 기본 타입끼리는 toXX 같은 함수를 사용 1 2 3 4 5 val number1 : Int = 4 val number2 : Long = number1.toLong() val number1 : Int? = 3 val number2 : Long = number1?.toLong() ?: 0L // nullable 변수면 재주껏 처리 사용자 정의 Type : 스마트 캐스트 (Smart Cast) 활용 1 2 3 4 5 6 7 8 9 10 11 12 13 14 fun printAgeIfPerson(obj: Any){ // is = Java의 instanceof if(obj is Person){ // 스마트 캐스트 되어서 obj 의 type은 Person println(obj.age) } } // OR // java의 (Person) obj와 같음 val person = obj as Person // !is, as? 등도 사용 가능 // as? : 전체가 null Any : Java의 Object 역할 + Primitive Type의 최상위도 포함 null은 포함 X, null도 포함하고 싶다면 Any?로 사용 equals, hashCode, toString 존재 Unit : void 비슷, 하지만 Unit은 그 자체로 제네릭에서 타입 인자로 사용 가능 Nothing : 함수가 정상적으로 끝나지 않았음을 표시 1 2 3 fun fail(message : String): Nothing{ throw IllegalArgumentException(message) } String interpolation 1 2 val person = Person(name = \u0026#34;lemon\u0026#34;, age = 20) val log = \u0026#34;이름 : ${person.name}, 나이 : ${person.age}\u0026#34; String indexing 1 2 3 val str = \u0026#34;ABC\u0026#34; // Java에서의 str.get(0) 대신 str[1] //ok 4. 연산자 Kotlin에선 \u0026gt;, \u0026lt; , ≥, ≤ 사용하면 자동으로 compateTo 호출해줌\n== (값 비교,동등성 : equals 자동 호출 ) , === (주소 비교, 동일성 : 같은 0x101 주소)\nin, !in : 컬렉션 / 범위에 포함되어 있는지?\na..b : a부터 b까지의 범위 객체를 생성\n1 2 3 4 5 6 7 val numbers = 1..100 // 1~100까지의 IntRange 객체 생성 println(1 in numbers) // true println(0 in numbers) // false // 두 식은 같음 if(0 \u0026lt;= score \u0026amp;\u0026amp; score \u0026lt;= 100) {..} if(score in 0..100) 연산자 오버로딩 1 2 3 4 5 6 7 8 9 data class Money(val amount : Long){ operator fun plus(other : Money): Money{ return Money(this.amount + other.amount) } } //아래와 같이 사용 가능 val sumMoney = money1 + money2 5. 조건문 if문은 Kotlin에선 expression (Java에선 statement) expression : 하나의 값으로 평가될 수 있는 문장 1 2 3 4 // Java에선 불가능하지만 Kotlin에선 if문도 expression이라 OK fun getPassOrFail(score : Int): String{ return if (score \u0026gt;= 50){ \u0026#34;P\u0026#34; } else { \u0026#34;F\u0026#34; } } Java에선 삼항연산자가 expression이지만, Kotlin은 if문이 expression이므로 삼항 연산자가 필요x\n따라서 삼항 연산자가 없음 When문\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // 기본 사용, Switch 대신 fun getGradeWithSwitch(score: Int): String{ return when(score / 10){ 9 -\u0026gt; \u0026#34;A\u0026#34; 8 -\u0026gt; \u0026#34;B\u0026#34; 7 -\u0026gt; \u0026#34;C\u0026#34; else -\u0026gt; \u0026#34;D\u0026#34; } } // 좌변에 expression 사용도 가능, is Type 등도 사용 OK when(score){ in 90..99 -\u0026gt; \u0026#34;A\u0026#34; in 80..89 -\u0026gt; \u0026#34;B\u0026#34; ... // 여러 조건 동시 검사 when (number){ 1,0,-1 -\u0026gt; println(\u0026#34;1,0,-1입니다\u0026#34;) else -\u0026gt; println(\u0026#34;아닙니다\u0026#34;) } // 조건문처럼 사용 fun printOdd(score: Int): Int{ when{ score % 2 == 1 -\u0026gt; println(\u0026#34;홀수\u0026#34;) score % 2 == 0 -\u0026gt; println(\u0026#34;짝수\u0026#34;) else -\u0026gt; println(\u0026#34;넌머임\u0026#34;) } return 1 } 6. 반복문 forEach 1 2 3 4 val numbers = listOf(1L, 2L, 3L) for (number in numbers){ println(number) } Progression : 등차수열, Range (Range는 Progression을 상속받음) 1 val intRange = 1..100 7. 예외 try-catch-finally : 그대로 (단, kotlin에선 expression, return 등 사용 가능) Kotlin에서는 모든 에러가 Unchecked Exception Checked : 무조건 처리해줘야 하는 Exception Unchecked Exception : 꼭 처리하지 않아도 되는.. try with resources 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // JAVA의 try with resources // Try가 끝나면 자동으로 닫아줌. 단 AutoCloseable 인터페이스의 구현체여야 함. public void readFile(String path) throws IOException{ try(BufferedReader reader = new BufferedReader(new FileReader(path))){ System.out.println(reader.readLine()); } } //Kotlin에서 사용, use가 try with resources 비슷하게 작동 fun readFile(path: String){ BufferedReader(FileReader(path)).use { reader -\u0026gt; println(reader.readLine()) } } 8. 함수 기본 1 2 //함수 본문이 식(Expression)으로만 이뤄져 있으면 = 사용 가능 fun max(a: Int, b: Int) : Int = if(a \u0026gt; b) {a} else {b} Default paramters 1 2 3 4 5 6 7 8 9 fun repeat( str : String, num : Int = 3, useNewLine : Boolean = true ){ for(i in 1..num){ if(useNewLine) { println(str)} else{ print(str) } } } named argument ( 파라미터 이름을 명시 가능, builder 비슷하게 사용 가능) 1 2 3 repeat(str = \u0026#34;Hello, World\u0026#34;, useNewLine = true) // 이 경우, num은 지정되지 않았으므로 기본값 (3) 사용됨 // builder 만들지 않고 builder처럼 사용 가능 가변인자 (vararg) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Java public void printAll(String... strings){ for(String str : strings){ System.out.println(str);\t} } String[] array = new String[]{\u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34;, \u0026#34;!!\u0026#34;}; printAll(array) printAll(\u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34;, \u0026#34;!!\u0026#34;) // Kotlin printAll(vararg strings : String){ for (str in strings){ println(str) } } val array = arrayOf(\u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34;, \u0026#34;!!\u0026#34;) printAll(*array) // spread 연산자 printAll(\u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34;, \u0026#34;!!\u0026#34;) 9. 클래스 클래스와 프로퍼티 1 2 3 4 5 6 7 8 class Person( val name:String, var age:Int ) // 프로퍼티 접근처럼 getter,setter 접근 person.age = 10 println(person.age) 생성자 검증, init 1 2 3 4 5 6 7 8 9 10 class Person( val name:String = \u0026#34;Patrick\u0026#34;, var age:Int = 1 ){ init{ if(age \u0026lt;= 0){ throw IllegalArgumentException(\u0026#34;나이는 ${age}일 수 없습니다.\u0026#34;) } } } Custom getter, Setter 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class Person( val name:String, var age:Int ){ // Property처럼 사용, 하나의 Expresson으로 표시되는 것을 =으로 표시 val isAdult: Boolean get() = this.age \u0026gt;= 20 //위와 같은 표현식 fun isAdult(): Boolean{ return this.age \u0026gt;= 20 } } 10. 상속 추상 클래스 1 2 3 4 5 6 7 // 기본적으로 public class abstract class Animal( protected val species:String, protected val legCount:Int ){ abstract fun move() } 상속 받는 클래스 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 //Java public class JavaPenguin extends JavaAnimal{ // 하위 객체에서 추가된 필드 private final int wingCount; public JavaPenguin(String species){ super(species, 2) this.wingCount = 2; } @Override public void move(){ System.out.println(\u0026#34;penguin is moving\u0026#34;); } // 상위 메소드의 getter를 오버라이드 @Override public int getLegCount(){ return super.legCount + this.wingCount; } } //Kotlin class penguin( species : String // 주 생성자 ) : Animal(species, 2) //Animal을 상속받아 Animal의 생성자를 바로 호출 { private val wingCount: Int = 2 // Annotation 대신 예약어 사용 override fun move(){ println(\u0026#34;cat is moving\u0026#34;) } override val legCount: Int get() = super.legCount + this.wingCount } // 단, 코틀린은 기본적으로 모두 final 키워드가 붙여져 있기 때문에, override 하려면 open 필요 abstract class Animal( protected val species:String, protected open val legCount: Int ){ abstract fun move() } Interface 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Java public interface JavaSwimable{ default void act(){System.out.println(\u0026#34;swim!\u0026#34;); } } public interface JavaFlyable{ default void act(){System.out.println(\u0026#34;swim!\u0026#34;); } } // Kotlin interface Swimable{ //Default 키워드 없이 구현 가능 fun act(){ println(\u0026#34;swim!\u0026#34;) } } interface Flyable{ fun act(){ println(\u0026#34;fly!\u0026#34;) } } Interface 구현 클래스 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // Java public final class JavaPenguin extends JavaAnimal implements JavaFlyable, JavaSwimable{ // 생성자 구현.. (생략됨) @Override public void act(){ JavaSwimable.super.act(); JavaFlyable.super.act(); } } // Kotlin class Penguin(): Animal(species, 2), Swimable, Flyable{ override fun act(){ super\u0026lt;Swimable\u0026gt;.act() super\u0026lt;Flyable\u0026gt;.act() } } Backing Field (getter, setter가 제공되는 property) 를 Interface에 선언 가능 1 2 3 4 5 6 7 8 9 10 11 // Kotlin interface Swimable{ // 하위 클래스에서 구현을 기대하는 property val swimAbility: Int } class Penguin(): Swimable{ // 다음과 같이 구현 override val swimAbility: Int get() = 3 } 클래스 상속시 주의할 점 (초기화 실행 순서) 상위 Class의 Constructor, Init 블락에선 하위 클래스의 Field에 접근하지 말 것! 상위 Class의 Constructor, Init 블럭에 사용되는 Property는 open 사용하지 말자! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 open class Base( open val number : Int = 100 ){ init{ println(\u0026#34;Base Class\u0026#34;) println(number) } } class Derived( override val number: Int ): Base(number){ init{ println(\u0026#34;Derived class\u0026#34;) } } // Derived 클래스 인스턴스화시 출력 /* Output : Base Class 0 Derived Class */ // 이유 : 상위 Class(Base)에서 number 호출 시, 하위 Class의 Number 가져옴. // 하지만 상위 Class(Base) contstructor 실행 단계라 하위 Class(Dervied) 초기화 안 된 상태 // 따라서 0 출력 키워드 정리 final (기본, 생략가능) : override 할 수 없게 막는다. open : override를 열어 준다. abstract : 반드시 override 해야 한다. override: 상위 타입을 오버라이드 한다. (Java에선 Annotation이지만 Kotlin에선 키워드) 11. 접근 제어 Java, Kotlin의 접근 제어\nPublic Java/Kotlin : 모든 곳에서 접근 가능 Protected Java : 같은 패키지, 또는 하위 클래스에서 접근 가능 Kotlin : 선언된 클래스, 또는 하위 클래스에서만 접근 가능 default(Java) / internal(Kotlin) Java : 같은 패키지에서만 접근 가능 Kotlin : 같은 Module에서만 접근 가능 Module : 한 번에 컴파일되는 Kotlin 코드 (같은 Gradle 프로젝트 등..) private Java/Kotlin : 선언된 클래스 내에서만 접근 가능 Property 접근 지시어\n1 2 3 4 5 6 7 8 9 10 class Car( // name에 대한 getter를 internal로 internal val name : String, var owner : String, _price : Int ){ // Setter만 private로 var price = _price private set } Java/Kotlin 혼합 사용시 주의점 Internal 키워드는 Bytecode 변환 (Java 변환시) public으로 변환됨 따라서 Java 코드에서는, Kotlin에서의 Internal 키워드를 가져올 수 있음 Kotlin의 Prtotected 또한 마찬가지 (Java에서는 같은 패키지에서도 접근 가능) 12. object static 함수와 변수 static : 정적으로 인스턴스끼리의 값을 공유 companion object : 클래스와 동행하는 유일한 object 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 //Java public class JavaPerson{ private static final int MIN_AGE = 1; // 정적 팩토리 메소드 public static JavaPerson newBaby(String name){ return new JavaPerson(name, MIN_AGE); } private String name; private int age; private JavaPerson(String name, int age){ this.name = name; this.age = age} } //Kotlin class Person private constructor( var name : String, var age : Int, ){ //Static 대신 companion Object 사용 companion object{ // const가 있으면 컴파일시 할당, 없으면 런타임에 할당 // 기본 타입 + String에만 할당 가능 const val MIN_AGE = 1 fun newBaby(name: String): Person{ return Person(name, MIN_AGE) } } } // Person.Companion.newBaby(\u0026#34;ABC\u0026#34;) 로 호출 가, // 혹은 다음과 같이 @JvmStatic 어노테이션 통해 Java Static처럼 바로 접근 가능 /* @JvmStatic fun newBaby(name: String) : Person{...} 이후 Person.newBaby(\u0026#34;ABC\u0026#34;) 사용 가능 */ Companion Object 활용 (Java와의 차이점) 하나의 객체로 간주됨 따라서 이름을 붙이거나 Interface 만들 수 있음 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // Kotlin interface Log{ fun log() } class Person private constructor( var name : String, var age : Int, ){ // 이름 설정 가능 companion object Factory{ const val MIN_AGE = 1 fun newBaby(name: String): Person{ return Person(name, MIN_AGE) } override fun log(){ println(\u0026#34;I am companion object!\u0026#34;) } } } // 이름이 있다면 이름을 바탕으로 접근 가능 // Person.Factory.newBaby(\u0026#34;ABC\u0026#34;); Singleton object 키워드 사용 1 2 3 4 5 6 7 8 9 10 11 12 13 14 //Java public class JavaSingleton{ private static final JavaSingleton INSTANCE = new JavaSingleton(); private JavaSingleton(){} public static Javasingleton getInstance(){ return INSTANCE; } } //Kotlin object Singleton{ var a: Int = 0 } // 이후 Singleton.a 와 같이 사용 가능 익명 클래스 (Anonymous class) 특정 Interface, Class 상속받은 구현체를 일회성으로 사용할 때 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // Java private static void moveSomething(Movable movable){ movable.move(); movable.fly(); } //익명 클래스 구현 moveSomething(new Movable(){ @Override public void move(){System.out.println(\u0026#34;move~\u0026#34;);} @Override public void fly(){System.out.println(\u0026#34;fly~\u0026#34;);} } // Kotlin private fun moveSomething(movable:Movable){ movable.move(); movable.fly();} // Object 키워드 사용해서 익명 클래스 구현 moveSomething(object : Movable{ override fun move(){ println(\u0026#34;move~\u0026#34;) } override fun fly(){println(\u0026#34;fly~\u0026#34;)} }) 13. 중첩 클래스 (nested class) 자주 안 쓸 것 같아서 생략.. 14. Data class, Enum class, Sealed Class/Interface Data class DTO를 편하게 사용 가능 field, constructor, getter, equals, hashCode, toString 를 기본적으로 제공 1 2 3 4 5 //Kotlin data class PersonDto(val name: String, val age: Int) // named Arguments를 활용하면 Builder 패턴도 사용하는 것과 같은 효과 PersonDto(name=\u0026#34;Patrick\u0026#34;, age=20) Enum class 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 //Java public enum JavaCountry{ KOREA(\u0026#34;KO\u0026#34;), AMERICA(\u0026#34;US\u0026#34;); private final String code; JavaCountry(String code) { this.code=code; } public String getCode() { return this.code; } } // Kotlin (동일 코드) enum class Country( private val code: String ){ KOREA(\u0026#34;KO\u0026#34;), AMERICA(\u0026#34;US\u0026#34;); } // When절을 활용해 간편하게 처리 // 또한, 이렇게 사용하는 경우 ENUM CLASS에 값이 생기면 컴파일러가 알려줌 fun handleCountry(country: Country){ when(country){ Country.KOREA -\u0026gt; LOGIC() Country.AMERICA -\u0026gt; LOGIC() // ELSE 없이도 사용 가능! } } Sealed Class/Interface 상속이 가능한 추상 클래스를 만들고 싶은데, 외부에서는 상속하지 못 했으면 좋겠다 컴파일 타임에 하위 클래스 타입을 모두 기억해, 런타임때 다른 클래스 타입 추가 불가 하위 클래스는 같은 패키지에 있어야 함 Enum과의 차이점 Class 상속 가능 Enum은 인스턴스가 싱글톤이지만, Sealed Class는 멀티 인스턴스 가능 1 2 3 4 5 6 7 8 9 10 //Kotlin sealed class Car( val name:String, val price:Long ) class Avante: Car(\u0026#34;Avante\u0026#34;,1_000L) class Sonata: Car(\u0026#34;Sonata\u0026#34;,2_000L) class Grandeur: Car(\u0026#34;Grandeur\u0026#34;,3_000L) // 위 Enum class처럼 when절 사용 가능 15. Collections (컬렉션) Collections\n컬렉션이 불변인지, 가변인지 설정 필요 Mutable(가변) 컬렉션 : element 추가,삭제 가능 Immutable(불변) 컬렉션 : element 추가,삭제 불가 단, 불변 컬렉션이라도 Reference Type의 엘리먼트 필드는 바꿀 수 있다. 예를 들어서, Index 1번 데이터에 접근해서 Price 필드를 1000 → 2000원으로 바꾸는건 가능 List\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 /* 기본 구현체 : ArrayList */ // 불변 리스트 생성 val numbers = listOf(100,200) // 가변 리스트 생성 val mutableNumbers = mutableListOf(100,200) // 빈 리스트 생성 val emptyList = emptyList\u0026lt;Int\u0026gt;() //numbers.get(0)과 동일 numbers[0] for(number in numbers){println(number)} //Index를 같이 가져와야 할 때 for((idx, number) in numbers.withIndex()){println(\u0026#34;${idx}, ${number}\u0026#34;)} Set 1 2 /* 기본 구현체 : LinkedHashSet */ val numbers = setOf(100,200) Map 1 2 3 4 5 6 7 8 9 10 val weekMap= mutableMapOf\u0026lt;Int, String\u0026gt;() // 배열처럼 접근 가능 weekMap[1] = \u0026#34;MONDAY\u0026#34; weekMap[2] = \u0026#34;TUESDAY\u0026#34; // to를 사용해 초기화도 가능 mapOf(1 to \u0026#34;MONDAY\u0026#34;, 2 to \u0026#34;TUESDAY\u0026#34;) for(key in weekMap.keys){println(key) println(weekMap[key]) } for((key, value) in weekMap.entries){println(key) println(value)} 컬렉션의 null 가능성\nList\u0026lt;Int?\u0026gt; : List 자체는 null X, 리스트 안에는 nullable List\u0026lt;Int\u0026gt;? : List 자체는 nullable, 리스트 안에는 null X List\u0026lt;Int?\u0026gt;? : List 자체도 nullable, 리스트 안에도 nullable Java와 혼용시 주의할 점\nJava는 불변/가변 리스트를 구분하지 않음 Kotlin의 불변 리스트를, Java에서 가변 리스트처럼 사용할 수 있음 Kotlin의 Non-null 리스트를, Java에서 null을 추가할 수 있음 Kotlin에서 Collections.unmodifiable을 사용하거나, 방어 로직 사용 Platform Type으로 타입 문제 생길 수 있음 16. Extension(확장), Infix(중위) , inline, Local(지역) 함수 확장 함수 Kotlin : Java와의 100% 호환성을 목표로함 그래서 Java로 만들어진 라이브러리에 Kotlin 코드가 추가 가능하면 좋겠다 클래스 내부 메소드처럼 사용하지만, 코드는 외부에 작성할 수 있게 하자! 제한 : 확장함수가 public인데, private, protected 멤버를 가져오면 캡슐화가 깨지므로.. 캡슐화 유지 위해, private, proteced 멤버 가져올 수 없음 멤버함수/ 확장함수의 시그니쳐(이름) 같은 경우 멤버 함수가 우선적으로 호출 따라서, 멤버함수가 이후 추가된다면 오류 생길 수도 있다. 1 2 3 4 5 6 7 8 // Kotlin // String 클래스를 확장 fun String.lastChar(){ return this[this.length - 1] } // 이후 Kotlin에선 다음과 같이 사용 가능 val str = \u0026#34;ABC\u0026#34; println(str.lastChar()); 중위 함수 변수, argument가 각각 하나씩만 있는 경우 var.functionName(argument) 대신 var functionName argument로 호출 가능 1 2 3 4 5 6 infix fun Int.add(other: Int): Int{ return this + other } // 다음과 같이 호출 가능 3 add 4 인라인 함수 우리가 아는 그 인라인 함수.. 함수를 파라미터로 전달하는 오버헤드 줄일 수 있음 1 inline fun Int.add(other: Int): Int{ 지역 함수 함수 안의 함수 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // Kotlin fun createPerson(firstName: String, lastName:String): Person{ if(firstName.isEmpty()){ throw IllegalArgumentException(\u0026#34;에러!!\u0026#34;)} if(lastName:String.isEmpty()){ throw IllegalArgumentException(\u0026#34;에러!!\u0026#34;)} return Person(firstName, lastName) } // Local 함수 사용 fun createPerson(firstName: String, lastName:String): Person{ // 함수 속 함수 fun validateName(name: String){if(name.isEmpty()){throw IllegalArgumentException(\u0026#34;에러!!\u0026#34;)} validateName(firstName) validateName(lastName) return Person(firstName, lastName) } 17. lambda Java에서는 함수를 변수 할당, 파라미터로 전달 불가 (1급 객체 X)\nFruit::isApple 처럼 가능한 것 처럼 보이지만, 실제로는 Predicate라는 Interface를 넘김 Kotlin에서는 함수를 변수 할당,파라미터로 전달 가능 (1급 객체 O)\nlambda 생성, 호출\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // Kotlin // 함수의 Type도 다음과 같이 설정 가능 val isApple : (Fruit) -\u0026gt; Boolean = fun(fruit: Fruit): Boolean { return fruit.name == \u0026#34;사과\u0026#34; } // 바로 위와 동일한 코드 val isApple2 = {fruit: Fruit -\u0026gt; fruit.name == \u0026#34;사과\u0026#34; } val isApple3 = { fruit -\u0026gt; println(\u0026#34;사과만 받는 함수입니다.\u0026#34;) fruit.name == \u0026#34;사과\u0026#34; // 여러 줄 lambda에서 return 생략 시, 마지막 줄이 자동으로 return됨 } // labmda 호출, 동일한 코드 isApple(Fruit(\u0026#34;사과\u0026#34;, 1000)) // 명시적으로 호출 isApple.invoke(Fruit(\u0026#34;사과\u0026#34;, 1000)) Parameter로 함수 받기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 // 함수 Type을 적어서, 함수를 받을 수 잇다. fun filterFruits(fruits: List\u0026lt;Fruit\u0026gt;, filterFunc : (Fruit) -\u0026gt; Boolean) : List\u0026lt;Fruit\u0026gt;{ val results = mutableListOf\u0026lt;Fruit\u0026gt;() for(fruit in fruits){ if(filterFunc(fruit)){ results.add(fruit)} } return results } //호출시 다음과 같이 lambda 전달해서 사용 가능 filterFruits(fruit, {fruit -\u0026gt; fruit.name ==\u0026#34;사과\u0026#34;} ) // 이 때, lambda가 마지막 파라미터면 다음과 같이 사용 가능 (위 코드와 기능은 동일) filterFruits(fruit) {fruit -\u0026gt; fruit.name ==\u0026#34;사과\u0026#34;} // 파라미터가 하나인 경우는, it으로 화살표 앞 부분 대체 가능 filterFruits(fruit) {it.name ==\u0026#34;사과\u0026#34;} Closure Kotlin은 lambda가 실행되는 시점에, 참조하고 있는 변수들을 모두 포획해서 값을 가진다. 아래 코드에선 targetFruitName =”수박” 시점을 포획해서 가지고 있는다. 이러한 데이터 구조를 Closure라고 부르고, 이런 구조여야 lambda가 1급 객체일 수 있다. 1 2 3 4 5 6 7 8 9 10 11 12 //Java String targetFruitName = \u0026#34;바나나\u0026#34; targetFruitName = \u0026#34;수박\u0026#34; //Java에서는 Lambda에서 사용되는 변수는, final 변수거나, //final 키워드가 붙지 않았어도 값이 실제로 변경되지 않는 (Effectively final) 변수만 사용 가능 filterFruits(fruits, (fruit) -\u0026gt; targetFruitName.equals(fruit.getName())); // ERROR! //Kotlin var targetFruitName = \u0026#34;바나나\u0026#34; targetFruitName = \u0026#34;수박\u0026#34; filterFruits(fruits) { it.name == targetFruitName } // It Works! 18. Collection 내장 함수+ FP filter 1 2 // 사과만 받기 val apples = fruits.filter { fruit -\u0026gt; fruit.name == \u0026#34;사과\u0026#34; } filterIndexed : filter에서 index가 필요한 경우 1 2 3 4 val apples = fruits.filterIndexed { idx, fruit -\u0026gt; println(idx) fruit.name == \u0026#34;사과\u0026#34; } map, mapIndex : 각각의 원소에 대해 lambda 함수 실행\nmapNotNull : mapping의 결과가 null이 아닌 것만 추출\n1 2 val values = fruits.filter { it.name == \u0026#34;사과\u0026#34; } .mapNotNull { it.nullOrValue() } // null 아닌 값만 추출 all : 조건을 모두 만족하면 true, 아니면 false 1 2 // 전부 사과인지 확인 val isAllApple = fruits.all { it.name == \u0026#34;사과\u0026#34; } none : 조건을 모두 불만족하면 true, 아니면 false 1 2 // 사과가 하나도 없는지 확인 val isNoApple = fruits.none { it.name ==\u0026#34;사과\u0026#34; } any : 하나라도 만족하면 true, 아니면 false 1 2 // 하나라도 만원 넘는 과일 있는지 확인 val hasExpensiveFruit = fruits.any { it.price \u0026gt; 10_000 } count : 개수 1 2 // Java의 list.size()와 동일 val fruitCount = fruits.count() sortedBy, sortedByDescending : 오름차순, 내림차순 정렬 1 2 val sortedFruit = fruit.sortedBy{it.price} //오름차순 val desendSortedFruit = fruit.sortedByDescending{it.price} // 내림차순 distinctBy : 변경된 값을 기준으로 중복 제거 1 2 3 // 과일이 각각 어떤 종류 있는지 확인 val distinctFruitNames = fruits.distinctBy{ it.name } // 이름을 기준으로 중복 제거 .map{it.name} first, firstOrNull, last, lastOrNull : 첫번째, 마지막 값 빈 리스트라서 first, last가 가져올 값이 없으면 Exception 발생 1 2 fruits.first() fruits.lastOrNull() groupBy : 어떤 Key를 기준으로 Grouping (List → Map) 1 2 3 4 5 6 7 8 // 과일 이름을 key로 리스트의 내용물을 Grouping // key : 사과, value : {사과1, 사과2, 사과3...} val map: Map\u0026lt;String, List\u0026lt;Fruit\u0026gt;\u0026gt; = fruits.groupBy{fruit -\u0026gt; fruit.name } // 다음과 같이 key, value 동시 처리도 가능 // 이름을 Key로, 가격 List를 만드는 예시 val map2 : Map\u0026lt;String, List\u0026lt;Long\u0026gt;\u0026gt; = fruits .groupBy({it.name}, {it.price}) associateBy : 어떤 중복되지 않는 Key를 기준으로 Map 변환(List→ Map) 단, id가 중복값 있을 시 값이 유실될 수 있다. id가 중복값인 경우, 가장 마지막 값이 value가 된다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 val map: Map\u0026lt;Long, Fruit\u0026gt; = fruits.associateBy { fruit -\u0026gt; fruit.id } val fruitList = listOf\u0026lt;Fruit\u0026gt;( // id가 동일 Fruit(1,\u0026#34;1\u0026#34;), Fruit(1,\u0026#34;2\u0026#34;), Fruit(1,\u0026#34;3\u0026#34;), ) val associateBy = fruitList.associateBy { it.id } println(associateBy.count()) // 1 for (entry in associateBy) { println(entry) // 1=Fruit(id=1, name=3) } // 마찬가지로 key, value 동시 처리 가능 val map: Map\u0026lt;Long, Long\u0026gt; = fruits .associateBy({it.id}, {it.price}) Map에서도 위의 함수들 대부분 사용 가능 1 2 val map: Map\u0026lt;String, List\u0026lt;Fruit\u0026gt;\u0026gt; = fruits.groupBy{fruit -\u0026gt; fruit.name } .filter { (key, value) -\u0026gt; key == \u0026#34;사과\u0026#34; } 중첩된 Collections 처리 flatMap, flatten 1 2 3 4 5 6 7 8 9 10 11 12 13 var fruitsInList: List\u0026lt;List\u0026lt;Fruit\u0026gt;\u0026gt; = listOf( listOf(Fruit(1,\u0026#34;사과\u0026#34;), Fruit(2,\u0026#34;수박\u0026#34;), Fruit(3,\u0026#34;사과\u0026#34;)), listOf(Fruit(4,\u0026#34;바나나\u0026#34;), Fruit(5,\u0026#34;사과\u0026#34;), Fruit(6,\u0026#34;바나나\u0026#34;)), ) // 리스트 구조 관계없이, 사과인 과일을 모두 뽑아주세요 // output : listOf(Fruit(1,\u0026#34;사과\u0026#34;), Fruit(3,\u0026#34;사과\u0026#34;), Fruit(5,\u0026#34;사과\u0026#34;)) val appleList : List\u0026lt;Fruit\u0026gt; = fruitsInList.flatMap { list -\u0026gt; list.filter{it.name == \u0026#34;사과\u0026#34;} // 2차원 리스트를 1차원으로 val flattenList : List\u0026lt;Fruit\u0026gt; = fruitsInList.flatten(); 19. TakeIf, scope Function TakeIf : 주어진 조건을 만족하면 그 값이, 그렇지 않으면 null 반환 TakeUnless : 주어진 조건 만족하지 않으면 그 값이, 그렇지 않으면 null 반환 1 2 3 4 5 6 7 8 // 이 함수를 fun getFruitOrNull(fruit: Fruit): Fruit?{ return if(fruit.name == \u0026#34;사과\u0026#34;){fruit} else {null} } //다음과 같이 사용 가능 val getfruit = fruit.takeIf { it.name == \u0026#34;사과\u0026#34; } Fruit(1L, \u0026#34;사과\u0026#34;) // 이 경우 fruit Fruit(1L, \u0026#34;바나나\u0026#34;) // 이 경우 null 반환 Scope function : 일시적인 영역을 형성하는 함수 즉, lamba를 활용해 일시적 영역을 만들고 코드를 더 간결하게 하거나 method chaining에 활용하는 함수 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 두 함수는 같음 fun printPerson(person: Person?){ if(person != null){ println(person.name) println(person.age) } } fun printPerson(person: Person?){ person?.let{ println(person.name) println(person.age) } } Scope function의 종류 let, run : lambda의 결과를 반환 let : it 사용 (생략 불가능한 대신, 다른 이름 붙일 수 있음) run : this 사용 (생략 가능한 대신, 다른 이름 붙일 수 없음.) also, apply : 객체 자체를 반환 also : it 사용 apply : this 사용 with : 유일하게 확장함수 아님. this 사용, lambda의 결과를 반환 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 val value1 = person.let{ it.age } // value1 값 : person.age // person.let{ person -\u0026gt; person.age } 처럼, 다른 이름 붙일 수 있음 val value2 = person.run{ this.age } // value2 값 : person.age // person.run{ age } 처럼, this 생략 가능 val value3 = person.also{ it.age } // value3 값 : person val value4 = person.apply{ this.age } // value4 값 : person with(person){ println(name) println(age) } /* it 쓰는 경우 : 일반 함수를 파라미터로 받음 block : (T) -\u0026gt; R : R this 쓰는 경우 : 확장 함수를 파라미터로 받음 block: T.() -\u0026gt; R : R 궁금하면 직접 구현체를 확인해 보자.... */ ","date":"2023-01-17T14:24:20Z","image":"https://blog.lemondouble.com/p/f8a9d54607e901b4/cover_hu_55ae88b987f4fe95.png","permalink":"https://blog.lemondouble.com/p/f8a9d54607e901b4/","title":"내가 볼려고 올리는 Kotlin Cheat Sheet"},{"content":"Fly.io 라는 회사에선 도커 이미지만 준비하면 간단히 도커 이미지를 특정 국가 등에 배포할 수 있는 기능을 제공합니다.\n(가입 완료 후) fly.toml 이란 파일을 적절히 작성 후\n1 flyctl deploy 만 입력하면, 잘 지지고 볶아서 이미지를 서버에 올려 주고, DNS, https 설정, IP 발급, 모니터링 (Grafana) 설정 등을 해 줍니다.\n그리고 클릭 몇 번으로 Scale-up도 해 줍니다. 직접 세팅해도 되긴 하지만, 꽤 귀찮은 작업을 많이 해 줘서 편합니다.\n또한, 프리 티어에서도 아래와 같은 스펙을 제공합니다.\n하지만 프리 티어인 만큼 스펙이 그리 널널하진 않습니다. 그래서 \u0026ldquo;데이터 관리가 중요해서 클라우드 서비스를 사용하고 싶지만, 그리 많은 컴퓨팅 자원을 사용하지 않는\u0026rdquo; 어플리케이션 사용이 필요합니다.\n그런 용도로 적절한 Password Manager인 Vaultwarden을 설치하는 방법을 포스팅하려고 합니다.\nVaultWarden (Bitwarden 호환 서버) Bitwarden 은 패스워드 매니저로써, 웹 브라우저, 윈도우, ios, android 앱 등을 통해 여러 디바이스에서 비밀번호 공유가 가능하게 해 줍니다.\n크롬을 사용하신 분이라면, \u0026ldquo;크롬 자동완성\u0026rdquo; 기능을 생각하시면 이해가 빠릅니다.\nBitwarden은 거기에 더해 OTP 기능을 지원하며,\n비밀번호 생성기와\nOrganization (조직) 설정 등으로 패스워드 등을 공유하는 기능도 제공합니다.\n만약 같이 개발을 하는데, Slack Webhook URL이나, 환경변수, 개발 DB ID 및 패스워드 등을 공유해야 할 때 유용합니다.\n그 이외에도 해킹된 DB 중 내 비밀번호가 일치하는게 있는지, 재사용된 비밀번호가 있는지 등의 보안 감사 기능과, 종단간 암호화를 이용한 민감한 데이터 전송 기능인 SEND 기능 등을 제공합니다.\n하지만, Bitwarden은 오픈소스라 패스워드 서버를 Self-hosting 가능하지만, 프로덕션 레벨에서 고가용성을 확보하기 위해 높은 사양과 많은 설정을 요구합니다. ( Bitwarden Install Docs )\n이런 문제를 해결하기 위해, Bitwarden Client와 호환되지만 사양을 낮추고, Database도 SQLite를 사용하여 하나의 Docker Image만으로도 서버를 실행시킬 수 있는 Vaultwarden이란 프로젝트가 있습니다.\n공식 서버가 2GB RAM을 요구하는 것에 반해, 해당 서버는 Idle 상태일 때 약 100MB의 Memory를 사용하므로, Fly.io를 이용하여 배포하면 간단히 사용할 수 있습니다!\n다음 스탭을 따라하여, 한번 Vaultwarden 서버를 배포해 봅시다.\nFly.io Docs ( Link ) 를 따라가 본인 OS와 맞는 flyctl을 설치하고, 로그인까지 진행합니다. Shell에서 아무 폴더나 하나 만들고, (저의 경우 ~/flyio/vaultwarden 이란 폴더에서 작업했습니다.) 해당 폴더로 이동합니다. 3 . 아래 커맨드를 입력하여 fly.toml 이란 설정 파일을 만듭니다. 이 때, app-name은 원하는 대로 수정합니다. 입력시, Region을 선택하라는 창이 나오는데 저는 가장 가까운 Tokyo(nrt) 리전을 선택했습니다. 1 flyctl launch --name app-name --image vaultwarden/server:latest --no-deploy 아래 커맨드를 입력하여 저희 데이터를 영구적으로 저장할 Volume을 만듭니다. Volume은 컨테이너가 죽어도 파일이 날아가지 않도록 저장할 수 있는 SSD와 비슷하다고 생각하시면 됩니다. 앱 이름은 3에서 설정한 app 이름과 맞춰 주시고, Region은마찬가지로 가장 가까운 Tokyo(nrt) 리전을 선택해 줍니다. 1 flyctl volumes create vaultwarden_data --size 1 --app app-name 아래 명령어를 이용해 어플리케이션 시작시 주입될 Admin Token Secrets를 만들어 줍니다. ADMIN_TOKEN은 관리자 페이지를 들어갈 수 있는 비밀번호라고 생각하시면 됩니다. 유출되지 않도록 별도의 비밀번호를 만들어 주시고, 저희는 초기 세팅을 위해 admin page로 진입하기 위해 해당 Secrets 값을 추가해 줍니다. 1 flyctl secrets set ADMIN_TOKEN=abc123456 폴더에 있는 fly.toml 파일을 다음 파일을 참고하여 수정해 줍니다. 기존 파일에서 [env] 부분, [mounts] 부분, [[services]] -\u0026gt; internal_port 부분을 수정해주면 됩니다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 app = \u0026#34;app-name\u0026#34; kill_signal = \u0026#34;SIGINT\u0026#34; kill_timeout = 5 processes = [] [env] ROCKET_PORT = 8080 [experimental] allowed_public_ports = [] auto_rollback = true [mounts] destination = \u0026#34;/data\u0026#34; source = \u0026#34;vaultwarden_data\u0026#34; [[services]] http_checks = [] internal_port = 8080 processes = [\u0026#34;app\u0026#34;] protocol = \u0026#34;tcp\u0026#34; script_checks = [] [services.concurrency] hard_limit = 25 soft_limit = 20 type = \u0026#34;connections\u0026#34; [[services.ports]] force_https = true handlers = [\u0026#34;http\u0026#34;] port = 80 [[services.ports]] handlers = [\u0026#34;tls\u0026#34;, \u0026#34;http\u0026#34;] port = 443 [[services.tcp_checks]] grace_period = \u0026#34;1s\u0026#34; interval = \u0026#34;15s\u0026#34; restart_limit = 0 timeout = \u0026#34;2s\u0026#34; 해당 폴더에서 아래 명령어를 쳐서 앱을 배포합니다. 1 flyctl deploy 배포 후, 아래 커맨드를 입력해 웹 페이지에 접속합니다. 이후, 계정 만들기를 눌러 계정 생성과 마스터 패스워드 생성을 진행합니다. 1 flyctl open (Optional) 이후 아래 커맨드를 이용해 어드민 화면으로 진입하고, 위에서 생성했던 ADMIN_TOKEN으로 로그인합니다. 이후 필요한 세팅이 있다면 설정합니다. 저의 경우는 신규 가입을 불허하고, 초대 기능을 껐습니다. 1 flyctl open /admin 이후 Bitwarden 클라이언트를 다운받아 좌상단의 화살표를 클릭하고, 배포한 어플리케이션 주소를 입력해 줍니다. app-name.fly.dev, 혹은 방금 여러분이 들어간 웹 페이지를 입력해 주시면 됩니다. 축하드립니다! 여기까지 오셨으면, 세팅을 완료하셨습니다!\n이후, 꽤 편한 패스워드 매니저를 써 보시면 됩니다.\nAppendix 1. OTP 추가는 어떻게 하나요? ID/Password 추가 시, 아래와 같은 TOTP 란에\n위와 같은 Secret Key를 넣어주면 됩니다.\nAppendix 2. 이미 사용하고 있는 구글 패스워드를 쓸 순 없을까요?\nA. 가능합니다!\n구글 패스워드 매니저에서 비밀번호를 내보내기 한 후,\nVaultwarden 웹 콘솔 -\u0026gt; 로그인 -\u0026gt; 도구 -\u0026gt; 데이터 가져오기 하면 크롬에서 쓰던 패스워드를 그대로 사용할 수 있습니다.\n","date":"2023-01-17T14:03:50Z","image":"https://blog.lemondouble.com/p/740c42a214d8ce6b/cover_hu_95ef679b551d6230.png","permalink":"https://blog.lemondouble.com/p/740c42a214d8ce6b/","title":"Fly.io 소개 및 Fly.io에 올리기 좋은 서비스 추천 (VaultWarden)"},{"content":"Arduino를 만지다 보면, 단순히 LED를 껐다 키는\n행위가 생각보다 큰 원초적 희열을 가져다 준 다는 점을 알게 됩니다.\n하지만.. 곧 위와 같은 단색 LED로는 한 가지 색깔밖에 낼 수 없단걸 깨닫고, 몬가몬가 \u0026ldquo;어떤 색깔이든 낼 수 있는 LED\u0026quot;를 찾게 되는데요..\n모험을 떠나다 보면 아래와 같은 Cathode (-), Anode (+) LED를 만나게 됩니다.\n그리고 이제 결선도를 찾아보면 대충 이런게 나오고\n대충 따라 만들다 보면 음\u0026hellip; 아두이노 Digital Output 따라서 껐다 켰다가 잘 됩니다.\n근데? 이제? 뭔가 진짜 쓸만한 걸 만들어 보고 싶어서? 여러 개의 LED를 병렬로 잇는다던가 할 때가 되면? 어떻게 해야 할 지? 고민이 됩니다. 뭔가 단색 LED보다 회로도 많이 복잡해 진 것 같고\u0026hellip;\n근데 사실 별거 없습니다! 사진은 3색 Anode LED인데, Cathode도 딱히 다르지 않습니다(..)\n원리를 파악해봅시다.\nLED를 다음과 같이 RGB 핀과 + 핀 (Cathode LED는 - 핀)을 분리해 줍니다. 그리고 굴러다니는 3V짜리 단추 건전지에 +극에 +핀을, RGB 핀을 반대쪽에 대 봅니다. ?? 그렇습니다. 사실 3색 LED는 그냥 단색 LED 세개가 병렬로 묶여 있었던 것입니다..\n그래서 한 핀씩 빼 보면, 사실 나머지 핀과 관계 없이 단색 LED처럼 잘 동작하는걸 알 수 있습니다(\u0026hellip;) 아.. 그래서 대충 3색 LED가 단색 LED 세개를 병렬로 묶어놓은 거란걸 알면,\n대충 단색 LED 연결할 때 저항 어쩌고 했던 기억이 납니다. 저항을 세개 쓰는게 대충 적정 전압 인가해 준 것 같습니다.\n위 아두이노 회로도 사실 이해가 갑니다. 사실 단색 LED 세개 병렬로 이은 것과 같은 회로였던 것입니다..\n아.. 이걸 몰라서 3색 LED가 몇달동안 창고에 쳐박혀 있었습니다.\n다음엔 하나 해먹을 각오 하고서라도 막 굴려 봐야겠다 하는 생각이 듭니다..\n","date":"2023-01-17T13:45:21Z","image":"https://blog.lemondouble.com/p/141eb2dfe7f486bf/cover_hu_bfb5632992732dc0.png","permalink":"https://blog.lemondouble.com/p/141eb2dfe7f486bf/","title":"3색 LED는 내부가 어떤 식으로 구성되어 있을까?"},{"content":"\n이런 서비스를 만든 이야기를 합니다!\n2024.06.30 추가 : 당시는 ChatGPT가 나오기 전이라, 이런 챗봇 서비스가 흔하지 않던 때였습니다!\n으아아 도대체 무슨 일이 일어나고 있는 거에요!!\n1. 왜 이런걸 개발하게 됐나요? 원래 길게 뭔가 적었었는데, 남의 TMI는 별로 재미없을 것 같아 간단하게 적습니다. 근데 줄여도 꽤 기네요!\n취미로 딥 러닝을 이용한 자연어 처리 입문을 읽다, BERT의 문장 임베딩을 이용한 한국어 챗봇 파트에서 Train Data를 트윗 기반으로 할 수 있을 것 같은데? 하는 생각이 문득 들었습니다.\n어 이거.. 트위터 멘션으로도 비슷하게 할 수 있지 않을까?\n이런 식으로 만들 수 있지 않을까?\n문제는 Train Data의 개수인데, Twitter API는 최근 트윗 요청 시, 최대 3200개까지밖에 트윗 데이터를 제공하지 않습니다.\n보수적으로, 이 중 절반이 절반이 퍼블릭 트윗 (허공에 쓰는 트윗) 이라 가정하면 약 1600개의 트윗 정도가 Train Data가 되는데, 아시다시피 머신러닝에서 Train Data 1600개는 데이터의 양이 매우 작아 의미있는 챗봇을 만들기는 힘듭니다.\n하지만 재밌어 보이면 일단 해볼만한게 아닐까요?\n그래서 만들어 보기로 했습니다. 아\u0026hellip; 그때는 몰랐습니다. 한 200라인쯤 되는 Colab 코드를 서비스로 만들기 위해 한달을 넘게 태우게 될 줄은\u0026hellip;\n2. Serverless ML? 머신 러닝 변환된 데이터를 만드는 아주아주 간단한 구조는 다음과 같습니다. 즉, 그냥 서버 한대로 한번에 하나씩 처리하면, 한 사람 처리하는데 약 15분씩 걸립니다!\n그러면 대충 한시간당 4명 처리가 가능하네요! 100명이면 25시간, 1000명이 오면 250시간이니까\u0026hellip;\n오늘 서비스를 등록하면 10일 뒤에 완료가 됩니다! 심지어 이건 챗봇에서 질문/답변은 제외한 값입니다!\n아.. 이건 좀\u0026hellip; 어떻게든 다른 방법을 생각해 봐야겠다\u0026hellip;\n아! 생각해보니 Serverless 아키텍쳐를 쓰면 좋을 것 같습니다. Serverless의 개념을 잘 모르시는 분도 계실테니, 간단히 그림을 통해 설명해 볼게요.\n이게 기존 클라우드 사용 방식입니다. 한달, 일주일, 하루 등 정해진 기간만큼 컴퓨터를 빌리고, 빌린 시간만큼 돈을 냅니다. VPS라는 친구가 이거 비슷한 개념이에요!\n이건 Serverless 방법입니다. 어떤 작업을 할 때 필요한 만큼 요청하면 딱 그 작업이 끝날 때 까지만 빌릴 수 있습니다.\n만약 동시에 사람이 100명이 들어온다면, 100개의 서버를 동시에 빌려서 처리할 수 있습니다. 만약 사람이 한 명도 안 들어온다면, 그 동안은 서버를 빌리지 않아도 됩니다.\n이런 아키텍쳐를 적용한다면, 이론적으론 몇 명이 들어와도 한 명이 들어온것과 같은 속도로 데이터를 처리할 수 있습니다! 물론 실제로는 다른 문제들이 있긴 합니다.\n아! 문제가 해결됐네요! 대충 이렇게 한번 해 보죠! 3. 녹록치 않은 현실 대충 어떻게 하면 될 줄 알았는데\u0026hellip;\n아\u0026hellip;. 여러분도 갑자기 머리가 아프시죠? 이걸 아주아주 간단하게 줄이면 다음과 같습니다.\n아 이러니까 좀 이해가 되네요!\n핵심적인 부분은 다음과 같습니다.\n서버비를 아끼기 위해, 회원가입 / 인증 토큰 발급 등만 홈 서버의 Spring 서버가 처리 무거운 연산인 머신러닝은 클라우드(AWS)에 요청을 보내고, AWS 서버 자원을 활용해 처리. AWS에서 처리 완료시 홈 서버에게 알려주고, 처리 완료 사실을 DB에 저장 개인적으로 홈 서버를 사용하는 것이 있으니, 가벼운 연산은 홈 서버가 처리해 서버 임대료를 줄이고, 무거운 연산은 클라우드 서비스(AWS)에게 맡겨 가성비와 성능을 둘 다 잡겠다는 꿈과 희망을 가지고 서비스를 만들었습니다!\n4. 런칭 - 부제 : Bottleneck은 생각치 못한 곳에서 터진다 죄송합니다.. 아니 그\u0026hellip; 계획은 분명히 완벽했는데..\n당연히 이 프로젝트의 Bottleneck은 머신러닝 부분이 될 줄 알았고, 그에 맞춰 전체 시스템을 작성했는데 의외로.. 그렇지가 않았다.\n삭제된 메세지 수는 처리량을 가늠할 수 있는 지표이다. 요청이 메세지 형태로 들어왔을 때, 1분동안 몇 개의 요청을 처리하고 요청(메세지)를 삭제했는가를 나타낸다.\n보다시피, 트윗 3200개를 긁어오는 스크립트와 BERT로 머신러닝 처리를 하는 Queue는 비슷하게 움직이며 요청을 전부 소화한 것을 볼 수 있다 (1번, 2번 그래프).\n하지만 모든 파이프라인 작업이 성공시 Spring에 처리가 완료되었음을 알리고, DB에 \u0026ldquo;사용 가능\u0026rdquo; field를 true로 바꿔주는 Queue(3번)가 처리량을 따라가지 못 하는 것을 볼 수 있다.\n문제점은, 초기 아키텍쳐는 ServiceReadyQueue가 바로바로 처리되는 것을 가정했다는 것이다.\n따라서 Train 완료시 트위터 알림(Serverless)과 Spring 알림이 동시에 가는데, 이 때 트위터 알림은 바로 가지만 DB 작성에는 지연이 걸린다. 즉, 알림을 받고 간 유저가 최대 33분동안 \u0026ldquo;사용할 수 없습니다!\u0026rdquo; 화면을 봐야 하는 문제가 생겼다.\n솔직히 여기서 고민을 좀 많이 했었다.\n아\u0026hellip; 평생 못 받아볼 트래픽인데\u0026hellip; 로그인 막을\u0026hellip; 막아야겠지..? 30분씩 밀리는데..?\nSpring에서 분당 10개의 메세지를 Consume하므로, 서비스에 등록하는 유저가 6초당 한명 이하라면 bottleneck은 자동으로 해소되겠지만, 모니터링 결과 도저히 줄어드는 추세가 아니어서 일단 로그인을 막기로 했다.\n기준은 명확했는데, 처리가 늦게 되는 것 은 괜찮지만, 처리가 되었다고 나왔는데 서비스를 사용할 수 없는 것 은 엄청나게 큰 결함이라고 생각했기 때문이다.\n따라서, 일단 React 프로젝트에서 급하게 수정해 로그인 버튼과 서비스 등록 버튼을 떼고, 다음과 같이 파이프라인을 급하게 수정했다.\n이렇게 만든 경우, 처리속도엔 변함이 없지만 알림을 받고 왔는데 사용할 수 없는 경우 라는 크리티컬한 경우를 피할 수 있게 된다.\n이후 잠깐동안 DB 쓰기가 완료될 때 까지(밀린 작업이 처리될 떄까지) 시간을 가지고, 다시 로그인/등록 버튼을 살렸다.\n이후, 생각한 대로 정상적으로 작동했다. 비록 Bottleneck 자체는 해결되지 못했지만, 적어도 완료 알림을 보고 온 사람이 몇십분간 서비스를 사용할 수 없는 크리티컬한 에러를 피했다.\n그리고 모니터링을 하며 원인 분석을 하던 중, 메세지 처리량이 최대 10개에서 Fixed된 것을 보고, 관련 내용을 찾던 중 spring-cloud-aws의 issue에서 이런 issue를 찾을 수 있었다. 요약하자면, 한 번에 10개의 메세지만 처리할 수 있고, 비동기가 아니라 동기식으로 작동하는 문제가 있다는 것이다.\n현재도 열려있는 다른 issue를 보니, 해당 문제는 대대적인 리팩토링이 필요해 3.0.0에서 수정할 예정이라고 한다. 결론적으론..\n아\u0026hellip; 그럼 핫픽스하긴 힘들겠네\u0026hellip;\n결론적으로, Bottleneck 자체를 간단하게는 해결할 순 없고 이를 해결하기 위해선 Spring이 하는 일 자체도 Serverless 아키텍쳐로 재작성하거나, 라이브러리 의존성을 뜯어고치는 대공사(\u0026hellip;) 를 해야 한다는 결론에 이르렀다. 뭐가 됐든, 런칭했는데 오늘 할 일은 아니니까 모니터링이나 좀 더 하기로 했다.\n앗! 이번엔 다른 Queue에 메세지가 쌓이기 시작했다. Tweet 3200개를 긁어오는 큐였다. 바로 모니터링에 들어갔고, 해당 Queue를 핸들링하는 Lambda 함수가 다음과 같이 Error Rate가 100%까지 올라가는 걸 볼 수 있 었다.\n이건 보나마나 API 리밋이네!\nCloudWatch에 들어가서, 해당 Lambda Function의 로그를 까 보니 역시나였다.\nAPI 리밋 맞네!\n무슨 종류의 리밋인진 모르겠지만 (트위터는 유저 리밋, 앱 리밋, 15분 리밋, 하루 리밋, 한달 리밋 등 아주 종류별로 제한이 있다..) 서비스 등록 버튼을 빼야겠다는 것은 자명했다(..) 서비스 등록 버튼을 빼고, 트위터에 관련 공지를 올리고 나니 폭풍같은 3시간이 지나갔다.\n일일 리밋을 먹어서 더이상 내가 할 수 있는 일이 없는고로(..) 서비스 부검을 하고 기록을 남기기로 해서, 지금 이걸 작성하고 있다.\n5. 결론 및 후기 아\u0026hellip; SQS -\u0026gt; Spring 으로 가는 곳이 Bottleneck이 될 줄도 몰랐고, 항상 꿈꿔왔지만 갑자기 많은 트래픽이 몰아쳐서 서버가 못 버틸 때, 패닉 상태가 오는 걸 처음 경험했던 것 같다.\n특히, 서비스 전체를 내가 만들었으니(\u0026hellip;) 장애 부분은 빠르게 찾았던 것 같은데, 라이브 서비스 중인 서비스를 어떻게 고칠 수 있을지? 사이드 이펙트는 어떻게 될지? 등을 고민하는게 가장 힘들었던 것 같다.\n특히 중간처럼 \u0026ldquo;서비스에 중대한 장애가 발생한 경우(알람은 왔는데 실행이 불가)\u0026rdquo; 라이브 서버에서 빠르게 고쳐야 하는데, 급하게 작성한 코드가 어떤 Side Effect를 일으킬 줄 모르는 채로 급하게 핫픽스를 해야 하는게 심리적으로도 부담되고, 가장 떨리는 떄였던 것 같다.\n솔직히 가장 크게 느낀 내용은\n아 이래서 자동화 테스트 자동화 테스트 하는구나\u0026hellip; 테스트가 잔뜩 있었으면 만약 비슷한 경우가 생기더라도, 조금 더 마음 편하게 수정할 수 있겠구나\n하는 생각을 했었다.\n그리고 의도하지 않았지만 AWS 덕을 크게 본 부분이 있는데(?)\nQueue 기반 아키텍쳐 사용을 통해 어떤 부분에서 병목이 발생하고, 에러가 발생하는지 빠르게 알 수 있었다. 만약 SQS를 사용하지 않았다면 Bottleneck을 찾는데 훨씬 많은 시간이 걸렸겠지만, Queue 기반 아키텍쳐를 사용한 덕분에 어떤 부분에서 병목이 나는지 쉽게 찾을 수 있었다. Cloudtrail 로그가 매우매우 편하고 중요하다는걸 깨달았다. 모니터링 시스템까지 구축하고 내놓으려다, 일단 한번 내 보고 반응 좋으면 더 개발하자(?) 라는 나이브한 마음가짐으로 일단 출시부터 해 봤었다. 하지만 역시나 문제가 생겼고, 개발 당시에 습관처럼 찍어놓은 상태 변화 및 Exception 출력 등이 에러 처리에 큰 도움이 되었다. 꼭 AWS 안 쓰더라도 로그를 잘 찍고, 로그 보는 법을 잘 알아두자.. DLQ 설정을 꼭 하자. Poller(서비스 처리자)가 N번 이상 처리했는데도 Exception 등으로 처리가 안 된 메세지들은 DLQ(Dead Letter Queue)라는 곳에 모아놓을 수 있는데, DLQ를 활용하면 이후 DLQ 리드라이브라는 기능으로 처리되지 않은 메세지를 원래 큐로 돌려보낼 수 있다. 즉, 모종의 이유로 에러가 발생해서 처리되지 못 한 요청이 있다면 해당 요청은 DLQ로 가고, 이후 DLQ 리드라이브를 이용해 중단점부터 다시 처리할 수 있다! 암튼.. 많이 힘들었는데 어떻게든 여차저차 넘긴 것 같아서 다행입니다! 혹시 제가 잘못 알고 있는 부분이나, 개선할 수 있는 부분이 있다면 얼마든지 의견 남겨 주세요! 긴 글 읽어주셔서 정말 감사합니다!\n","date":"2022-02-27T15:39:07+09:00","image":"https://blog.lemondouble.com/p/8b50460f83230126/image-18_hu_ebe9495a0a869175.png","permalink":"https://blog.lemondouble.com/p/8b50460f83230126/","title":"Lemon Toolbox 첫 런칭 회고"}]