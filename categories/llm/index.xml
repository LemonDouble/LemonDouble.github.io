<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on Lemon Blog</title><link>https://blog.lemondouble.com/categories/llm/</link><description>Recent content in LLM on Lemon Blog</description><generator>Hugo -- gohugo.io</generator><language>ko</language><lastBuildDate>Thu, 30 Mar 2023 19:15:11 +0900</lastBuildDate><atom:link href="https://blog.lemondouble.com/categories/llm/index.xml" rel="self" type="application/rss+xml"/><item><title>ChatGPT는 상식 문제를 잘 풀 수 있을까? (논문)</title><link>https://blog.lemondouble.com/p/dedda5393e239315/</link><pubDate>Thu, 30 Mar 2023 19:15:11 +0900</pubDate><guid>https://blog.lemondouble.com/p/dedda5393e239315/</guid><description>&lt;img src="https://blog.lemondouble.com/p/dedda5393e239315/cover.png" alt="Featured image of post ChatGPT는 상식 문제를 잘 풀 수 있을까? (논문)" />&lt;p>요즘 심심할 때 논문을 하나씩 읽는 취미가 생겼습니다.
그 중, arxiv에서 재밌는 논문을 하나 읽게 되어서, 정리하고 공유해 보려고 합니다.&lt;/p>
&lt;p>논문 제목은 &lt;code>ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models&lt;/code> (&lt;a class="link" href="https://arxiv.org/abs/2303.16421" target="_blank" rel="noopener"
>Arxiv Link&lt;/a>) 입니다.&lt;/p>
&lt;p>주제는, &lt;strong>대규모 언어 모델(ChatGPT) 는 상식적인 문제를 잘 풀 수 있는가?&lt;/strong> 입니다. 꽤 재밌는 주제라서, 제가 공부하는 겸 공유하는 겸, 겸사겸사 읽은 내용을 정리해 보려고 합니다.&lt;/p>
&lt;p>제가 논문을 읽어 본 적도 없고, 학부 기억도 가물가물 해서 틀릴 수 있음을 미리 알려드립니다! (틀린거 있음 알려주세요 헤헤)&lt;/p>
&lt;h2 id="1-요약">1. 요약
&lt;/h2>&lt;p>논문은 다음과 같은 질문을 던집니다.&lt;/p>
&lt;ol>
&lt;li>GPT는 상식에 관한 질문에 효과적으로 답할 수 있는가?&lt;/li>
&lt;li>GPT는 상식에 대해 잘 알고 있는가?&lt;/li>
&lt;li>GPT는 특정 질문에 답하기 위해 필요한 상식과 아닌 상식을 잘 구분할 수 있는가?&lt;/li>
&lt;li>GPT는 주어진 상식을 효과적으로 활용하여 질문에 답할 수 있는가?&lt;/li>
&lt;/ol>
&lt;p>그리고 이에 대한 간략한 답은 다음과 같습니다.&lt;/p>
&lt;ol>
&lt;li>효과적으로 답할 수 있으나, 사회규범, 인과관계, 시간과 관계된 특정 분야에 대해선 잘 답하지 못한다.&lt;/li>
&lt;li>대부분의 상식을 잘 알고 있다.&lt;/li>
&lt;li>질문에 대답하는데 밀접한(필요한) 상식과 아닌 상식을 잘 구분하지 못 한다.&lt;/li>
&lt;li>문맥상에서 추가적인 상식을 입력해 주더라도, 잘 사용하지 못 한다.&lt;/li>
&lt;/ol>
&lt;h2 id="2-상식의-분류">2. 상식의 분류
&lt;/h2>&lt;p>연구진은 상식을 8가지 카테고리로 분류했습니다.
각각 다음과 같습니다.&lt;/p>
&lt;hr>
&lt;p>&lt;strong>1. 일반(General) 상식 (널리 공유되는 상식)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>해는 동쪽에서 뜬다&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>2. 물리적(Physical) 상식 (물리적 세계에 대한 지식)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>유리컵은 떨어지면 깨진다. 물은 아래로 흐른다&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>3. 사회적(Social) 상식 (사회규범에 대한 지식)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>도움을 받았으면 감사합니다~ 라고 해야 한다.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>4. 과학(Science) 상식 (과학 개념의 원리/지식)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>중력은 모든 물체를 지구 중심으로 끌어당긴다&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>5. 인과 관계(Event) 상식 (인과 관계 및 순서에 대한 지식)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>물컵을 넘어트리면 -&amp;gt; 물이 쏟아진다&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>6. 숫자와 관련된(Numerical) 상식 (숫자와 관련된 상식)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>사람 손가락은 2개고 손가락은 10개&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>7. 전형적인(Prototypical) 상식 ( 개념에 대한 지식)&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>제비는 새의 일종이며, 날개가 있다&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>8. 시간과 관련된(Temporal) 상식&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>해외여행은 산책보다 시간이 오래 걸린다.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>그리고, 연구진은 각 카테고리별로 상식에 관련된 데이터 셋 11개를 준비했습니다.&lt;/p>
&lt;p>왼쪽부터 데이터셋 이름 / 카테고리 / 예시 질문 입니다.&lt;/p>
&lt;p>&lt;img src="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-19-51-27.png"
width="639"
height="413"
srcset="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-19-51-27_hu_946a58ef92da0da3.png 480w, https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-19-51-27_hu_892ff76f2ea00c3a.png 1024w"
loading="lazy"
alt="데이터셋 카테고리 및 예시 질문"
class="gallery-image"
data-flex-grow="154"
data-flex-basis="371px"
>&lt;/p>
&lt;h2 id="3-chatgpt는-상식-질문에-효과적으로-답할-수-있을까">3. ChatGPT는 상식 질문에 효과적으로 답할 수 있을까?
&lt;/h2>&lt;p>연구진은 위 데이터셋마다 100개씩 문제를 뽑아 GPT3, GPT3.5, ChatGPT 모델에 물어봤습니다.&lt;/p>
&lt;p>아래는 GPT3, GPT3.5, ChatGPT의 각 질문에 대한 정답 비율입니다.&lt;/p>
&lt;p>&lt;img src="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-19-54-34.png"
width="535"
height="208"
srcset="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-19-54-34_hu_7d8ccc9578b70063.png 480w, https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-19-54-34_hu_15cbe30c15803ccb.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="257"
data-flex-basis="617px"
>&lt;/p>
&lt;p>주목할 점은 다음과 같습니다.&lt;/p>
&lt;ul>
&lt;li>전체적으로 잘 답변함
&lt;ul>
&lt;li>과학 (Science) 분야는 약 94%로 가장 높은 정답률을 보임.&lt;/li>
&lt;li>사회규범(Social), 인과관계(Event), 시간 (Temporal) 에 대한 정답률은 저조 (70% 미만)&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>전체적으로 GPT 3.5 모델에 비해, 튜닝 모델인 ChatGPT의 정확도가 높음
&lt;ul>
&lt;li>일부 질문에 대해 GPT 3.5가 정답률이 높은 것으로 나와있으나, 이는 ChatGPT가 &amp;ldquo;주어진 데이터만으로는 정답을 알 수 없습니다&amp;rdquo; 라는 답변이 나와 생긴 착시에 가까움.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="4-chatgpt는-질문에-답변하기-위해-필요한-상식과-아닌-상식을-잘-구분할-수-있을까">4. ChatGPT는 질문에 답변하기 위해 필요한 상식과 아닌 상식을 잘 구분할 수 있을까?
&lt;/h2>&lt;p>연구진은 위 질문 데이터셋에서, 각 데이터셋마다 20개를 다시 뽑았습니다.&lt;/p>
&lt;p>이후, GPT에게 &amp;ldquo;이 질문에 답변하기 위해 어떤 지식이 필요한지&amp;rdquo; 물어본 뒤, 인간의 답변을 바탕으로 Precision(정확도), Recall(재현율), F1 Score를 구했습니다.&lt;/p>
&lt;p>&lt;img src="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-20-04-51.png"
width="309"
height="309"
srcset="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-20-04-51_hu_2774cd353a503d77.png 480w, https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-20-04-51_hu_9d67c8eda677ab40.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="100"
data-flex-basis="240px"
>&lt;/p>
&lt;p>Precision? Recall? 어려운 말이 나오니까 한번 정리하고 들어갈게요.&lt;/p>
&lt;hr>
&lt;ul>
&lt;li>
&lt;p>Precision : GPT가 낸 정답의 정확도&lt;/p>
&lt;ul>
&lt;li>GPT의 답변 중 정답은 몇 %인가?&lt;/li>
&lt;li>예를 들어 5개의 답변 중, 3개는 정답 2개는 오답이라면 Precision은 60%&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Recall : 전체 정답 중 GPT가 찾아낸 정답의 개수&lt;/p>
&lt;ul>
&lt;li>사람이 예측한 정답 중, GPT가 몇 개나 찾아냈는가?&lt;/li>
&lt;li>예를 들어 예측 정답이 5개고, GPT가 답변 10개 중 4개는 정답, 6개는 오답이라면 Recall은 80% (4/5)&lt;/li>
&lt;li>Recall을 계산할 때 오답은 중요하지 않음!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>F1 Score : Precision과 Recall의 조화평균&lt;/p>
&lt;ul>
&lt;li>Precision만 지표로 삼으면? : 확률 제일 높은 답을 &lt;strong>딱 하나만&lt;/strong> 하면 됨&lt;/li>
&lt;li>Recall만 지표로 삼으면? : 답변을 10000개씩 쓰면 그 중 &lt;strong>몇개는 얻어걸림&lt;/strong>&lt;/li>
&lt;li>하지만 우리가 원하는 건 &amp;ldquo;정확한 답을&amp;rdquo; &amp;ldquo;충분히 많이&amp;rdquo; 해 주는 AI 모델&lt;/li>
&lt;li>따라서 Precision과 Recall의 조화평균을 내면 &amp;ldquo;정확도&amp;rdquo; 비슷한 지표가 나옴!&lt;/li>
&lt;li>즉, F1 Score가 높음 == 답변을 잘 했다!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;hr>
&lt;p>어려운 이야기를 했는데! 결론적으로, 다음과 같은 결과를 얻었습니다.&lt;/p>
&lt;ul>
&lt;li>GPT는 Precision은 높지만 Recall은 높음
&lt;ul>
&lt;li>전체 데이터셋에서 Precesion은 **55.88%**지만, Recall은 &lt;strong>84.42%&lt;/strong>&lt;/li>
&lt;li>&lt;strong>즉, 질문을 해결하는데 필요한 지식을 거의 대부분 알려주지만, 정확도가 높진 않음&lt;/strong>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>GPT는 과학 분야에선 비교적 성능이 좋지만, (F1 74~76%), 사회규범, 시간 분야에서 특히 성능이 낮음(F1 50% 이하)&lt;/li>
&lt;/ul>
&lt;h2 id="5-chatgpt는-상식에-박식할까">5. ChatGPT는 상식에 박식할까?
&lt;/h2>&lt;p>연구진은 3. 에서 생성된, 필요한 지식을 기반으로 질문 프롬프트를 수동으로 만들었습니다. 이후, GPT가 생성한 답변이 정답인지 오답인지를 수동으로 평가했습니다.&lt;/p>
&lt;p>&lt;img src="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-20-59-33.png"
width="314"
height="357"
srcset="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-20-59-33_hu_20f0b5c18e9fb4b0.png 480w, https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-20-59-33_hu_879552020225cd4c.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="87"
data-flex-basis="211px"
>&lt;/p>
&lt;p>위 질문의 경우는 ChatGPT가 생성한 오답의 예시입니다.&lt;/p>
&lt;p>GPT는 &lt;code>blowing into a trash bag and tying it with a rubber band may create a cushion-like surface, but it is unlikely to be durable or comfortable for prolonged use as an outdoor pillow&lt;/code> (번역 : 쓰레기 봉투에 바람을 불어 고무줄로 묶으면 쿠션같은 표면을 가지지만, 야외용 배개로 오래 쓰기엔 내구성이 떨어지거나 편하지 않을 것) 이라 했지만, &lt;strong>쓰레기 봉투로 배개를 만드는 건 흔한 관행이므로 오답으로 처리&lt;/strong>했다고 합니다.&lt;/p>
&lt;p>&lt;img src="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-21-05-54.png"
width="323"
height="235"
srcset="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-21-05-54_hu_651a451421b4a447.png 480w, https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-21-05-54_hu_90d6d03f894370bf.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="137"
data-flex-basis="329px"
>&lt;/p>
&lt;p>위 결과 표에 따라, 연구진은 다음과 같은 결과를 얻었습니다.&lt;/p>
&lt;ul>
&lt;li>GPT는 박식하며, 질문에 답변하기 위한 대부분의 상식을 알고 있음.
&lt;ul>
&lt;li>전체 데이터셋에서 평균 82.66%의 정확도를 보임&lt;/li>
&lt;li>대부분의 데이터셋에서 70% 이상의 정확도를 보임.&lt;/li>
&lt;li>단, 사회규범 영역에서 54.92%로 성능이 낮음.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>하지만, GPT의 답변 중 오해의 소지가 있거나, 지나치게 일반화되어 필요없는 지식도 들어가 있음
&lt;ul>
&lt;li>전체 답변의 26.25%에 관련성이 낮고 오해의 소지가 있는 정보가 포함되어 있음.&lt;/li>
&lt;li>15% 정도의 설명이 지나치게 일반화되어, 질문 답변에 필요한 구체적 정보가 아님&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="6-chatgpt는-응답시-대화-맥락에서-추가된-상식을-활용하여-답변에-활용할-수-있을까">6. ChatGPT는 응답시 대화 맥락에서 추가된 상식을 활용하여, 답변에 활용할 수 있을까?
&lt;/h2>&lt;p>연구진은 GPT가 문맥에서 나온 상식을 이용하여 답변에 활용할 수 있는지 확인하기 위해, 4.에서처럼 GPT에게 질문에 답변할 때 필요한 상식을 추론하게 한 후, 같은 질문에 다시 답하게 하여 답변이 평가되는지 확인했습니다.&lt;/p>
&lt;p>&lt;img src="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-21-17-10.png"
width="303"
height="384"
srcset="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-21-17-10_hu_6ddff77f5f86b49b.png 480w, https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-21-17-10_hu_e6e89d4f12dfbc59.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="78"
data-flex-basis="189px"
>&lt;/p>
&lt;p>위 예시는, 이전에 오답이었던 답변이 추가 설명을 생성한 뒤에도 변경되지 않는 예시를 보여줍니다.&lt;/p>
&lt;p>&lt;img src="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-21-22-59.png"
width="318"
height="263"
srcset="https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-21-22-59_hu_9d33f72ac9651a0f.png 480w, https://blog.lemondouble.com/p/dedda5393e239315/2023-03-30-21-22-59_hu_bdc32ca33c8be8d0.png 1024w"
loading="lazy"
class="gallery-image"
data-flex-grow="120"
data-flex-basis="290px"
>&lt;/p>
&lt;p>위 결과 표에 따라, 연구진은 다음과 같은 결과를 얻었습니다.&lt;/p>
&lt;ul>
&lt;li>GPT는 GPT가 생성한 상식 설명을 대화 맥락 추가하는 것 만으로는, 효율적으로 답변에 활용할 수 없음.
&lt;ul>
&lt;li>맥락에 설명이 추가되는 경우, 오답이 정답으로 바뀌는 경우 (C-&amp;gt;W) 및 정답이 오답으로 바뀌는 경우 (W-&amp;gt;C)가 모두 존재&lt;/li>
&lt;li>Social IQA 데이터셋의 경우, 맥락에 추가된 상식이 잘못되어 오히려 정답이 오답으로 바뀌는 것이(5개) 오답이 정답으로 바뀌는 것보다(1개) 많았음.&lt;/li>
&lt;li>연구진은 이미 Model에 생성된 지식이 있어, 단순히 추가 정보를 생성하는 것이 큰 효과가 없는 것으로 추정함.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>심지어, 대화 맥락에 &amp;ldquo;GPT가 생성한 상식&amp;rdquo; 이 아닌, &amp;ldquo;유저가 직접 올바른 상식&amp;quot;을 추가해줘도 정답률이 100%가 되지 못 했음.&lt;/p>
&lt;ul>
&lt;li>사람이 주석을 단 CoS-E, ECQA 데이터셋을 바탕으로 대화 맥락에 정답 (Golden Knowledge) 를 추가함.
&lt;ul>
&lt;li>CoS-E는 오답-&amp;gt;정답이 4개 증가&lt;/li>
&lt;li>ECQA는 오답-&amp;gt;정답이 8개, 정답-&amp;gt;오답이 1개 증가&lt;/li>
&lt;li>연구진은 복잡한 추론 (예를 들어, 부정 추론) 등을 활용할 수 있는 능력이 부족하다고 추론
&lt;ul>
&lt;li>부정 추론의 예시
&lt;ul>
&lt;li>Q: 농구공에 구멍이 났지만, 모양이 그대로라면 틀린 것은?&lt;/li>
&lt;li>A: 구멍이 뚫렸다, B:미국에서 인기가 있다, C: 공기가 가득 차 있다&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>CoS-E 데이터셋은 &amp;ldquo;구멍이 뚫린 물체에는 공기가 머무를 수 없다&amp;rdquo; 라고 설명했지만, GPT는 여전히 A를 예측&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>// TODO : 더 쓰기..&lt;/p></description></item></channel></rss>